#!/usr/bin/env python3
# ================================================================
# LOW-RANK FISHER PRECONDITIONING TEST: Fisher DP-SGD + Fisher Matrix Calibration
#    * Baseline: Standard linear regularization calibration
#    * Enhanced: Low-rank Fisher matrix preconditioning for influence vectors
#    * Comparison of calibration quality and computational efficiency
# ================================================================

import os, glob, argparse, copy, math
import numpy as np
import torch, torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as T
from torch.utils.data import DataLoader, Subset, Sampler
from tqdm import tqdm
from torch.autograd import grad  # Required for per-sample gradients

# Project-specific helpers
from fisher_dp_sgd import compute_fisher, topk_eigh_with_floor, maha_clip
from dp_sgd import train_with_vanilla_dp
from dp_sat import train_with_dp_sat
from privacy_accounting import (
    get_privacy_params_for_target_epsilon, 
)
from model import CNN
from mia import evaluate_membership_inference, confidence_attack, shadow_model_attack, prepare_mia_data_sample_level, prepare_mia_data_user_level
from influence_function import calibrate_model_research_protocol, get_evaluation_slice
from config import set_random_seeds, get_random_seed

set_random_seeds()  # Set reproducible random seeds
models_dir = './saved_models'; os.makedirs(models_dir, exist_ok=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Diagnostic functions (moved from influence_function.py)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def diagnose_calibration(model, critical_data, critical_targets, device):
    """
    Simple diagnostic function to measure critical slice performance.
    Replacement for the removed function in influence_function.py.
    """
    if len(critical_data) == 0:
        return {'loss': float('inf'), 'accuracy': 0.0, 'num_samples': 0}
    
    model.eval()
    with torch.no_grad():
        output = model(critical_data)
        loss = F.cross_entropy(output, critical_targets)
        predictions = output.argmax(dim=1)
        accuracy = (predictions == critical_targets).float().mean()
    
    return {
        'loss': loss.item(),
        'accuracy': accuracy.item() * 100,
        'num_samples': len(critical_data)
    }

def print_calibration_effect(before_stats, after_stats, target_class="all"):
    """
    Print the effect of calibration on the evaluation slice.
    """
    if target_class == "all":
        slice_description = "All Classes"
    else:
        slice_description = f"Class {target_class}"
    
    print(f"\nğŸ“Š Calibration Effect Analysis ({slice_description}):")
    print(f"   â€¢ Samples evaluated: {before_stats['num_samples']}")
    print(f"   â€¢ Loss before:   {before_stats['loss']:.4f}")
    print(f"   â€¢ Loss after:    {after_stats['loss']:.4f}")
    print(f"   â€¢ Î” Loss:        {after_stats['loss'] - before_stats['loss']:+.4f}")
    print(f"   â€¢ Accuracy before: {before_stats['accuracy']:.2f}%")
    print(f"   â€¢ Accuracy after:  {after_stats['accuracy']:.2f}%")
    print(f"   â€¢ Î” Accuracy:      {after_stats['accuracy'] - before_stats['accuracy']:+.2f}%")
    
    if after_stats['loss'] < before_stats['loss']:
        print(f"   âœ… SUCCESS: Calibration reduced evaluation slice loss!")
    else:
        print(f"   âš ï¸  WARNING: Calibration increased evaluation slice loss")
    
    if after_stats['accuracy'] > before_stats['accuracy']:
        print(f"   âœ… SUCCESS: Calibration improved evaluation slice accuracy!")
    else:
        print(f"   âš ï¸  WARNING: Calibration reduced evaluation slice accuracy")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Low-Rank Fisher Preconditioning for Influence Function Calibration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def calibrate_with_low_rank_fisher(model, pub_loader, priv_loader, critical_data, critical_targets, 
                                  device, lam, U, eta=100, trust_tau=0.01, reg=10.0, 
                                  strict=True, clean_model=None, target_layer="conv1"):
    """Enhanced calibration using low-rank Fisher matrix preconditioning for influence vectors."""
    
    k = len(lam)
    print(f"ğŸ¯ Low-Rank Fisher Preconditioning Calibration:")
    print(f"   â€¢ Using pre-computed Fisher eigenpairs: k={k}")
    print(f"   â€¢ Target layers: {target_layer}")
    print(f"   â€¢ Eigenvalue range: [{lam.min().item():.6f}, {lam.max().item():.6f}]")
    print(f"   â€¢ Eta: {eta}")
    print(f"   â€¢ Trust tau: {trust_tau}")
    print(f"   â€¢ Regularization: {reg}")
    
    # Get Fisher-specific parameters (same as used in Fisher computation)
    if target_layer == "all":
        fisher_param_names = [n for n,_ in model.named_parameters()]
    elif "," in target_layer:
        layers = [s.strip() for s in target_layer.split(",")]
        fisher_param_names = [n for n,_ in model.named_parameters()
                             if any(l in n for l in layers)]
    else:
        fisher_param_names = [n for n,_ in model.named_parameters()
                             if target_layer in n]
    
    fisher_params = [dict(model.named_parameters())[n] for n in fisher_param_names]
    fisher_param_dim = sum(p.numel() for p in fisher_params)
    
    # Get all model parameters for full model update
    all_params = list(model.parameters())
    total_params = sum(p.numel() for p in all_params)
    
    print(f"   â€¢ Fisher parameters: {fisher_param_dim} (layers: {target_layer})")
    print(f"   â€¢ Total parameters: {total_params}")
    print(f"   â€¢ Fisher coverage: {fisher_param_dim/total_params*100:.1f}%")
    
    # Verify dimension compatibility
    expected_fisher_dim = U.shape[0]
    if fisher_param_dim != expected_fisher_dim:
        print(f"   âŒ DIMENSION MISMATCH:")
        print(f"      â€¢ Expected Fisher dim: {expected_fisher_dim}")
        print(f"      â€¢ Actual Fisher dim: {fisher_param_dim}")
        print(f"      â€¢ Falling back to standard calibration...")
        return calibrate_model_research_protocol(
            model, pub_loader, priv_loader, critical_data, critical_targets, 
            device=device, method='linear', eta=eta, trust_tau=trust_tau,
            strict=strict, clean_model=clean_model, reg=reg
        )
    
    # Get critical slice gradient (what we want to improve)
    if len(critical_data) == 0:
        print(f"   âš ï¸  No critical data provided, using random gradient")
        # Generate random gradient only for Fisher parameters
        fisher_grad_vec = torch.randn(fisher_param_dim, device=device)
    else:
        print(f"   ğŸ“Š Computing critical slice gradient...")
        model.eval()
        critical_data, critical_targets = critical_data.to(device), critical_targets.to(device)
        
        # Compute loss on critical slice
        output = model(critical_data)
        critical_loss = F.cross_entropy(output, critical_targets)
        
        # Compute gradient
        model.zero_grad()
        critical_loss.backward()
        
        # Extract gradient only for Fisher parameters
        fisher_grad_vec = torch.cat([p.grad.view(-1) for p in fisher_params]).detach()
        print(f"   â€¢ Critical slice loss: {critical_loss.item():.4f}")
        print(f"   â€¢ Fisher gradient norm: {fisher_grad_vec.norm().item():.4f}")
    
    # Get public data influence vectors using Fisher preconditioning
    print(f"   ğŸ“Š Computing Fisher-preconditioned influence vectors...")
    
    public_gradients = []
    for batch_idx, batch_data in enumerate(pub_loader):
        if batch_idx >= eta:  # Limit to eta samples
            break
            
        if len(batch_data) == 3:
            x, y, _ = batch_data  # Handle user-level data
        else:
            x, y = batch_data
            
        x, y = x.to(device), y.to(device)
        
        # Compute per-sample gradients for this batch
        for i in range(x.size(0)):
            if len(public_gradients) >= eta:
                break
                
            model.zero_grad()
            sample_output = model(x[i:i+1])
            sample_loss = F.cross_entropy(sample_output, y[i:i+1])
            sample_loss.backward()
            
            # Extract gradient only for Fisher parameters
            sample_grad = torch.cat([p.grad.view(-1) for p in fisher_params]).detach()
            public_gradients.append(sample_grad)
        
        if len(public_gradients) >= eta:
            break
    
    print(f"   â€¢ Collected {len(public_gradients)} public sample gradients")
    
    if len(public_gradients) == 0:
        print(f"   âŒ No public gradients available, skipping calibration")
        return model
    
    # Stack public gradients
    G_public = torch.stack(public_gradients)  # [eta, fisher_param_dim]
    
    # Compute Fisher-preconditioned influence vectors
    print(f"   ğŸ¯ Computing influence vectors with Fisher preconditioning...")
    
    # Method: Use low-rank Fisher matrix for preconditioning
    # influence_vec = F^(-1) @ grad_vec where F^(-1) â‰ˆ U @ diag(1/Î») @ U^T
    
    # Project gradient to Fisher subspace
    grad_proj = U.T @ fisher_grad_vec  # [k]
    
    # Apply inverse eigenvalues (Fisher preconditioning)
    inv_lam = 1.0 / (lam + reg)  # Add regularization to prevent numerical issues
    influence_proj = grad_proj * inv_lam  # [k]
    
    # Project back to full space
    fisher_influence_vec = U @ influence_proj  # [fisher_param_dim]
    
    print(f"   â€¢ Fisher influence norm: {fisher_influence_vec.norm().item():.4f}")
    print(f"   â€¢ Regularized eigenvalue range: [{(lam + reg).min().item():.6f}, {(lam + reg).max().item():.6f}]")
    
    # Compute standard influence vectors for comparison
    print(f"   ğŸ“Š Computing standard influence vectors for comparison...")
    
    # Standard method: simple regularization
    G_public_reg = G_public.T @ G_public + reg * torch.eye(fisher_param_dim, device=device)
    
    # For computational efficiency with large matrices, use iterative solver
    try:
        # Try direct solve first (for smaller problems)
        if fisher_param_dim <= 10000:
            standard_influence_vec = torch.linalg.solve(G_public_reg, fisher_grad_vec)
        else:
            # Use conjugate gradient for larger problems
            print(f"   â€¢ Using iterative solver for large parameter space ({fisher_param_dim} params)")
            standard_influence_vec = _conjugate_gradient_solve(G_public_reg, fisher_grad_vec, max_iter=100)
    except:
        print(f"   âš ï¸  Standard method failed, using Fisher method only")
        standard_influence_vec = fisher_influence_vec
    
    print(f"   â€¢ Standard influence norm: {standard_influence_vec.norm().item():.4f}")
    
    # Compare influence vector magnitudes
    fisher_norm = fisher_influence_vec.norm().item()
    standard_norm = standard_influence_vec.norm().item()
    print(f"   â€¢ Fisher vs Standard ratio: {fisher_norm / (standard_norm + 1e-8):.3f}")
    
    # Apply trust region constraint
    influence_vec = fisher_influence_vec  # Use Fisher-preconditioned version
    
    # Trust region: limit parameter change magnitude (relative to Fisher parameters)
    fisher_param_norm = torch.cat([p.view(-1) for p in fisher_params]).norm()
    max_change = trust_tau * fisher_param_norm
    influence_norm = influence_vec.norm()
    
    if influence_norm > max_change:
        influence_vec = influence_vec * (max_change / influence_norm)
        print(f"   â€¢ Applied trust region: scaled by {max_change / influence_norm:.3f}")
    
    print(f"   â€¢ Final influence norm: {influence_vec.norm().item():.4f}")
    print(f"   â€¢ Parameter change magnitude: {influence_vec.norm().item() / fisher_param_norm.item() * 100:.3f}%")
    
    # Apply parameter update ONLY to Fisher parameters
    calibrated_model = copy.deepcopy(model)
    idx = 0
    for name, p in calibrated_model.named_parameters():
        if name in fisher_param_names:
            param_size = p.numel()
            p.data += influence_vec[idx:idx + param_size].view(p.shape)
            idx += param_size
    
    print(f"   âœ… Low-rank Fisher preconditioning calibration complete!")
    print(f"   â€¢ Updated {len(fisher_param_names)} parameter groups: {fisher_param_names}")
    
    return calibrated_model

def _conjugate_gradient_solve(A, b, max_iter=100, tol=1e-6):
    """Solve Ax = b using conjugate gradient method for large sparse systems."""
    x = torch.zeros_like(b)
    r = b - A @ x
    p = r.clone()
    rsold = r @ r
    
    for i in range(max_iter):
        Ap = A @ p
        alpha = rsold / (p @ Ap + 1e-8)
        x = x + alpha * p
        r = r - alpha * Ap
        rsnew = r @ r
        
        if torch.sqrt(rsnew) < tol:
            break
            
        beta = rsnew / (rsold + 1e-8)
        p = r + beta * p
        rsold = rsnew
    
    return x

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Synthetic users + batch sampler (reused from main.py)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class SyntheticUserDataset(torch.utils.data.Dataset):
    """Assigns each sample a user_id âˆˆ {0,â€¦,K-1} (round-robin)."""
    def __init__(self, base_ds, num_users, perm=None):
        self.base = base_ds
        if perm is None: perm = np.arange(len(base_ds))
        self.uid = torch.tensor(perm % num_users, dtype=torch.long)
    def __len__(self): return len(self.base)
    def __getitem__(self, idx):
        x,y = self.base[idx]
        return x, y, self.uid[idx].item()

class UserBatchSampler(Sampler):
    """Yield all indices of exactly ONE user per iteration."""
    def __init__(self, user_ids, shuffle=True):
        self.by_user = {}
        for idx,u in enumerate(user_ids):
            u_key = int(u)  # Convert to Python int to avoid numpy int64 issues
            self.by_user.setdefault(u_key, []).append(idx)
        self.uids = list(self.by_user.keys())
        self.shuffle = shuffle
    def __iter__(self):
        order = np.random.permutation(self.uids) if self.shuffle else self.uids
        for u in order: 
            u_key = int(u)  # Convert to Python int
            yield self.by_user[u_key]
    def __len__(self): return len(self.uids)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Helper functions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def get_device(args):
    if args.cpu: return torch.device('cpu')
    if args.mps and torch.backends.mps.is_available():
        print('Using MPS'); return torch.device('mps')
    if torch.cuda.is_available():
        idx = 0 if args.cuda_id is None else args.cuda_id
        print(f'Using CUDA:{idx}'); return torch.device(f'cuda:{idx}')
    print('Using CPU'); return torch.device('cpu')

def unpack_batch(batch_data):
    """Helper function to handle both (x, y) and (x, y, user_id) formats"""
    if len(batch_data) == 3:
        return batch_data[0], batch_data[1], batch_data[2]  # x, y, user_id
    else:
        return batch_data[0], batch_data[1], None  # x, y, None

def accuracy(model, loader, device):
    model.eval(); tot=correct=0
    with torch.no_grad():
        for batch_data in loader:
            x, y, _ = unpack_batch(batch_data)
            x,y = x.to(device), y.to(device)
            correct += (model(x).argmax(1)==y).sum().item()
            tot += y.size(0)
    return 100*correct/tot

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ABLATION: Fisher DP-SGD with Optional DP-SAT Optimization
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_fisher_dp_with_optimizer(model, train_loader, fisher,
                                  epsilon=8.0, delta=1e-6,
                                  clip_radius=10.0, k=32, lam_floor=5e-1,
                                  device="cuda", target_layer="conv1",
                                  adaptive_clip=True, quantile=0.95, sample_level=None,
                                  epochs=1, sigma=None, full_complement_noise=False,
                                  use_dp_sat=False, lambda_flatness=0.01,
                                  optimizer_name="Normal", positive_noise_correlation=False):
    """
    Fisher DP-SGD with optional DP-SAT optimization.
    
    This function explores the synergy between:
    1. Fisher-informed noise (curvature-aware)
    2. DP-SAT optimization (sharpness-aware)
    
    Args:
        clip_radius: Target Euclidean sensitivity Î”â‚‚ (same as vanilla DP-SGD for fair comparison).
                    This will be converted to appropriate Mahalanobis threshold internally.
        use_dp_sat: If True, apply DP-SAT flatness adjustment after Fisher noise
        lambda_flatness: Flatness coefficient for DP-SAT (only used if use_dp_sat=True)
        optimizer_name: String identifier for logging purposes
        positive_noise_correlation: If False (default), use negatively correlated noise (noise âˆ 1/âˆšÎ»).
                                   If True, use positively correlated noise (noise âˆ âˆšÎ»).
        
    Algorithm when use_dp_sat=True (CORRECTED):
        1. Compute per-sample gradients and clip them (standard DP-SGD)
        2. Add Fisher-informed noise â†’ g_fisher_priv
        3. Apply DP-SAT flatness adjustment: g_flat = Î» * g_{t-1}^{fisher_priv} / ||g_{t-1}^{fisher_priv}||_2
        4. Final update: Î¸ â† Î¸ - Î·(g_fisher_priv + g_flat)
    """
    model.train()
    opt = torch.optim.SGD(model.parameters(), lr=1e-3)
    
    # Fisher eigendecomposition
    lam, U = topk_eigh_with_floor(fisher, k=k, lam_floor=lam_floor)
    lam, U = lam.to(device), U.to(device)
    
    # Compute both scaling factors
    inv_sqrt_lam = lam.rsqrt()  # 1/âˆšÎ» (negatively correlated: less noise in high curvature)
    sqrt_lam = lam.sqrt()       # âˆšÎ» (positively correlated: more noise in high curvature)
    
    # Choose noise scaling strategy
    if positive_noise_correlation:
        noise_scaling = sqrt_lam
        strategy_name = "Positively correlated noise (noise âˆ âˆšÎ»)"
    else:
        noise_scaling = inv_sqrt_lam
        strategy_name = "Negatively correlated noise (noise âˆ 1/âˆšÎ», default)"
    
    # Clipping always uses inverse scaling to maintain consistent Mahalanobis norm definition
    clip_scaling = inv_sqrt_lam
    
    # Privacy accounting
    if sigma is not None:
        print(f"   â€¢ Using provided sigma: {sigma:.4f}")
    else:
        sigma_single_epoch = math.sqrt(2*math.log(1.25/delta)) / epsilon
        sigma = sigma_single_epoch / math.sqrt(epochs)
        print(f"   â€¢ Legacy accounting: Ïƒ_single={sigma_single_epoch:.3f}, Ïƒ_adjusted={sigma:.3f}")
    
    actual_k = len(lam)
    if actual_k != k:
        print(f"âš ï¸  Using k={actual_k} eigenpairs (requested {k}) due to matrix rank constraints")

    # Gather parameter objects
    if target_layer == "all":
        names = [n for n,_ in model.named_parameters()]
    elif "," in target_layer:
        layers = [s.strip() for s in target_layer.split(",")]
        names = [n for n,_ in model.named_parameters()
                 if any(l in n for l in layers)]
    else:
        names = [n for n,_ in model.named_parameters()
                 if target_layer in n]
    params = [dict(model.named_parameters())[n] for n in names]
    param_dim = sum(p.numel() for p in params)

    # Auto-detect DP mode if not specified
    if sample_level is None:
        first_batch = next(iter(train_loader))
        if len(first_batch) == 3:
            _, _, user_ids = first_batch
            unique_users = torch.unique(user_ids)
            sample_level = len(unique_users) > 1
        else:
            sample_level = True

    mode_str = "Sample-level" if sample_level else "User-level"
    opt_type = "DP-SAT" if use_dp_sat else "Normal"
    
    print(f"\nğŸ¯ Fisher DP + {optimizer_name} Optimizer: {mode_str} DP  layers={target_layer}  Îµ={epsilon}")
    if sigma is not None:
        print(f"   â€¢ Proper privacy accounting: Ïƒ={sigma:.4f}")
    else:
        print(f"   â€¢ Multi-epoch privacy: T={epochs}, Ïƒ_single={sigma_single_epoch:.3f}, Ïƒ_adjusted={sigma:.3f}")
    print(f"   â€¢ Fisher subspace: k={actual_k}, complement dim={param_dim-actual_k}")
    print(f"   â€¢ Target Euclidean sensitivity: Î”â‚‚ = {clip_radius:.3f} (will convert to Mahalanobis)")
    print(f"   â€¢ Full complement noise: {full_complement_noise}")
    print(f"   â€¢ Optimizer type: {opt_type}")
    print(f"   â€¢ Noise scaling strategy: {strategy_name}")
    if use_dp_sat:
        print(f"   â€¢ DP-SAT flatness coefficient Î»: {lambda_flatness:.4f}")
        print(f"   â€¢ âœ… CORRECTED: Uses previous step Fisher gradient for flatness")
    print(f"   â€¢ Adaptive clipping: {adaptive_clip}")
    
    if not sample_level:
        print("   â€¢ User-level mode: Clipping aggregated user gradients")
    else:
        print("   â€¢ Sample-level mode: Clipping individual sample gradients")
    print()

    noise_l2, grad_norm, flatness_norm = [], [], []
    euclidean_norms = []  # Track Euclidean norms for calibration
    adaptive_radius_computed = False
    euclidean_target = clip_radius  # Store the target Euclidean sensitivity
    actual_radius = clip_radius  # Will be calibrated to Mahalanobis space
    calibration_computed = False  # Flag for norm calibration
    
    # Initialize previous step's Fisher-noisy gradient for DP-SAT
    g_fisher_priv_prev = None

    for batch_idx, batch_data in enumerate(tqdm(train_loader, desc=f"Fisher DP + {opt_type} ({mode_str})")):
        # Accept (x,y) OR (x,y,uid)
        if len(batch_data) == 3:
            x, y, user_ids = batch_data
        else:
            x, y = batch_data
            user_ids = None
        x, y = x.to(device), y.to(device)

        model.zero_grad()
        losses = F.cross_entropy(model(x), y, reduction="none")

        if sample_level:
            # SAMPLE-LEVEL DP: Compute per-sample gradients
            per_g = []
            for i in range(x.size(0)):
                gi = grad(losses[i], params, retain_graph=True)
                per_g.append(torch.cat([g.view(-1) for g in gi]).detach())
            per_g = torch.stack(per_g)

            # Calibration or adaptive clipping: collect EUCLIDEAN norms (like vanilla DP-SGD)
            if (adaptive_clip and not adaptive_radius_computed) or not calibration_computed:
                batch_euclidean_norms = []
                batch_mahalanobis_norms = []
                
                for i in range(per_g.size(0)):
                    # Compute both norms for calibration
                    euclidean_norm = per_g[i].norm().item()
                    proj = (U.T @ per_g[i]) * clip_scaling
                    mahalanobis_norm = proj.norm().item()
                    
                    batch_euclidean_norms.append(euclidean_norm)
                    batch_mahalanobis_norms.append(mahalanobis_norm)
                
                euclidean_norms.extend(batch_euclidean_norms)
                
                # Adaptive clipping: use Euclidean norms (like vanilla DP-SGD)
                if adaptive_clip and not adaptive_radius_computed:
                    if len(euclidean_norms) >= 100 or batch_idx == 0:
                        euclidean_adaptive_radius = np.quantile(euclidean_norms, quantile)
                        euclidean_target = euclidean_adaptive_radius  # Update target
                        adaptive_radius_computed = True
                        
                        print(f"ğŸ“Š Fisher + {opt_type} adaptive clipping from {len(euclidean_norms)} samples (EUCLIDEAN norms):")
                        print(f"   â€¢ Mean Euclidean: {np.mean(euclidean_norms):.3f}")
                        print(f"   â€¢ {quantile:.1%} quantile: {euclidean_adaptive_radius:.3f}")
                        print(f"   â†’ Using Euclidean target: Î”â‚‚ = {euclidean_target:.3f}")
                
                # Calibration: find Mahalanobis threshold that matches Euclidean sensitivity
                if not calibration_computed and len(euclidean_norms) >= 50:
                    # Find what Mahalanobis threshold gives similar clipping behavior as Euclidean target
                    euclidean_clip_rate = np.mean(np.array(batch_euclidean_norms) > euclidean_target)
                    
                    # Binary search for Mahalanobis threshold that gives similar clip rate
                    maha_norms = np.array(batch_mahalanobis_norms)
                    maha_low, maha_high = maha_norms.min(), maha_norms.max()
                    
                    for _ in range(10):  # Binary search iterations
                        maha_mid = (maha_low + maha_high) / 2
                        maha_clip_rate = np.mean(maha_norms > maha_mid)
                        
                        if maha_clip_rate > euclidean_clip_rate:
                            maha_low = maha_mid
                        else:
                            maha_high = maha_mid
                    
                    actual_radius = (maha_low + maha_high) / 2
                    calibration_computed = True
                    
                    print(f"ğŸ¯ Ablation norm calibration completed:")
                    print(f"   â€¢ Target Euclidean sensitivity: Î”â‚‚ = {euclidean_target:.3f}")
                    print(f"   â€¢ Calibrated Mahalanobis threshold: {actual_radius:.3f}")
                    print(f"   â€¢ Euclidean clip rate: {euclidean_clip_rate:.1%}")
                    print(f"   â€¢ Mahalanobis clip rate: {np.mean(maha_norms > actual_radius):.1%}")
                    print(f"   â†’ Fair comparison: same effective sensitivity bound Î”â‚‚\n")
                    
                    euclidean_norms = []  # Reset for actual training statistics

            # Mahalanobis clipping with calibrated threshold
            for i in range(per_g.size(0)):
                per_g[i], nrm = maha_clip(per_g[i], U, clip_scaling, actual_radius)
                grad_norm.append(nrm)
            g_bar = per_g.mean(0)

        else:
            # USER-LEVEL DP: Compute gradient per user
            user_gradients = []
            unique_users = torch.unique(user_ids) if user_ids is not None else [0]
            
            for uid in unique_users:
                if user_ids is not None:
                    mask = (user_ids == uid)
                    user_losses = losses[mask]
                else:
                    user_losses = losses
                    mask = torch.ones_like(losses, dtype=torch.bool)
                
                user_total_loss = user_losses.sum()
                user_grad = grad(user_total_loss, params, retain_graph=True)
                user_grad_flat = torch.cat([g.view(-1) for g in user_grad]).detach()
                user_gradients.append(user_grad_flat)
                
                # Calibration or adaptive clipping: collect EUCLIDEAN norms
                if (adaptive_clip and not adaptive_radius_computed) or not calibration_computed:
                    euclidean_norm = user_grad_flat.norm().item()
                    euclidean_norms.append(euclidean_norm)
                    
                    if adaptive_clip and not adaptive_radius_computed:
                        if len(euclidean_norms) >= min(10, len(train_loader)) or batch_idx == 0:
                            euclidean_adaptive_radius = np.quantile(euclidean_norms, quantile)
                            euclidean_target = euclidean_adaptive_radius
                            adaptive_radius_computed = True
                            
                            print(f"ğŸ“Š Fisher + {opt_type} adaptive clipping from {len(euclidean_norms)} users (EUCLIDEAN norms):")
                            print(f"   â€¢ Mean Euclidean: {np.mean(euclidean_norms):.3f}")
                            print(f"   â€¢ {quantile:.1%} quantile: {euclidean_adaptive_radius:.3f}")
                            print(f"   â†’ Using Euclidean target: Î”â‚‚ = {euclidean_target:.3f}")
                    
                    # Calibration for user-level
                    if not calibration_computed and len(euclidean_norms) >= 5:
                        # Simple heuristic: use median ratio for calibration
                        proj = (U.T @ user_grad_flat) * inv_sqrt_lam
                        mahalanobis_norm = proj.norm().item()
                        ratio = euclidean_norm / (mahalanobis_norm + 1e-8)
                        actual_radius = euclidean_target / (ratio + 1e-8)
                        calibration_computed = True
                        
                        print(f"ğŸ¯ Ablation user-level norm calibration:")
                        print(f"   â€¢ Target Euclidean sensitivity: Î”â‚‚ = {euclidean_target:.3f}")
                        print(f"   â€¢ Calibrated Mahalanobis threshold: {actual_radius:.3f}")
                        print(f"   â€¢ Sample ratio: ||g||â‚‚/||g||_{{Fâ»Â¹}} â‰ˆ {ratio:.3f}\n")
                        
                        euclidean_norms = []  # Reset
            
            # Mahalanobis clipping for each user gradient
            clipped_user_grads = []
            for user_grad_flat in user_gradients:
                clipped_grad, user_norm = maha_clip(user_grad_flat, U, clip_scaling, actual_radius)
                grad_norm.append(user_norm)
                clipped_user_grads.append(clipped_grad)
            
            g_bar = torch.stack(clipped_user_grads).mean(0)

        # ============================================================
        # FISHER-INFORMED NOISE (Two-component)
        # ============================================================
        
        # 1. Low-rank noise in Fisher subspace (anisotropic)
        z_fisher = torch.randn(actual_k, device=device)
        fisher_noise = U @ (z_fisher * noise_scaling) * sigma * euclidean_target  # Use Euclidean target for noise scale
        
        if full_complement_noise:
            # 2. Complement noise in orthogonal subspace (isotropic)
            z_full = torch.randn_like(g_bar)
            z_complement = z_full - U @ (U.T @ z_full)  # Project to complement
            complement_noise = z_complement * sigma * euclidean_target  # Use Euclidean target
            total_noise = fisher_noise + complement_noise
            complement_noise_norm = complement_noise.norm().item()
        else:
            # Only Fisher subspace noise (preserves curvature-aware benefits)
            total_noise = fisher_noise
            complement_noise_norm = 0.0
        
        g_fisher_priv = g_bar + total_noise
        
        # Track noise components
        fisher_noise_norm = fisher_noise.norm().item()
        total_noise_norm = total_noise.norm().item()
        noise_l2.append(total_noise_norm)

        # ============================================================
        # OPTIONAL: DP-SAT OPTIMIZATION (Sharpness-Aware) - CORRECTED
        # ============================================================
        
        if use_dp_sat:
            if g_fisher_priv_prev is not None:
                # Apply DP-SAT flatness adjustment using PREVIOUS step's Fisher-noisy gradient
                g_fisher_priv_prev_norm = g_fisher_priv_prev.norm() + 1e-8
                g_flat = lambda_flatness * g_fisher_priv_prev / g_fisher_priv_prev_norm
                flatness_norm.append(g_flat.norm().item())
                
                # Final gradient: current Fisher-noisy + flatness adjustment from previous step
                g_final = g_fisher_priv + g_flat
            else:
                # First step: no previous gradient available, use standard Fisher DP
                g_final = g_fisher_priv
                flatness_norm.append(0.0)  # No flatness adjustment
                
            # Store current Fisher-noisy gradient for next iteration
            g_fisher_priv_prev = g_fisher_priv.clone().detach()
        else:
            # Standard Fisher DP: just use Fisher-noisy gradient
            g_final = g_fisher_priv
            flatness_norm.append(0.0)  # No flatness adjustment

        # Scatter back to model parameters
        idx = 0
        for p in params:
            n = p.numel()
            p.grad = g_final[idx:idx+n].view_as(p)
            idx += n
        opt.step()

    grad_type = "â€–g_userâ€–_Mah" if not sample_level else "â€–gâ€–_Mah"
    print(f"\nğŸ“Š  Fisher DP + {optimizer_name} final stats:")
    print(f"   â€¢ Target Euclidean sensitivity: Î”â‚‚ = {euclidean_target:.3f} (same as vanilla DP-SGD)")
    print(f"   â€¢ Calibrated Mahalanobis threshold: {actual_radius:.3f}")
    print(f"   â€¢ Median {grad_type} = {np.median(grad_norm):.2f}")
    print(f"   â€¢ Fisher noise â„“â‚‚ âˆˆ [{min(noise_l2):.1f},{max(noise_l2):.1f}]")
    if use_dp_sat and len(flatness_norm) > 0 and max(flatness_norm) > 0:
        print(f"   â€¢ Flatness adjustment â„“â‚‚ âˆˆ [{min(flatness_norm):.3f},{max(flatness_norm):.3f}]")
        print(f"   â€¢ Flatness coefficient Î» = {lambda_flatness:.4f}")
    if full_complement_noise:
        print(f"   â€¢ Last batch: Fisher={fisher_noise_norm:.1f}, Complement={complement_noise_norm:.1f}")
    else:
        print(f"   â€¢ Last batch: Fisher only={fisher_noise_norm:.1f} (complement disabled)")
    print(f"   â€¢ Privacy: (Îµ={epsilon}, Î´={delta}) over {epochs} epochs")
    print(f"   â€¢ âœ… FAIR COMPARISON: Same effective sensitivity Î”â‚‚ as vanilla DP-SGD")

    return model

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ablation Study Main Function
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_low_rank_fisher_test(args, device, priv_loader, eval_base, priv_base, priv_idx, priv_ds, Fmat, pub_loader):
    """
    Test low-rank Fisher preconditioning for influence function calibration.
    
    Comparison:
    1. Fisher DP + Standard Linear Calibration (Baseline)
    2. Fisher DP + Low-Rank Fisher Preconditioning Calibration (Enhanced)
    """
    
    print("\n" + "="*70)
    print("ğŸ”¬  LOW-RANK FISHER PRECONDITIONING TEST")
    print("="*70)
    
    # Load baseline model for initialization
    baseline = CNN().to(device)
    opt_b = torch.optim.SGD(baseline.parameters(), lr=1e-3, momentum=.9)
    
    print('\nâš™ï¸  Training baseline for initializationâ€¦')
    for epoch in tqdm(range(args.epochs)):
        baseline.train()
        for batch_data in priv_loader:
            if args.sample_level:
                x, y = batch_data
            else:
                x, y, _ = batch_data
            x, y = x.to(device), y.to(device)
            opt_b.zero_grad(); F.cross_entropy(baseline(x),y).backward(); opt_b.step()

    # Privacy accounting setup
    if not args.use_legacy_accounting:
        sample_rate = len(priv_loader) / len(priv_base)
        steps_per_epoch = len(priv_loader)
        
        noise_multiplier, total_steps = get_privacy_params_for_target_epsilon(
            target_epsilon=args.target_epsilon,
            target_delta=args.delta,
            sample_rate=sample_rate,
            epochs=args.epochs,
            steps_per_epoch=steps_per_epoch
        )
        
        sigma = noise_multiplier * args.clip_radius
        display_epsilon = args.target_epsilon
        
        print(f"\nğŸ”’ Privacy Accounting:")
        print(f"   â€¢ Target (Îµ, Î´): ({args.target_epsilon}, {args.delta})")
        print(f"   â€¢ Noise multiplier: {noise_multiplier:.4f}")
        print(f"   â€¢ Sigma: {sigma:.4f}")
    else:
        sigma = None
        display_epsilon = args.epsilon

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Train Fisher DP Model (Baseline for Calibration)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print(f"\n{'='*50}")
    print("ğŸ¯ Training Fisher DP Model")
    print(f"{'='*50}")
    
    fisher_dp_model = copy.deepcopy(baseline)
    fisher_dp_model = train_fisher_dp_with_optimizer(
        fisher_dp_model, priv_loader, Fmat,
        epsilon=display_epsilon, delta=args.delta,
        sigma=sigma,
        full_complement_noise=args.full_complement_noise,
        clip_radius=args.clip_radius,
        k=args.k, device=device,
        target_layer=args.dp_layer,
        adaptive_clip=args.adaptive_clip,
        quantile=args.quantile,
        sample_level=args.sample_level,
        epochs=args.epochs,
        use_dp_sat=False,  # Focus on calibration, not DP-SAT
        optimizer_name="Fisher DP",
        positive_noise_correlation=args.positive_noise_correlation
    )
    
    # Compute Fisher eigenpairs once for reuse in calibration
    print(f"\nğŸ“Š Computing Fisher eigenpairs for calibration (k={args.k})...")
    lam, U = topk_eigh_with_floor(Fmat, k=args.k, lam_floor=1e-3)
    lam, U = lam.to(device), U.to(device)
    actual_k = len(lam)
    print(f"   â€¢ Computed {actual_k} eigenpairs")
    print(f"   â€¢ Eigenvalue range: [{lam.min().item():.6f}, {lam.max().item():.6f}]")
    
    # Create evaluation loader for critical slice extraction and final accuracy measurement
    eval_loader = DataLoader(eval_base, batch_size=128, shuffle=False)
    
    # Get critical slice using EVALUATION data (eval_loader) for the slice-gradient
    critical_data, critical_targets = get_evaluation_slice(eval_loader, args.target_class, device=device)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Test 1: Standard Linear Calibration (Baseline)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print(f"\n{'='*50}")
    print("ğŸ“ TEST 1: Standard Linear Calibration (Baseline)")
    print(f"{'='*50}")
    
    standard_calibrated = copy.deepcopy(fisher_dp_model)
    
    if len(critical_data) > 0:
        before_stats = diagnose_calibration(standard_calibrated, critical_data, critical_targets, device)
        
        # Apply standard linear calibration
        standard_calibrated = calibrate_model_research_protocol(
            standard_calibrated, pub_loader, priv_loader,
            critical_data, critical_targets, device=device,
            method='linear',  # Force linear method for fair comparison
            eta=args.calibration_k,
            trust_tau=args.trust_tau,
            strict=True,
            clean_model=baseline,
            reg=args.reg
        )
        
        after_stats = diagnose_calibration(standard_calibrated, critical_data, critical_targets, device)
        print_calibration_effect(before_stats, after_stats, args.target_class)
    else:
        print(f"âš ï¸  No samples found for target_class {args.target_class}")
        # Still apply calibration without diagnosis
        standard_calibrated = calibrate_model_research_protocol(
            standard_calibrated, pub_loader, priv_loader,
            critical_data, critical_targets, device=device,
            method='linear',
            eta=args.calibration_k,
            trust_tau=args.trust_tau,
            strict=True,
            clean_model=baseline,
            reg=args.reg
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Test 2: Low-Rank Fisher Preconditioning Calibration (Enhanced)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print(f"\n{'='*50}")
    print("ğŸ¯ TEST 2: Low-Rank Fisher Preconditioning Calibration (Enhanced)")
    print(f"{'='*50}")
    
    fisher_preconditioned = copy.deepcopy(fisher_dp_model)
    
    if len(critical_data) > 0:
        before_stats_fp = diagnose_calibration(fisher_preconditioned, critical_data, critical_targets, device)
        
        # Apply low-rank Fisher preconditioning calibration
        fisher_preconditioned = calibrate_with_low_rank_fisher(
            fisher_preconditioned, pub_loader, priv_loader,
            critical_data, critical_targets, device=device,
            lam=lam, U=U,
            eta=args.calibration_k,
            trust_tau=args.trust_tau,
            reg=args.reg,
            strict=True,
            clean_model=baseline,
            target_layer=args.dp_layer
        )
        
        after_stats_fp = diagnose_calibration(fisher_preconditioned, critical_data, critical_targets, device)
        print_calibration_effect(before_stats_fp, after_stats_fp, args.target_class)
    else:
        print(f"âš ï¸  No samples found for target_class {args.target_class}")
        # Still apply calibration without diagnosis
        fisher_preconditioned = calibrate_with_low_rank_fisher(
            fisher_preconditioned, pub_loader, priv_loader,
            critical_data, critical_targets, device=device,
            lam=lam, U=U,
            eta=args.calibration_k,
            trust_tau=args.trust_tau,
            reg=args.reg,
            strict=True,
            clean_model=baseline,
            target_layer=args.dp_layer
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Evaluation and Comparison
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print(f"\n{'='*70}")
    print("ğŸ“Š LOW-RANK FISHER PRECONDITIONING RESULTS")
    print(f"{'='*70}")
    
    # Compute accuracies
    baseline_acc = accuracy(baseline, eval_loader, device)
    fisher_dp_acc = accuracy(fisher_dp_model, eval_loader, device)
    standard_acc = accuracy(standard_calibrated, eval_loader, device)
    fisher_preconditioned_acc = accuracy(fisher_preconditioned, eval_loader, device)
    
    dp_mode = "Sample-level" if args.sample_level else f"User-level ({args.users} users)"
    print(f"\nğŸ¯ Accuracy Comparison ({dp_mode} DP):")
    print(f"   â€¢ Baseline (Non-DP)                : {baseline_acc:6.2f}%")
    print(f"   â€¢ Fisher DP (No Calibration)       : {fisher_dp_acc:6.2f}%")
    print(f"   â€¢ Fisher DP + Standard Linear      : {standard_acc:6.2f}%")
    print(f"   â€¢ Fisher DP + Fisher Preconditioning: {fisher_preconditioned_acc:6.2f}%")
    
    # Compute improvements
    fisher_improvement = fisher_dp_acc - baseline_acc
    standard_improvement = standard_acc - fisher_dp_acc
    fisher_preconditioned_improvement = fisher_preconditioned_acc - fisher_dp_acc
    fisher_vs_standard = fisher_preconditioned_acc - standard_acc
    
    print(f"\nğŸ“ˆ Improvement Analysis:")
    print(f"   â€¢ Fisher DP benefit:               {fisher_improvement:+5.2f}% vs baseline")
    print(f"   â€¢ Standard calibration:            {standard_improvement:+5.2f}% vs Fisher DP")
    print(f"   â€¢ Fisher preconditioning:          {fisher_preconditioned_improvement:+5.2f}% vs Fisher DP")
    print(f"   â€¢ Fisher vs Standard calibration:  {fisher_vs_standard:+5.2f}% improvement")
    
    print(f"\nğŸ¯ Fisher Preconditioning Effectiveness:")
    if fisher_vs_standard > 0.5:
        print(f"   âœ… SIGNIFICANT: Fisher preconditioning provides meaningful improvement (+{fisher_vs_standard:.2f}%)")
    elif fisher_vs_standard > 0.1:
        print(f"   âœ… MODEST: Fisher preconditioning provides small improvement (+{fisher_vs_standard:.2f}%)")
    elif fisher_vs_standard > 0:
        print(f"   âš ï¸  MINIMAL: Fisher preconditioning provides tiny improvement (+{fisher_vs_standard:.2f}%)")
    else:
        print(f"   âŒ NO BENEFIT: Fisher preconditioning does not improve over standard ({fisher_vs_standard:.2f}%)")
    
    # Identify best method
    best_acc = max(standard_acc, fisher_preconditioned_acc)
    best_method = "Fisher Preconditioning" if fisher_preconditioned_acc >= standard_acc else "Standard Linear"
    
    print(f"\nğŸ† Best Calibration Method: {best_method} ({best_acc:.2f}%)")
    
    # Computational analysis
    total_params = sum(p.numel() for p in fisher_dp_model.parameters())
    print(f"\nğŸ’» Computational Analysis:")
    print(f"   â€¢ Total parameters: {total_params:,}")
    print(f"   â€¢ Fisher rank used: {args.k}")
    print(f"   â€¢ Compression ratio: {total_params / args.k:.1f}x")
    print(f"   â€¢ Memory efficiency: Fisher method uses ~{args.k * 2} floats vs ~{total_params**2 // 1000000}M for full matrix")
    
    if total_params > 100000:
        print(f"   ğŸš€ SCALABILITY: Fisher preconditioning essential for large models")
    elif total_params > 10000:
        print(f"   âš¡ EFFICIENCY: Fisher preconditioning provides computational benefits")
    else:
        print(f"   ğŸ“Š COMPARISON: Both methods computationally feasible for this model size")
    
    # Quality analysis
    print(f"\nğŸ” Calibration Quality Analysis:")
    print(f"   â€¢ Fisher preconditioning leverages loss curvature information")
    print(f"   â€¢ Standard method uses simple regularization")
    print(f"   â€¢ Fisher method should be more principled for influence estimation")
    
    if fisher_vs_standard > 0.1:
        print(f"   âœ… QUALITY ADVANTAGE: Fisher preconditioning shows clear benefit")
    elif fisher_vs_standard > 0:
        print(f"   âœ… SLIGHT ADVANTAGE: Fisher preconditioning shows modest benefit")
    else:
        print(f"   âš ï¸  NO QUALITY ADVANTAGE: Standard method performs as well")
    
    # Save models for further analysis
    print(f"\nğŸ’¾ Saving models...")
    
    # Save standard calibrated model
    standard_path = os.path.join(models_dir, 'Standard_Linear_Calibrated_Test.pth')
    torch.save({
        'model_state_dict': standard_calibrated.state_dict(),
        'model_type': 'fisher_dp_standard_calibrated',
        'accuracy': standard_acc,
        'improvement_vs_fisher': standard_improvement,
        'calibration_method': 'linear',
        'calibration_k': args.calibration_k,
        'trust_tau': args.trust_tau,
        'reg': args.reg,
        'fisher_preconditioning_test': True
    }, standard_path)
    print(f"âœ… Saved Standard Linear Calibrated to {standard_path}")
    
    # Save Fisher preconditioned model
    fisher_preconditioned_path = os.path.join(models_dir, 'Fisher_Preconditioned_Calibrated_Test.pth')
    torch.save({
        'model_state_dict': fisher_preconditioned.state_dict(),
        'model_type': 'fisher_dp_fisher_preconditioned_calibrated',
        'accuracy': fisher_preconditioned_acc,
        'improvement_vs_fisher': fisher_preconditioned_improvement,
        'improvement_vs_standard': fisher_vs_standard,
        'calibration_method': 'fisher_preconditioning',
        'calibration_k': args.calibration_k,
        'trust_tau': args.trust_tau,
        'reg': args.reg,
        'fisher_k': args.k,
        'total_params': total_params,
        'compression_ratio': total_params / args.k,
        'fisher_preconditioning_test': True
    }, fisher_preconditioned_path)
    print(f"âœ… Saved Fisher Preconditioned Calibrated to {fisher_preconditioned_path}")

    # Return results for potential further analysis
    return {
        'baseline_acc': baseline_acc,
        'fisher_dp_acc': fisher_dp_acc,
        'standard_acc': standard_acc,
        'fisher_preconditioned_acc': fisher_preconditioned_acc,
        'fisher_vs_standard_improvement': fisher_vs_standard,
        'best_method': best_method,
        'total_params': total_params,
        'compression_ratio': total_params / args.k
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Main function
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser('Low-Rank Fisher Preconditioning Test for Fisher DP-SGD')
    
    # Device arguments
    parser.add_argument('--mps', action='store_true')
    parser.add_argument('--cuda-id', type=int)
    parser.add_argument('--cpu', action='store_true')
    
    # Data arguments
    parser.add_argument('--dataset-size', type=int, default=10000)
    parser.add_argument('--private-ratio', type=float, default=0.8)
    parser.add_argument('--epochs', type=int, default=10)
    
    # Clean up
    parser.add_argument('--clean', action='store_true',
                       help='Remove all saved models before training')
    
    # Privacy arguments
    privacy_group = parser.add_mutually_exclusive_group()
    privacy_group.add_argument('--use-legacy-accounting', action='store_true',
                              help='Use legacy privacy accounting (not recommended)')
    
    epsilon_group = parser.add_mutually_exclusive_group()
    epsilon_group.add_argument('--epsilon', type=float, default=None,
                              help='Privacy epsilon (legacy accounting only)')
    epsilon_group.add_argument('--target-epsilon', type=float, default=None,
                              help='Target epsilon for DP (proper accounting)')
    
    parser.add_argument('--delta', type=float, default=1e-5)
    parser.add_argument('--clip-radius', type=float, default=1.0)
    
    # Fisher DP arguments
    parser.add_argument('--k', type=int, default=64)
    parser.add_argument('--dp-layer', type=str, default='conv1')
    parser.add_argument('--full-complement-noise', action='store_true',
                       help='Use full complement noise in orthogonal subspace')
    
    # Fisher DP noise scaling strategy  
    noise_strategy_group = parser.add_mutually_exclusive_group()
    noise_strategy_group.add_argument('--negatively-correlated-noise', action='store_true', default=True,
                                     help='Fisher DP: noise inversely correlated with curvature (noise âˆ 1/âˆšÎ», less noise in high curvature directions, default)')
    noise_strategy_group.add_argument('--positively-correlated-noise', action='store_true',
                                     help='Fisher DP: noise positively correlated with curvature (noise âˆ âˆšÎ», more noise in high curvature directions)')
    
    # Adaptive clipping
    parser.add_argument('--adaptive-clip', action='store_true')
    parser.add_argument('--quantile', type=float, default=0.95)
    
    # DP mode
    parser.add_argument('--sample-level', action='store_true')
    parser.add_argument('--users', type=int, default=10)
    
    # Standard calibration arguments
    parser.add_argument('--calibration-k', type=int, default=100,
                       help='Number of top-k samples to use for calibration')
    parser.add_argument('--trust-tau', type=float, default=0.005,
                       help='Trust region parameter: max relative parameter change (default: 0.005 = 0.5%%)')
    parser.add_argument('--reg', type=float, default=50.0,
                       help='Regularization parameter for influence vectors (default: 50.0 for stability)')
    parser.add_argument('--target-class', default="all",
                       help='Target class for calibration: "all" for general utility (default), or integer for specific class')
    
    # MIA evaluation arguments
    parser.add_argument('--run-mia', action='store_true',
                       help='Run membership inference attack evaluation on calibrated models')
    parser.add_argument('--mia-size', type=int, default=1000,
                       help='Number of samples for MIA evaluation (default: 1000)')
    
    args = parser.parse_args()
    
    # Map new argument names for consistency  
    args.positive_noise_correlation = getattr(args, 'positively_correlated_noise', False)
    
    # Parse target_class argument - can be "all" or integer
    if args.target_class == "all":
        print(f"ğŸ“Š Using ALL CLASSES for calibration (general utility improvement)")
    else:
        try:
            args.target_class = int(args.target_class)
            print(f"ğŸ“Š Using target class {args.target_class} for calibration (targeted improvement)")
        except ValueError:
            print(f"âŒ Error: target_class must be 'all' or an integer, got '{args.target_class}'")
            exit(1)
    
    # Validate privacy parameters
    if args.use_legacy_accounting:
        if args.epsilon is None:
            print("âŒ Error: --use-legacy-accounting requires --epsilon parameter")
            exit(1)
        if args.target_epsilon is not None:
            print("âŒ Error: Cannot use --target-epsilon with --use-legacy-accounting")
            exit(1)
    else:
        if args.epsilon is not None:
            print("âŒ Error: --epsilon is only for legacy accounting")
            exit(1)
        if args.target_epsilon is None:
            args.target_epsilon = 10.0
    
    device = get_device(args)
    
    # Clean up if requested
    if args.clean:
        print('Cleaning saved modelsâ€¦')
        for f in glob.glob(os.path.join(models_dir,'*Test*.pth')):
            os.remove(f); print('  removed',f)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Data preparation - FIXED: Match main.py's proper 3-way split
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    trans = T.Compose([T.ToTensor(), T.Normalize((.5,.5,.5),(.5,.5,.5))])
    trainset = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=trans)
    testset = torchvision.datasets.CIFAR10('./data', train=False, download=True, transform=trans)
    
    # Data partitioning for differential privacy (MATCHING main.py):
    # - priv_base: From trainset - used for baseline training, DP training
    # - pub_base: From testset subset - used for calibration (public data)  
    # - eval_base: From testset subset - used for evaluation (independent)
    
    # Private data: use subset of trainset for training
    perm_train = np.random.permutation(min(args.dataset_size, len(trainset)))
    priv_idx = perm_train[:min(args.dataset_size, len(trainset))]
    priv_base = Subset(trainset, priv_idx)  # Training data from trainset
    
    # Public data: use subset of testset for calibration
    pub_ratio = 0.5  # Use 50% of testset for calibration, 50% for evaluation
    perm_test = np.random.permutation(len(testset))
    pub_split = int(len(testset) * pub_ratio)
    pub_idx, eval_idx = perm_test[:pub_split], perm_test[pub_split:]
    
    pub_base = Subset(testset, pub_idx)    # Calibration data from testset
    eval_base = Subset(testset, eval_idx)  # Evaluation data from testset
    
    print(f'ğŸ“Š Low-Rank Fisher Preconditioning Test Data:')
    print(f'   â€¢ Private data: {len(priv_base)} samples from CIFAR-10 trainset (for training)')
    print(f'   â€¢ Public data: {len(pub_base)} samples from CIFAR-10 testset (for calibration)')
    print(f'   â€¢ Evaluation data: {len(eval_base)} samples from CIFAR-10 testset (for evaluation)')
    print(f'   ğŸ”§ Proper 3-way split - no circularity, no domain mismatch!')
    
    # Setup data loaders based on DP mode
    if args.sample_level:
        print('ğŸ“Š Using SAMPLE-level DP')
        priv_loader = DataLoader(priv_base, batch_size=128, shuffle=True)
        priv_ds = None
        # Public loader for calibration (sample-level)
        pub_loader = DataLoader(pub_base, batch_size=128, shuffle=False)
    else:
        print(f'ğŸ‘¥ Using USER-level DP ({args.users} synthetic users)')
        priv_ds = SyntheticUserDataset(priv_base, args.users)
        priv_loader = DataLoader(priv_ds, batch_sampler=UserBatchSampler(priv_ds.uid))
        # Public loader for calibration (user-level) - use same synthetic user structure
        pub_ds = SyntheticUserDataset(pub_base, args.users)
        pub_loader = DataLoader(pub_ds, batch_size=128, shuffle=False)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Fisher matrix computation
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print('\nğŸ” Computing Fisher matrix for low-rank preconditioning testâ€¦')
    
    # Train a baseline model for Fisher computation
    fisher_baseline = CNN().to(device)
    fisher_opt = torch.optim.SGD(fisher_baseline.parameters(), lr=1e-3, momentum=.9)
    
    for epoch in tqdm(range(5), desc="Training Fisher baseline"):  # Fewer epochs for Fisher
        fisher_baseline.train()
        for batch_data in priv_loader:
            if args.sample_level:
                x, y = batch_data
            else:
                x, y, _ = batch_data
            x, y = x.to(device), y.to(device)
            fisher_opt.zero_grad()
            F.cross_entropy(fisher_baseline(x),y).backward()
            fisher_opt.step()
    
    Fmat, _ = compute_fisher(fisher_baseline, priv_loader, device,
                            target_layer=args.dp_layer, rho=1e-2)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Run low-rank Fisher preconditioning test
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    results = run_low_rank_fisher_test(args, device, priv_loader, eval_base, priv_base, 
                                      priv_idx, priv_ds, Fmat, pub_loader)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Final summary
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print(f"\n{'='*70}")
    print("ğŸ¯ LOW-RANK FISHER PRECONDITIONING TEST SUMMARY")
    print(f"{'='*70}")
    
    print(f"ğŸ¯ Fisher Preconditioning Configuration:")
    print(f"   â€¢ Fisher rank: k={args.k}")
    print(f"   â€¢ Calibration samples (Î·): {args.calibration_k}")
    print(f"   â€¢ Trust region (Ï„): {args.trust_tau}")
    print(f"   â€¢ Regularization: {args.reg}")
    print(f"   â€¢ Total parameters: {results['total_params']:,}")
    print(f"   â€¢ Compression ratio: {results['compression_ratio']:.1f}x")
    
    print(f"\nğŸ“Š Results Comparison:")
    print(f"   â€¢ Fisher DP (No Calibration):       {results['fisher_dp_acc']:6.2f}%")
    print(f"   â€¢ Standard Linear Calibration:      {results['standard_acc']:6.2f}%")
    print(f"   â€¢ Fisher Preconditioning:           {results['fisher_preconditioned_acc']:6.2f}%")
    
    improvement = results['fisher_vs_standard_improvement']
    print(f"\nğŸ¯ Fisher Preconditioning Effectiveness: {improvement:+.2f}%")
    
    if improvement > 0.5:
        print(f"   âœ… EXCELLENT: Fisher preconditioning provides significant benefit!")
        print(f"   ğŸ’¡ Recommendation: Use Fisher preconditioning for influence function calibration")
    elif improvement > 0.1:
        print(f"   âœ… GOOD: Fisher preconditioning provides modest benefit")
        print(f"   ğŸ’¡ Recommendation: Fisher preconditioning may be worth the added complexity")
    elif improvement > 0:
        print(f"   âš ï¸  MARGINAL: Fisher preconditioning provides small benefit")
        print(f"   ğŸ’¡ Recommendation: Standard linear method may be sufficient")
    else:
        print(f"   âŒ NO BENEFIT: Fisher preconditioning does not help")
        print(f"   ğŸ’¡ Recommendation: Stick with standard linear calibration")
    
    print(f"\nğŸ† Best Method: {results['best_method']}")
    
    print(f"\nğŸ’» Computational Insights:")
    if results['total_params'] > 100000:
        print(f"   ğŸš€ SCALABILITY: Fisher preconditioning essential for large models")
        print(f"   â€¢ Full matrix would require ~{results['total_params']**2 // 1000000}M floats")
        print(f"   â€¢ Fisher method uses only ~{args.k * 2} floats")
        print(f"   â€¢ Memory reduction: {results['total_params']**2 // (args.k * 2 * 1000):.0f}kÃ— smaller")
    elif results['total_params'] > 10000:
        print(f"   âš¡ EFFICIENCY: Fisher preconditioning provides computational benefits")
        print(f"   â€¢ Avoids expensive full matrix operations")
        print(f"   â€¢ Uses low-rank approximation with {results['compression_ratio']:.1f}Ã— compression")
    else:
        print(f"   ğŸ“Š COMPARISON: Both methods computationally feasible for this model size")
        print(f"   â€¢ Fisher method still provides theoretical advantages")
    
    print(f"\nğŸ” Quality Analysis:")
    print(f"   â€¢ Fisher preconditioning leverages loss curvature information")
    print(f"   â€¢ Provides more principled influence function estimation")
    print(f"   â€¢ Should be more accurate for identifying influential samples")
    
    if improvement > 0.1:
        print(f"   âœ… QUALITY ADVANTAGE: Clear benefit from curvature-aware preconditioning")
    elif improvement > 0:
        print(f"   âœ… SLIGHT ADVANTAGE: Modest benefit from Fisher information")
    else:
        print(f"   âš ï¸  NO QUALITY ADVANTAGE: Standard regularization performs as well")
    
    print(f"\nğŸ“ Experimental Conclusion:")
    if improvement > 0.3:
        print(f"   ğŸ‰ STRONG EVIDENCE: Fisher preconditioning significantly improves calibration")
    elif improvement > 0.1:
        print(f"   âœ… MODERATE EVIDENCE: Fisher preconditioning provides measurable improvement")
    elif improvement > 0:
        print(f"   âš ï¸  WEAK EVIDENCE: Fisher preconditioning provides minimal improvement")
    else:
        print(f"   âŒ NO EVIDENCE: Fisher preconditioning does not improve over standard method")
    
    print(f"\nğŸ”¬ Theoretical Implications:")
    print(f"   â€¢ Fisher information matrix captures loss landscape curvature")
    print(f"   â€¢ Low-rank approximation preserves most important directions")
    print(f"   â€¢ Preconditioning should improve influence function accuracy")
    print(f"   â€¢ Results {'support' if improvement > 0 else 'do not support'} theoretical expectations")
    
    print(f"\nâœ… Low-rank Fisher preconditioning test complete! Models saved in {models_dir}/")

if __name__ == "__main__":
    main() 