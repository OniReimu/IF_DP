# influence_function.py
# ==============================================================
# Curvature-aware DP -- Influence-function calibration utilities
# ==============================================================

import torch, numpy as np, cvxpy as cp, copy
import torch.nn.functional as F
from tqdm import tqdm
from data.common import move_to_device
from config import get_logger

logger = get_logger("calibration")

# --------------------------------------------------------------
# 0.  Small helpers
# --------------------------------------------------------------
def unpack_batch(batch_data):
    """
    Accept dict/tuple/list batches and return (x, y, user_id or None).
    Mirrors data.common.unpack_batch so calibration works with text loaders.
    """
    if isinstance(batch_data, dict):
        labels = batch_data.get("labels")
        if labels is None and "label" in batch_data:
            labels = batch_data["label"]
        user_ids = batch_data.get("user_ids")
        feature_keys = [k for k in batch_data.keys() if k not in {"labels", "label", "user_ids"}]
        if not feature_keys:
            raise ValueError("Dictionary batch must contain feature fields")
        if len(feature_keys) == 1:
            features = batch_data[feature_keys[0]]
        else:
            features = {k: batch_data[k] for k in feature_keys}
        return features, labels, user_ids

    if isinstance(batch_data, (tuple, list)):
        if len(batch_data) < 2:
            raise ValueError("Batch must contain at least (x, y)")
        batch_tuple = tuple(batch_data) + (None,)
        return batch_tuple[0], batch_tuple[1], batch_tuple[2]

    raise ValueError(f"Unsupported batch type: {type(batch_data)}")


def _infer_batch_size(features):
    """Best-effort batch size detection for tensor/dict/list feature containers."""
    if torch.is_tensor(features):
        return features.size(0)
    if isinstance(features, dict):
        for value in features.values():
            return _infer_batch_size(value)
        raise ValueError("Dictionary batch must contain at least one feature tensor")
    if isinstance(features, (list, tuple)):
        if not features:
            raise ValueError("Empty feature container")
        first = features[0]
        if torch.is_tensor(first) or isinstance(first, (dict, list, tuple)):
            return _infer_batch_size(first)
        return len(features)
    raise ValueError(f"Cannot infer batch size from type {type(features)}")


def _slice_feature(features, idx):
    """Return idx-th sample from tensor/dict/list feature container."""
    if torch.is_tensor(features):
        return features[idx:idx+1]
    if isinstance(features, dict):
        return {k: _slice_feature(v, idx) for k, v in features.items()}
    if isinstance(features, list):
        if not features:
            raise ValueError("Empty feature list")
        first = features[0]
        if torch.is_tensor(first) or isinstance(first, (dict, list, tuple)):
            return [_slice_feature(v, idx) for v in features]
        return features[idx]
    if isinstance(features, tuple):
        if not features:
            raise ValueError("Empty feature tuple")
        first = features[0]
        if torch.is_tensor(first) or isinstance(first, (dict, list, tuple)):
            return tuple(_slice_feature(v, idx) for v in features)
        return features[idx]
    raise ValueError(f"Unsupported feature type for slicing: {type(features)}")


def _ensure_tensor(values):
    if torch.is_tensor(values):
        return values
    return torch.as_tensor(values)

# --------------------------------------------------------------
# 1.  Critical slice extraction + gradients
# --------------------------------------------------------------
def get_evaluation_slice(eval_loader, target_class="all", max_samples_per_class=200, device="cpu"):
    """
    Extract evaluation slice for calibration.
    
    Paper mapping â€” Step a (Critical slice):
    S_crit = {all classes in the public test split} when target_class == "all".
    
    Args:
        target_class: 
            - "all": Use all classes (recommended for general utility improvement)
            - int: Use specific class only (for targeted fairness applications)
        max_samples_per_class: Maximum samples per class to avoid memory issues
    """
    if target_class == "all":
        logger.info("Using ALL classes for calibration (general utility improvement).")
        
        # Collect samples by class for balanced sampling
        class_samples = {}
        
        for batch_data in eval_loader:
            x, y, _ = unpack_batch(batch_data)
            
            for i in range(len(x)):
                class_id = y[i].item()
                if class_id not in class_samples:
                    class_samples[class_id] = {'x': [], 'y': []}
                
                # Add sample if we haven't reached the limit for this class
                if len(class_samples[class_id]['x']) < max_samples_per_class:
                    class_samples[class_id]['x'].append(x[i:i+1])
                    class_samples[class_id]['y'].append(y[i:i+1])
        
        # Combine all classes
        all_x, all_y = [], []
        total_samples = 0
        
        for class_id in sorted(class_samples.keys()):
            class_x = torch.cat(class_samples[class_id]['x'])
            class_y = torch.cat(class_samples[class_id]['y'])
            all_x.append(class_x)
            all_y.append(class_y)
            total_samples += len(class_x)
            logger.info("   â€¢ Class %s: %s samples", class_id, len(class_x))
        
        if not all_x:
            logger.warn("No samples found in evaluation data.")
            return torch.empty(0, 3, 32, 32, device=device), \
                   torch.empty(0, dtype=torch.long, device=device)
        
        eval_x = torch.cat(all_x).to(device)
        eval_y = torch.cat(all_y).to(device)
        
        logger.success("Evaluation slice: %s samples across %s classes", total_samples, len(class_samples))
        
    else:
        # Single class mode (legacy behavior)
        logger.info("Using SINGLE CLASS %s for calibration (targeted improvement).", target_class)
        
        eval_x, eval_y = [], []
        for batch_data in eval_loader:
            x, y, _ = unpack_batch(batch_data)
                
            m = y == target_class
            if m.any():
                eval_x.append(x[m])
                eval_y.append(y[m])
        
        if not eval_x:
            logger.warn("No samples of class %s", target_class)
            return torch.empty(0, 3, 32, 32, device=device), \
                   torch.empty(0, dtype=torch.long, device=device)
        
        eval_x = torch.cat(eval_x).to(device)
        eval_y = torch.cat(eval_y).to(device)
        logger.success("Evaluation slice: %s samples of class %s", len(eval_x), target_class)
    
    return eval_x, eval_y


# Keep old function name for backward compatibility but mark as deprecated
def get_critical_slice(eval_loader, target_class: int = 3, device="cpu"):
    """
    âš ï¸  DEPRECATED: This function is deprecated and causes overfitting to single class.
    Use get_evaluation_slice with target_class="all" for general utility improvement.
    """
    logger.warn("get_critical_slice is deprecated and causes single-class overfitting.")
    logger.warn("Recommendation: use get_evaluation_slice(target_class='all') instead.")
    return get_evaluation_slice(eval_loader, target_class=target_class, device=device)


@torch.no_grad()
def _frob_norm(tensor_dict):
    return torch.sqrt(
        sum(v.pow(2).sum() for v in tensor_dict.values())
    )


def compute_slice_gradient(model, crit_x, crit_y, device):
    """J = âˆ‡_Î¸ 1/m Î£ â„“(s,Î¸)  on critical slice."""
    if not len(crit_x):
        return {n: torch.zeros_like(p.data) for n, p in model.named_parameters()}
    model.zero_grad()
    F.cross_entropy(model(crit_x), crit_y, reduction='mean').backward()
    return {n: (p.grad.detach().clone() if p.grad is not None else torch.zeros_like(p))
            for n, p in model.named_parameters()}
import heapq
# --------------------------------------------------------------
# 2.  Influence-vector bank  Hâ»Â¹ âˆ‡â„“(z)
# --------------------------------------------------------------

def compute_top_influence_vectors(model, public_loader, J_flat, J_keys, device, eta=200, reg=10.0):
    """
    ðŸ”’ PRIVACY-PRESERVING: Compute influence vectors using ONLY public data and DP model.
    """
  
    top_heap = []
    model.eval()
    J_flat_device = J_flat.to(device)
    
    # Counter serves as a unique tie-breaker
    push_count = 0 

    # éåŽ† public æ•°æ®
    for batch_data in tqdm(public_loader, desc="Top-K Influence Mining"):
        x, y, _ = unpack_batch(batch_data)
        x = move_to_device(x, device)
        y = _ensure_tensor(y).to(device)
        batch_size = _infer_batch_size(x)

        for i in range(batch_size):
            sample_x = _slice_feature(x, i)
            sample_y = y[i:i+1]
            
            # --- 1. è®¡ç®—å•æ ·æœ¬æ¢¯åº¦ ---
            model.zero_grad()
            loss = F.cross_entropy(model(sample_x), sample_y)
            loss.backward()
            
            vec_parts = []
            vec_dict = {}
            for n, p in model.named_parameters():
                if n in J_keys:
                    if p.grad is not None:
                        g = p.grad.detach()
                        g_norm = g.norm() + 1e-8
                        v = g / ((1.0 + g_norm) * reg)
                        vec_dict[n] = v.cpu() 
                        vec_parts.append(v.flatten())
                    else:
                        vec_dict[n] = torch.zeros_like(p).cpu()
                        vec_parts.append(torch.zeros_like(p).flatten().to(device))

            # --- 2. è®¡ç®— Score ---
            v_flat = torch.cat(vec_parts)
            score = -torch.dot(J_flat_device, v_flat).item()

            # --- 3. ç»´æŠ¤å † (Top-eta) ---
            # ä½¿ç”¨ push_count ä½œä¸º tie-breaker
            # Tuple ç»“æž„: (score, push_count, vec_dict)
            if len(top_heap) < eta:
                heapq.heappush(top_heap, (score, push_count, vec_dict))
            else:
                # å †é¡¶æ˜¯æœ€å°çš„ (Min-Heap)ï¼Œå¦‚æžœå½“å‰ score æ¯”å †é¡¶å¤§ï¼Œåˆ™æ›¿æ¢
                if score > top_heap[0][0]:
                    heapq.heapreplace(top_heap, (score, push_count, vec_dict))
            
            push_count += 1
                    
    # æŒ‰åˆ†æ•°ä»Žå¤§åˆ°å°æŽ’åºè¿”å›ž
    top_heap.sort(key=lambda x: x[0], reverse=True)
    
    final_scores = [item[0] for item in top_heap]
    # æ³¨æ„ï¼šçŽ°åœ¨ vec_dict åœ¨ç´¢å¼• 2
    final_vecs = [item[2] for item in top_heap] 
    
    return final_vecs, final_scores

    # elif method == "public-fisher":                       # -----------------
    #     logger.info("Using privacy-preserving 'public-fisher' method (reg=%s)", reg)
    #     logger.info("   Computing Fisher information from PUBLIC data only")
        
    #     # Use a smaller subset for Fisher computation to avoid memory issues
    #     max_fisher_samples = min(100, len(public_samples))  # Much smaller for memory efficiency
    #     fisher_samples = public_samples[:max_fisher_samples]
        
    #     logger.info("   Using %s public samples for Fisher matrix computation", max_fisher_samples)
        
    #     # Compute Fisher matrix from public data only (privacy-preserving)
    #     public_grads = []
    #     for x, y in tqdm(fisher_samples, desc="Computing public Fisher"):
    #         try:
    #             model.zero_grad()
    #             F.cross_entropy(model(x), y).backward()
    #             grad_vec = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])
    #             public_grads.append(grad_vec.detach().cpu())  # Move to CPU to save GPU memory
    #         except RuntimeError as e:
    #             logger.warn("Error computing gradient: %s", e)
    #             continue
        
    #     if len(public_grads) >= 10:  # Need minimum samples for stable Fisher
    #         try:
    #             G_public = torch.stack(public_grads)
    #             dim = G_public.shape[1]
                
    #             # Add strong regularization for stability
    #             fisher_public = (G_public.T @ G_public) / len(G_public) + reg * torch.eye(dim)
                
    #             logger.info("   Public Fisher shape: %s", tuple(fisher_public.shape))
    #             logger.success("Computed Fisher from %s public samples", len(public_grads))
                
    #             # Compute influence vectors using public Fisher
    #             for x, y in tqdm(public_samples[:200], desc="Public-Fisher influence vectors"):  # Limit total computations
    #                 try:
    #                     model.zero_grad()
    #                     F.cross_entropy(model(x), y).backward()
    #                     grad_vec = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None]).cpu()
                        
    #                     # Solve: fisher_public * v = grad_vec
    #                     try:
    #                         influence_vec_flat = torch.linalg.solve(fisher_public, grad_vec.unsqueeze(1)).squeeze(1)
    #                     except:
    #                         # Fallback to pseudo-inverse if singular
    #                         logger.warn("Using pseudo-inverse for stability")
    #                         influence_vec_flat = torch.pinverse(fisher_public) @ grad_vec
                        
    #                     # Reconstruct parameter-wise influence vector
    #                     vec = {}
    #                     idx = 0
    #                     for n, p in model.named_parameters():
    #                         if p.grad is not None:
    #                             n_params = p.numel()
    #                             vec[n] = influence_vec_flat[idx:idx+n_params].view_as(p).to(device)
    #                             idx += n_params
    #                         else:
    #                             vec[n] = torch.zeros_like(p)
    #                     infl_vecs.append(vec)
    #                 except Exception as e:
    #                     logger.warn("Error in influence computation: %s", e)
    #                     # Fallback: create zero influence vector
    #                     vec = {n: torch.zeros_like(p) for n, p in model.named_parameters()}
    #                     infl_vecs.append(vec)
                        
    #         except Exception as e:
    #             logger.warn("Error computing public Fisher matrix: %s", e)
    #             logger.warn("Falling back to linear method")
    #             return compute_influence_vectors(model, public_loader, train_loader, device, method="linear", reg=reg, strict=strict)
    #     else:
    #         logger.warn("Insufficient public gradients (%s < 10), falling back to linear method", len(public_grads))
    #         return compute_influence_vectors(model, public_loader, train_loader, device, method="linear", reg=reg, strict=strict)

    # elif method == "batch":                                # -----------------
    #     raise ValueError(
    #         f"ðŸš¨ PRIVACY VIOLATION: 'batch' method uses private training data statistics. "
    #         f"This violates the post-processing theorem. Use method='linear' instead."
    #     )

    # elif method == "original":                             # -----------------
    #     raise ValueError(
    #         f"ðŸš¨ PRIVACY VIOLATION: 'original' method uses private training data statistics. "
    #         f"This violates the post-processing theorem. Use method='linear' instead."
    #     )

    # else:                                                  # -----------------
    #     raise ValueError(f"unknown method '{method}'. Choose 'linear' for privacy-preserving influence functions.")

    # logger.success("Computed %s privacy-preserving influence vectors using only public data", len(infl_vecs))
    # return infl_vecs, public_samples

# --------------------------------------------------------------
# 3.  Trust-region scaling (shared util)
# --------------------------------------------------------------
def apply_trust_region(calib_dict, ref_norm, tau=0.05):
    """
    Scale Î”Î¸ so that â€–Î”Î¸â€–â‚‚ â‰¤ Ï„Â·ref_norm.
    (ref_norm = â€–Î¸Ì‚_DPâ€–â‚‚  or any baseline scale)
    """
    corr_norm = _frob_norm(calib_dict)
    if corr_norm < 1e-12:
        return calib_dict
    scale = min(1.0, tau * ref_norm / corr_norm)
    if scale < 1.0:
        for n in calib_dict:
            calib_dict[n].mul_(scale)
    return calib_dict

# --------------------------------------------------------------
# 4.  Research-protocol calibration  (lightly modified)
# --------------------------------------------------------------
def calibrate_model_research_protocol(model,
                                      public_loader,
                                      train_loader,
                                      crit_x, crit_y,
                                      device,
                                      method="linear",
                                      eta=200,
                                      target_improve=0.1,
                                      trust_tau=0.01,  # âœ¨ MUCH MORE CONSERVATIVE: 1% instead of 10%
                                      strict=True,
                                      clean_model=None,
                                      reg=10.0):  # âœ¨ MUCH MORE CONSERVATIVE: Higher regularization
    """
    ðŸ”’ PRIVACY-PRESERVING influence calibration following post-processing theorem.
    
    This function only uses:
    1. DP model parameters (output of DP algorithm)  
    2. Public dataset (dataset B)
    3. No statistics from private training data (dataset A)
    
    Args:
        clean_model: Non-DP baseline model for measuring Î”L_DP (utility drop due to DP)
        trust_tau: Trust region parameter (default 0.1 = 10% of model norm)
        reg: Regularization for influence vector computation (smaller = less regularization)
    """

    model.eval()

    # ---- 0) MEASURE Î”L_DP: Utility drop due to DP noise
    # Paper mapping â€” Step b (Measure utility drop):
    #   Î”L_DP = (1/|S_crit|) Î£_{sâˆˆS_crit} [â„“(s, Î¸Ì‚_DP) âˆ’ â„“(s, Î¸Ì‚_clean)]
    if clean_model is not None and len(crit_x) > 0:
        logger.info("MEASURING Î”L_DP: Utility drop due to DP noise on critical slice")
        
        clean_model.eval()
        model.eval()
        
        with torch.no_grad():
            # Loss on clean model
            clean_loss = F.cross_entropy(clean_model(crit_x), crit_y, reduction='mean')
            # Loss on DP model  
            dp_loss = F.cross_entropy(model(crit_x), crit_y, reduction='mean')
            
            delta_L_DP = dp_loss.item() - clean_loss.item()
            
        logger.info("   â€¢ Clean model loss: %.4f", clean_loss.item())
        logger.info("   â€¢ DP model loss:    %.4f", dp_loss.item())
        logger.info("   â€¢ Î”L_DP = L_DP - L_clean = %+0.4f", delta_L_DP)
        
        if delta_L_DP > 0:
            logger.info("   ðŸ“ˆ DP noise caused utility degradation (higher loss).")
            logger.info("   ðŸŽ¯ Goal: use calibration to recover utility loss.")
        else:
            logger.warn("   ðŸ“‰ DP model performs better than clean (unusual).")
    else:
        if clean_model is None:
            logger.warn("Skipping Î”L_DP measurement: no clean_model provided.")
        else:
            logger.warn("Skipping Î”L_DP measurement: no critical slice samples.")

    # ---- a) slice gradient J
    # Paper mapping â€” Step c(a): J = âˆ‡_Î¸ (1/m Î£_{sâˆˆS_crit} â„“(s, Î¸Ì‚)) at the current Î¸Ì‚
    J = compute_slice_gradient(model, crit_x, crit_y, device)
    J_flat = torch.cat([v.flatten() for v in J.values()])
    J_keys = list(J.keys())
    # ---- b) influence vectors (PRIVACY-PRESERVING: only uses public data + DP model)
    # Paper mapping â€” Step c(b): v(z) â‰ˆ H_{Î¸Ì‚}^{-1} âˆ‡_Î¸ â„“(z, Î¸Ì‚) for z in the public pool
    infl_vecs, scores = compute_top_influence_vectors(
        model, 
        public_loader, 
        J_flat, 
        J_keys, 
        device, 
        eta=eta, 
        reg=reg
    )

    # ---- d) choose Î· most helpful samples 
    # Paper mapping â€” Step d(i): Initial selection via sparse re-weighting
    # âœ¨ Implementation: indicator weights by selecting the Î· largest (most helpful) Î±(z)
    scores = np.array(scores)
    idx = np.argsort(scores)[-eta:]  # Select largest (most positive) scores
    w = np.zeros_like(scores)
    w[idx] = 1.0

    logger.info("Influence score statistics:")
    logger.info("   â€¢ Score range: [%.4f, %.4f]", scores.min(), scores.max())
    logger.info(
        "   â€¢ Selected %s samples with HIGHEST scores: [%.4f, %.4f]",
        eta,
        scores[idx].min(),
        scores[idx].max(),
    )
    logger.info("   â€¢ Mean selected score: %.4f", scores[idx].mean())
    logger.info("   â€¢ Score std: %.4f", scores.std())
    logger.info("   â€¢ Median score: %.4f", np.median(scores))
    logger.info("   ðŸŽ¯ Goal: Select samples that help reduce evaluation slice loss (positive scores)")
    
    # Additional diagnostic: check if we have any helpful samples
    positive_scores = scores[scores > 0]
    negative_scores = scores[scores < 0]
    logger.info("   â€¢ Helpful samples (score > 0): %s out of %s", len(positive_scores), len(scores))
    logger.info("   â€¢ Harmful samples (score < 0): %s out of %s", len(negative_scores), len(scores))
    
    if len(positive_scores) == 0:
        logger.warn("No helpful public samples found. All would increase evaluation loss.")
        logger.warn("This suggests domain mismatch between public and evaluation data.")
    elif len(positive_scores) < eta:
        logger.warn("Only %s helpful samples, but requesting %s", len(positive_scores), eta)
        logger.warn("Reducing eta to %s to avoid harmful samples", len(positive_scores))
        # Only use actually helpful samples
        helpful_idx = np.where(scores > 0)[0]
        w = np.zeros_like(scores)
        w[helpful_idx] = 1.0
        eta = len(helpful_idx)

    # ðŸ” DIAGNOSTIC: Check influence vector statistics
    infl_norms = [torch.sqrt(sum(v[n].pow(2).sum() for n in v.keys())).item() for v in infl_vecs]
    logger.info("Influence vector diagnostics:")
    logger.info("   â€¢ Influence vector norms range: [%.4f, %.4f]", min(infl_norms), max(infl_norms))
    logger.info("   â€¢ Mean influence vector norm: %.4f", np.mean(infl_norms))
    logger.info("   â€¢ Influence vectors used (nonzero weights): %s", np.sum(w > 0))

    # ---- e) Bias computation
    # Paper mapping â€” Step d(ii): Î”Î¸ = - (1/n) H_{Î¸Ì‚}^{-1} Î£_z w_z âˆ‡â„“(z, Î¸Ì‚)
    # Here we average v(z) â‰ˆ H^{-1}âˆ‡â„“(z) over selected z, implementing the same update up to scaling.
    # Move in direction that helpful public samples suggest
    n_selected = np.sum(w > 0)  # Number of selected samples
    if n_selected == 0:
        logger.warn("No helpful samples found for calibration - skipping update")
        logger.warn("This suggests the public data can't help improve the evaluation slice")
        return model
        
    delta = {n: torch.zeros_like(p) for n, p in model.named_parameters()}
    
    for i, weight in enumerate(w):
        if weight > 0:  # Only sum over helpful samples
            for n in delta:
                delta[n] += infl_vecs[i][n].to(delta[n].device) * weight / n_selected  # Average of helpful influence vectors

    # ðŸ” DIAGNOSTIC: Check delta before trust region
    delta_norm_before = _frob_norm(delta)
    logger.info("Parameter update diagnostics:")
    logger.info("   â€¢ Raw Î”Î¸ norm before trust region: %.4f", delta_norm_before)
    logger.info("   â€¢ Using CORRECTED formula: Î”Î¸ = (1/Î·) Î£_{helpful} Hâ»Â¹ âˆ‡â„“(public_sample)")
    logger.info("   â€¢ Helpful samples used Î·: %s", n_selected)
    logger.info("   â€¢ Direction: Move model toward what helpful public samples suggest")
    logger.info("   â€¢ Privacy-preserving: Uses only public data + DP model output")

    # ---- f) trust-region clip (IMPROVED: more generous)
    # Paper mapping â€” Step d(iii): Back-tracking line search over Î± âˆˆ {1, 1/2, 1/4, ...}
    # Note: We enforce a conservative trust region here. The explicit back-tracking
    #       line search that picks Î± by minimizing L_crit(Î¸Ì‚ + Î±Î”Î¸) is executed in
    #       ablation_optimized.calibrate_with_line_search(), which wraps this update.
    ref_norm = torch.sqrt(sum(p.pow(2).sum()
                              for p in model.parameters())).item()
    logger.info("   â€¢ Model parameter norm â€–Î¸â€–: %.4f", ref_norm)
    logger.info("   â€¢ Trust region limit (Ï„ Ã— â€–Î¸â€–): %.4f", trust_tau * ref_norm)
    
    delta = apply_trust_region(delta, ref_norm, tau=trust_tau)
    delta_norm_after = _frob_norm(delta)
    logger.info("   â€¢ Final Î”Î¸ norm after trust region: %.4f", delta_norm_after)
    logger.info(
        "   â€¢ Trust region scaling factor: %.4f",
        delta_norm_after / (delta_norm_before + 1e-12),
    )
    
    # ðŸ” DIAGNOSTIC: Check relative parameter change
    relative_change = delta_norm_after / ref_norm
    logger.info("   â€¢ Relative parameter change: %.4f (%.2f%%)", relative_change, relative_change * 100)
    
    if relative_change > 0.2:
        logger.warn("Large parameter change (>20%%) may cause instability")
    elif relative_change < 0.005:
        logger.warn("Very small parameter change (<0.5%%) may be ineffective")
        logger.warn("Suggestion: try decreasing reg parameter or increasing trust_tau")

    # ---- g) apply calibration: Î¸Ì‚* = Î¸Ì‚_DP + Î”Î¸
    with torch.no_grad():
        for n, p in model.named_parameters():
            p.add_(delta[n])

    logger.success(
        "Privacy-preserving calibration applied (â€–Î”Î¸â€–â‚‚ â‰ˆ %.4f, trust Ï„=%s)",
        _frob_norm(delta),
        trust_tau,
    )
    logger.success("Post-processing theorem: privacy guarantees preserved.")
    return model


def calibrate_model(model, public_loader, train_loader, fisher,
                    top_k=200, device=None, target_class="all", **kw):
    """
    ðŸ”„ UPDATED: Now uses ALL CLASSES by default for general utility improvement.
    
    Args:
        target_class: "all" for general utility (default), or int for specific class
    """
    eval_x, eval_y = get_evaluation_slice(public_loader, target_class, device=device)
    return calibrate_model_research_protocol(
        model, public_loader, train_loader,
        eval_x, eval_y, device,
        method="linear", eta=top_k, trust_tau=0.01, reg=10.0, strict=True, **kw)


def calibrate_model_efficient(model, public_loader, train_loader, fisher,
                             top_k=200, device=None, method="linear",
                             target_class="all", **kw):
    """
    ðŸ”„ UPDATED: Now uses ALL CLASSES by default for general utility improvement.
    
    Args:
        target_class: "all" for general utility (default), or int for specific class
    """
    eval_x, eval_y = get_evaluation_slice(public_loader, target_class, device=device)
    return calibrate_model_research_protocol(
        model, public_loader, train_loader,
        eval_x, eval_y, device,
        method=method, eta=top_k, trust_tau=0.01, reg=10.0, strict=True, **kw)
