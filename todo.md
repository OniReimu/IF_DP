# TODOs for Ablation Experiments and Paper Draft

## üî¨ Ablation / Sweep Experiments

All commands below assume CIFAR-10, strict DP setup (frozen backbone + DP on `conv1,conv2`), and `target-epsilon 2.0` unless otherwise noted.

### 0. Base Configuration (Reference)

- [ ] **Base run (Fisher DP + Fisher DP-SAT, Œµ=2.0)**
  - Command:
    ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

### 1. Clip Radius Sweep (Œµ = 2.0)

- [ ] **clip_radius = 1.0**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 1.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **clip_radius = 1.5**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 1.5 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **clip_radius = 2.5**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.5 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **clip_radius = 3.0**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 3.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

### 2. Fisher Rank k Sweep (Œµ = 2.0, lower k)

- [ ] **k = 256**
  - ```bash
    uv run ablation.py --mps --k 256 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **k = 512**
  - ```bash
    uv run ablation.py --mps --k 512 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **k = 1024**
  - ```bash
    uv run ablation.py --mps --k 1024 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

### 3. Epoch Sweep (Œµ = 2.0, change training length)

- [ ] **epochs = 150**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 150 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **epochs = 200**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 200 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **(Optional combos)** ‚Äì lower k with fewer epochs
  - Examples:
    - `--k 512 --epochs 150`
    - `--k 512 --epochs 200`

### 4. œÅ_sat Sweep for Fisher DP-SAT (Œµ = 2.0)

- [ ] **œÅ_sat = 0.0005**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.0005 --dp-sat-mode fisher
    ```

- [ ] **œÅ_sat = 0.002**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.002 --dp-sat-mode fisher
    ```

- [ ] **œÅ_sat = 0.005**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.005 --dp-sat-mode fisher
    ```

### 5. Slightly Deeper DP Layer Set (Œµ = 2.0)

- [ ] **DP on conv1, conv2, conv3 (if conv3 exists)**
  - ```bash
    uv run ablation.py --mps --k 2048 --epochs 300 --dataset-size 50000 \
      --target-epsilon 2.0 --delta 1e-5 \
      --dp-layer conv1,conv2,conv3 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

### 6. Optional: Looser Privacy (Œµ = 4.0)

- [ ] **Œµ = 4.0, k = 512, epochs = 300**
  - ```bash
    uv run ablation.py --mps --k 512 --epochs 300 --dataset-size 50000 \
      --target-epsilon 4.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

- [ ] **Œµ = 8.0, k = 512, epochs = 300**
  - ```bash
    uv run ablation.py --mps --k 512 --epochs 300 --dataset-size 50000 \
      --target-epsilon 8.0 --delta 1e-5 \
      --dp-layer conv1,conv2 --clip-radius 2.0 \
      --run-mia --users 100 --calibration-k 2000 \
      --rho_sat 0.001 --dp-sat-mode fisher
    ```

### Minimal Recommended Subset

If time is limited, prioritise:
- [ ] **k sweep**: `k = 256`, `k = 512`
- [ ] **clip-radius sweep**: `clip_radius = 1.5`, `clip_radius = 2.5`
- [ ] **œÅ_sat tweaks**: `rho_sat = 0.0005`, `rho_sat = 0.002`


## Notes

Right now your relative story is quite nice (Fisher + DP-SAT + Calib is clearly best), but yeah: 51% vs 61.7% non-DP at Œµ=2, user-level, 100 users still feels sad.

The key thing: with user-level Œµ=2 on only 100 users, the bar for accuracy is much harsher than the usual sample-level CIFAR stuff. A ~10-point gap to non-DP might actually be close to the Pareto front. But there are knobs you can turn.

I‚Äôll give you a concrete ‚Äútuning menu‚Äù with priorities.

‚∏ª

1. Privacy knobs (the big levers)

(1) Try a looser Œµ: 4 or 8

Right now:
	‚Ä¢	Œµ = 2.0, Œ¥ = 1e-5, 100 users
	‚Ä¢	1-AUC is ~0.49‚Äì0.51 for all DP variants ‚Üí MI is already at random.

That‚Äôs actually telling you: you have room to relax Œµ a bit without obviously harming MI robustness:
	‚Ä¢	Run the same ablation with:
	‚Ä¢	--target-epsilon 4.0
	‚Ä¢	and maybe --target-epsilon 8.0
	‚Ä¢	Keep everything else fixed (epochs, clip radius, users).

If at Œµ=4 your best model jumps from ~51% ‚Üí ~55‚Äì58% and MI AUC still ~0.5, that‚Äôs a very defensible ‚Äúaccuracy vs privacy‚Äù plot:
	‚Ä¢	‚ÄúAt Œµ=2 we get 51.3% with near-random MI.‚Äù
	‚Ä¢	‚ÄúAt Œµ=4 we get X% with still near-random MI.‚Äù

If you must stick to Œµ=2 for the main claim, you can still include Œµ=4/8 curves as secondary ablations.

(2) Use a proper DP-SGD accountant for œÉ (This one has been fixed, can skip this) 

Your current code uses the ‚Äúlegacy‚Äù œÉ:

sigma_single_epoch = sqrt(2*log(1.25/delta)) / epsilon
sigma = sigma_single_epoch / sqrt(epochs)

This is a hacky composition bound, not the DP-SGD/RDP formula. In practice:
	‚Ä¢	A proper RDP / moments accountant will usually allow a smaller œÉ than that legacy bound for the same Œµ, given your actual sampling rate and number of steps.
	‚Ä¢	Smaller œÉ ‚Üí less noise ‚Üí higher accuracy for the same Œµ.

Concrete action:
	‚Ä¢	Compute œÉ with Opacus or your own RDP library based on:
	‚Ä¢	number of users (100),
	‚Ä¢	sampling scheme (batch of how many users per step),
	‚Ä¢	total steps (‚âà epochs √ó steps_per_epoch).
	‚Ä¢	Then call train_with_dp(..., sigma=that_value) and ignore the legacy œÉ branch.

This is probably the single most ‚Äúprincipled‚Äù way to get extra accuracy without cheating on Œµ.

‚∏ª

2. Training hyperparameters under DP

At Œµ=2, you‚Äôre in a very noisy regime. Getting the optimiser schedule right matters a lot.

(3) Reduce epochs + increase / schedule LR

You used:

--epochs 300

DP-SGD behaviour is quite different from non-DP:
	‚Ä¢	Too many noisy updates at low LR can just wander, not converge.
	‚Ä¢	Often it‚Äôs better to:
	‚Ä¢	train for fewer epochs (e.g. 80‚Äì150),
	‚Ä¢	use a larger initial LR with a good schedule (cosine, step decay).

Concrete things to try:
	‚Ä¢	Halve epochs:
	‚Ä¢	--epochs 150 with LR 1e-3 (or a bit larger, e.g. 3e-3), cosine decay.
	‚Ä¢	Or:
	‚Ä¢	--epochs 100, LR 3e-3 with a step schedule.

DP-SAT in particular tends to like a reasonably high LR early on; otherwise the sharpness-aware perturbation is a tiny correction under huge noise.

(4) Clip radius sweep

You‚Äôre using:

--clip-radius 2.0

Even though you have adaptive/norm calibration, the base target still matters.

Easy sweep:
	‚Ä¢	--clip-radius {1.0, 1.5, 2.0, 3.0}

For each, record:
	‚Ä¢	fraction of gradients clipped,
	‚Ä¢	final accuracy.

Under DP, often the sweet spot is somewhere around ‚Äú20‚Äì40% of per-user gradients clipped‚Äù ‚Äî too small C ‚Üí heavy clipping bias; too big C ‚Üí huge sensitivity, you need more noise for the same Œµ and gradients get drowned.

You already have nice log stats in train_with_dp (Mahalanobis norms, etc.), so it‚Äôs easy to sanity-check.

‚∏ª

3. Fisher / architecture knobs

(5) Reduce Fisher rank k

You‚Äôre currently using:

--k 2048
--calibration-k 2000
--dp-layer conv1,conv2

For conv1+conv2 only, k=2048 might be:
	‚Ä¢	larger than the actual parameter count of those layers, or
	‚Ä¢	so large that the Fisher noise becomes ‚Äúalmost isotropic‚Äù again (it covers nearly all directions).

Try:
	‚Ä¢	--k 64
	‚Ä¢	--k 128
	‚Ä¢	--k 256

This:
	‚Ä¢	keeps the main curvature directions,
	‚Ä¢	but lets the anisotropy actually do something, instead of just approximating full-covariance noise that‚Äôs numerically fragile.

You can keep --calibration-k large (used only for norm calibration), but the noise rank k is the important one.

(6) DP-layer choice: include a bit more of the network

Right now you‚Äôre DP-training only:

--dp-layer conv1,conv2

If those are the only layers updated on private data, they might be too ‚Äúearly‚Äù to absorb all the task-specific adaptation the dataset needs, especially with 100 users and a relatively hard task.

Two options:
	1.	Still strict DP on full model, but Fisher only on shallow layers
	‚Ä¢	Add the next conv layer to the DP set with either Fisher or vanilla DP:
	‚Ä¢	--dp-layer conv1,conv2,conv3
	‚Ä¢	In code:
	‚Ä¢	treat conv1+conv2 as Fisher block,
	‚Ä¢	treat conv3 (and maybe the head) with plain Euclidean DP-SGD (clip + isotropic noise).
	2.	Fisher on more layers, smaller k per layer
	‚Ä¢	E.g. all conv layers Fisher-DP, but with small per-layer k (like 32‚Äì64) to keep the Fisher computation manageable.
	‚Ä¢	This might let later features adapt better, pushing accuracy up.

Even a DP head only variant is worth checking:
	‚Ä¢	--dp-layer fc (or whatever your last linear is),
	‚Ä¢	Fisher or Euclidean DP there,
	‚Ä¢	frozen backbone from non-DP or public training.

That often gives you a large chunk of the non-DP accuracy back for a given Œµ.

‚∏ª

4. DP-SAT-specific tweaks

Given your new results:
	‚Ä¢	Vanilla DP-SGD ‚Üí +DP-SAT: 36.0 ‚Üí 37.4
	‚Ä¢	Fisher DP ‚Üí +DP-SAT: 46.0 ‚Üí 49.9
	‚Ä¢	Fisher + Calib ‚Üí +DP-SAT: 47.7 ‚Üí 51.3

So DP-SAT is helping quite a bit now (‚âà +4‚Äì5 points in the Fisher regime). Nice.

Still, you can try:

(7) œÅ (rho_sat) sweep

If you‚Äôre using the default rho_sat=0.001, that‚Äôs arbitrary. Try:
	‚Ä¢	rho_sat ‚àà {5e-4, 1e-3, 2e-3, 5e-3}

For each, record Fisher+DP-SAT accuracy (with and without calibration). You may find:
	‚Ä¢	one œÅ gives ~1‚Äì2 extra points,
	‚Ä¢	too large œÅ destabilizes training early.

(8) Turn off DP-SAT for vanilla, keep for Fisher

For the paper narrative, it‚Äôs nice to show:
	‚Ä¢	the incremental contribution of DP-SAT on vanilla vs Fisher:
	‚Ä¢	‚ÄúImproves vanilla by +1.4 points‚Äù
	‚Ä¢	‚ÄúImproves Fisher by +3.9 points‚Äù

But if your main concern is peak accuracy, you can decide:
	‚Ä¢	only use DP-SAT in the Fisher runs where it gives the biggest benefit.

‚∏ª

5. Realistic expectations

Given:
	‚Ä¢	user-level Œµ=2, Œ¥=1e-5,
	‚Ä¢	only 100 users,
	‚Ä¢	the task is non-trivial (non-DP baseline 61.7%),

then:
	‚Ä¢	getting 51‚Äì55% with random MI is already a solid result;
	‚Ä¢	expecting to hit 60%+ at that Œµ and N might be unrealistic.

So I‚Äôd aim for something like:
	‚Ä¢	At Œµ=2: best model around 52‚Äì55% (you‚Äôre already at 51.3, so +2‚Äì3 points via the knobs above is plausible).
	‚Ä¢	At Œµ=4: best model maybe 56‚Äì60%.
	‚Ä¢	Non-DP baseline: 61‚Äì62%.

That gives you a clean privacy‚Äìutility curve and a compelling ‚Äúfree-lunch utility‚Äù story: Fisher + DP-SAT + calibration consistently dominates vanilla DP-SGD at the same Œµ.