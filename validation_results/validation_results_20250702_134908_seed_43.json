{
  "results": [
    {
      "baseline": 69.22,
      "fisher_dp": 66.66,
      "vanilla_dp": 63.04,
      "dp_sat": 64.22,
      "fisher_vs_vanilla": 3.62,
      "fisher_vs_dp_sat": 2.44,
      "dp_sat_vs_vanilla": 1.18,
      "baseline_confidence_auc": 0.5343,
      "fisher_dp_confidence_auc": 0.5294,
      "vanilla_dp_confidence_auc": 0.5297,
      "dp_sat_confidence_auc": 0.5203,
      "baseline_shadow_auc": 0.523,
      "fisher_dp_shadow_auc": 0.5218,
      "vanilla_dp_shadow_auc": 0.5102,
      "dp_sat_shadow_auc": 0.5107,
      "fisher_dp_worst_auc": 0.5294,
      "vanilla_dp_worst_auc": 0.5297,
      "dp_sat_worst_auc": 0.5203,
      "experiment_name": "25_users_positive",
      "users": 25,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 25 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (25 synthetic users)\n\u25b6  25 synthetic users (2000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([25, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 9.01e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0005\n\u2022 Required noise multiplier: 0.6152\n\u2022 Sigma (for both methods): 1.2305\n\u2022 Total steps: 1250\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.611\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.766\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.611 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.611\n\u2022 Median \u2016g_user\u2016_Mah = 6128.72\n\u2022 Total noise \u2113\u2082 \u2208 [78.2,106.4]\n\u2022 Last batch noise: Fisher only=101.9 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.213 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5742.45\n\u2022 Isotropic noise \u2113\u2082 \u2208 [339.0,345.3]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5480.04\n\u2022 Isotropic noise \u2113\u2082 \u2208 [341.1,347.7]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udcca  Accuracy summary (User-level (25 users) DP)\nbaseline         :  69.22% (cat 50.60%)\nFisher DP        :  66.66% (cat 52.80%)\nVanilla DP       :  63.04% (cat 55.20%)\nDP-SAT           :  64.22% (cat 39.00%)\nFisher vs Vanilla: +3.62% improvement\nFisher vs DP-SAT : +2.44% improvement\nDP-SAT vs Vanilla: +1.18% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5343 \u00b1 0.0000\nFisher DP: 0.5294 \u00b1 0.0000\nVanilla DP: 0.5297 \u00b1 0.0000\nDP-SAT: 0.5203 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5230 \u00b1 0.0000\nFisher DP: 0.5218 \u00b1 0.0000\nVanilla DP: 0.5102 \u00b1 0.0000\nDP-SAT: 0.5107 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5294\n\u2022 Vanilla DP: 0.5297\n\u2022 DP-SAT: 0.5203\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0091 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0094 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 69.16,
      "fisher_dp": 66.36,
      "vanilla_dp": 63.0,
      "dp_sat": 64.2,
      "fisher_vs_vanilla": 3.36,
      "fisher_vs_dp_sat": 2.16,
      "dp_sat_vs_vanilla": 1.2,
      "baseline_confidence_auc": 0.5345,
      "fisher_dp_confidence_auc": 0.5306,
      "vanilla_dp_confidence_auc": 0.5299,
      "dp_sat_confidence_auc": 0.5202,
      "baseline_shadow_auc": 0.524,
      "fisher_dp_shadow_auc": 0.5313,
      "vanilla_dp_shadow_auc": 0.5101,
      "dp_sat_shadow_auc": 0.5109,
      "fisher_dp_worst_auc": 0.5313,
      "vanilla_dp_worst_auc": 0.5299,
      "dp_sat_worst_auc": 0.5202,
      "experiment_name": "25_users_negative",
      "users": 25,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 25 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (25 synthetic users)\n\u25b6  25 synthetic users (2000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([25, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 7.91e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0005\n\u2022 Required noise multiplier: 0.6152\n\u2022 Sigma (for both methods): 1.2305\n\u2022 Total steps: 1250\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 7.906] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.151\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.930\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.151 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.151\n\u2022 Median \u2016g_user\u2016_Mah = 6586.37\n\u2022 Total noise \u2113\u2082 \u2208 [156.4,175.3]\n\u2022 Last batch noise: Fisher only=167.8 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=2.647 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5569.54\n\u2022 Isotropic noise \u2113\u2082 \u2208 [339.0,345.3]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5415.36\n\u2022 Isotropic noise \u2113\u2082 \u2208 [341.1,347.7]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udcca  Accuracy summary (User-level (25 users) DP)\nbaseline         :  69.16% (cat 50.60%)\nFisher DP        :  66.36% (cat 59.80%)\nVanilla DP       :  63.00% (cat 55.40%)\nDP-SAT           :  64.20% (cat 38.80%)\nFisher vs Vanilla: +3.36% improvement\nFisher vs DP-SAT : +2.16% improvement\nDP-SAT vs Vanilla: +1.20% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5345 \u00b1 0.0000\nFisher DP: 0.5306 \u00b1 0.0000\nVanilla DP: 0.5299 \u00b1 0.0000\nDP-SAT: 0.5202 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5240 \u00b1 0.0000\nFisher DP: 0.5313 \u00b1 0.0000\nVanilla DP: 0.5101 \u00b1 0.0000\nDP-SAT: 0.5109 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5313\n\u2022 Vanilla DP: 0.5299\n\u2022 DP-SAT: 0.5202\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0111 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0097 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 72.72,
      "fisher_dp": 69.6,
      "vanilla_dp": 63.34,
      "dp_sat": 60.32,
      "fisher_vs_vanilla": 6.26,
      "fisher_vs_dp_sat": 9.28,
      "dp_sat_vs_vanilla": -3.02,
      "baseline_confidence_auc": 0.5493,
      "fisher_dp_confidence_auc": 0.5471,
      "vanilla_dp_confidence_auc": 0.5237,
      "dp_sat_confidence_auc": 0.5189,
      "baseline_shadow_auc": 0.5271,
      "fisher_dp_shadow_auc": 0.5268,
      "vanilla_dp_shadow_auc": 0.5123,
      "dp_sat_shadow_auc": 0.5175,
      "fisher_dp_worst_auc": 0.5471,
      "vanilla_dp_worst_auc": 0.5237,
      "dp_sat_worst_auc": 0.5189,
      "experiment_name": "50_users_positive",
      "users": 50,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 50 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (50 synthetic users)\n\u25b6  50 synthetic users (1000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([50, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.05e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0010\n\u2022 Required noise multiplier: 0.6677\n\u2022 Sigma (for both methods): 1.3354\n\u2022 Total steps: 2500\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.170\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.921\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.170 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.170\n\u2022 Median \u2016g_user\u2016_Mah = 3594.99\n\u2022 Total noise \u2113\u2082 \u2208 [83.2,95.8]\n\u2022 Last batch noise: Fisher only=93.2 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=2.899 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4051.92\n\u2022 Isotropic noise \u2113\u2082 \u2208 [367.0,376.4]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4277.08\n\u2022 Isotropic noise \u2113\u2082 \u2208 [367.2,378.1]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udcca  Accuracy summary (User-level (50 users) DP)\nbaseline         :  72.72% (cat 52.00%)\nFisher DP        :  69.60% (cat 49.00%)\nVanilla DP       :  63.34% (cat 56.40%)\nDP-SAT           :  60.32% (cat 38.20%)\nFisher vs Vanilla: +6.26% improvement\nFisher vs DP-SAT : +9.28% improvement\nDP-SAT vs Vanilla: -3.02% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5493 \u00b1 0.0000\nFisher DP: 0.5471 \u00b1 0.0000\nVanilla DP: 0.5237 \u00b1 0.0000\nDP-SAT: 0.5189 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5271 \u00b1 0.0000\nFisher DP: 0.5268 \u00b1 0.0000\nVanilla DP: 0.5123 \u00b1 0.0000\nDP-SAT: 0.5175 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5471\n\u2022 Vanilla DP: 0.5237\n\u2022 DP-SAT: 0.5189\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0283 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0048 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 72.72,
      "fisher_dp": 59.94,
      "vanilla_dp": 63.34,
      "dp_sat": 60.32,
      "fisher_vs_vanilla": -3.4,
      "fisher_vs_dp_sat": -0.38,
      "dp_sat_vs_vanilla": -3.02,
      "baseline_confidence_auc": 0.5493,
      "fisher_dp_confidence_auc": 0.5302,
      "vanilla_dp_confidence_auc": 0.5237,
      "dp_sat_confidence_auc": 0.5189,
      "baseline_shadow_auc": 0.5271,
      "fisher_dp_shadow_auc": 0.5149,
      "vanilla_dp_shadow_auc": 0.5123,
      "dp_sat_shadow_auc": 0.5175,
      "fisher_dp_worst_auc": 0.5302,
      "vanilla_dp_worst_auc": 0.5237,
      "dp_sat_worst_auc": 0.5189,
      "experiment_name": "50_users_negative",
      "users": 50,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 50 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (50 synthetic users)\n\u25b6  50 synthetic users (1000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([50, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.05e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0010\n\u2022 Required noise multiplier: 0.6677\n\u2022 Sigma (for both methods): 1.3354\n\u2022 Total steps: 2500\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.183\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.916\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.183 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.183\n\u2022 Median \u2016g_user\u2016_Mah = 7227.98\n\u2022 Total noise \u2113\u2082 \u2208 [166.2,192.6]\n\u2022 Last batch noise: Fisher only=187.5 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=2.916 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4051.92\n\u2022 Isotropic noise \u2113\u2082 \u2208 [367.0,376.4]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4277.08\n\u2022 Isotropic noise \u2113\u2082 \u2208 [367.2,378.1]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udcca  Accuracy summary (User-level (50 users) DP)\nbaseline         :  72.72% (cat 52.00%)\nFisher DP        :  59.94% (cat 45.20%)\nVanilla DP       :  63.34% (cat 56.40%)\nDP-SAT           :  60.32% (cat 38.20%)\nFisher vs Vanilla: -3.40% improvement\nFisher vs DP-SAT : -0.38% improvement\nDP-SAT vs Vanilla: -3.02% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5493 \u00b1 0.0000\nFisher DP: 0.5302 \u00b1 0.0000\nVanilla DP: 0.5237 \u00b1 0.0000\nDP-SAT: 0.5189 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5271 \u00b1 0.0000\nFisher DP: 0.5149 \u00b1 0.0000\nVanilla DP: 0.5123 \u00b1 0.0000\nDP-SAT: 0.5175 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5302\n\u2022 Vanilla DP: 0.5237\n\u2022 DP-SAT: 0.5189\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0114 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0048 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.64,
      "fisher_dp": 51.86,
      "vanilla_dp": 54.54,
      "dp_sat": 55.94,
      "fisher_vs_vanilla": -2.68,
      "fisher_vs_dp_sat": -4.08,
      "dp_sat_vs_vanilla": 1.4,
      "baseline_confidence_auc": 0.6077,
      "fisher_dp_confidence_auc": 0.5398,
      "vanilla_dp_confidence_auc": 0.54,
      "dp_sat_confidence_auc": 0.5353,
      "baseline_shadow_auc": 0.5847,
      "fisher_dp_shadow_auc": 0.5101,
      "vanilla_dp_shadow_auc": 0.5096,
      "dp_sat_shadow_auc": 0.485,
      "fisher_dp_worst_auc": 0.5398,
      "vanilla_dp_worst_auc": 0.54,
      "dp_sat_worst_auc": 0.5353,
      "experiment_name": "100_users_positive",
      "users": 100,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 100 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (100 synthetic users)\n\u25b6  100 synthetic users (500.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([100, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 4.24e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0020\n\u2022 Required noise multiplier: 0.7629\n\u2022 Sigma (for both methods): 1.5259\n\u2022 Total steps: 5000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 42.426] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.329\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.859\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.329 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.329\n\u2022 Median \u2016g_user\u2016_Mah = 3424.06\n\u2022 Total noise \u2113\u2082 \u2208 [97.5,134.0]\n\u2022 Last batch noise: Fisher only=127.0 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.554 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2901.22\n\u2022 Isotropic noise \u2113\u2082 \u2208 [419.3,429.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3070.60\n\u2022 Isotropic noise \u2113\u2082 \u2208 [419.0,431.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udcca  Accuracy summary (User-level (100 users) DP)\nbaseline         :  73.64% (cat 57.60%)\nFisher DP        :  51.86% (cat 16.40%)\nVanilla DP       :  54.54% (cat 47.60%)\nDP-SAT           :  55.94% (cat 46.80%)\nFisher vs Vanilla: -2.68% improvement\nFisher vs DP-SAT : -4.08% improvement\nDP-SAT vs Vanilla: +1.40% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6077 \u00b1 0.0000\nFisher DP: 0.5398 \u00b1 0.0000\nVanilla DP: 0.5400 \u00b1 0.0000\nDP-SAT: 0.5353 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5847 \u00b1 0.0000\nFisher DP: 0.5101 \u00b1 0.0000\nVanilla DP: 0.5096 \u00b1 0.0000\nDP-SAT: 0.4850 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5398\n\u2022 Vanilla DP: 0.5400\n\u2022 DP-SAT: 0.5353\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0045 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0047 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.64,
      "fisher_dp": 53.84,
      "vanilla_dp": 54.54,
      "dp_sat": 55.94,
      "fisher_vs_vanilla": -0.7,
      "fisher_vs_dp_sat": -2.1,
      "dp_sat_vs_vanilla": 1.4,
      "baseline_confidence_auc": 0.6077,
      "fisher_dp_confidence_auc": 0.5366,
      "vanilla_dp_confidence_auc": 0.54,
      "dp_sat_confidence_auc": 0.5353,
      "baseline_shadow_auc": 0.5847,
      "fisher_dp_shadow_auc": 0.5039,
      "vanilla_dp_shadow_auc": 0.5096,
      "dp_sat_shadow_auc": 0.485,
      "fisher_dp_worst_auc": 0.5366,
      "vanilla_dp_worst_auc": 0.54,
      "dp_sat_worst_auc": 0.5353,
      "experiment_name": "100_users_negative",
      "users": 100,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 100 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (100 synthetic users)\n\u25b6  100 synthetic users (500.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([100, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 4.24e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0020\n\u2022 Required noise multiplier: 0.7629\n\u2022 Sigma (for both methods): 1.5259\n\u2022 Total steps: 5000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 42.426] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.548\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.785\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.548 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.548\n\u2022 Median \u2016g_user\u2016_Mah = 4054.67\n\u2022 Total noise \u2113\u2082 \u2208 [191.4,258.8]\n\u2022 Last batch noise: Fisher only=246.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.888 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2901.22\n\u2022 Isotropic noise \u2113\u2082 \u2208 [419.3,429.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3070.60\n\u2022 Isotropic noise \u2113\u2082 \u2208 [419.0,431.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udcca  Accuracy summary (User-level (100 users) DP)\nbaseline         :  73.64% (cat 57.60%)\nFisher DP        :  53.84% (cat 28.40%)\nVanilla DP       :  54.54% (cat 47.60%)\nDP-SAT           :  55.94% (cat 46.80%)\nFisher vs Vanilla: -0.70% improvement\nFisher vs DP-SAT : -2.10% improvement\nDP-SAT vs Vanilla: +1.40% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6077 \u00b1 0.0000\nFisher DP: 0.5366 \u00b1 0.0000\nVanilla DP: 0.5400 \u00b1 0.0000\nDP-SAT: 0.5353 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5847 \u00b1 0.0000\nFisher DP: 0.5039 \u00b1 0.0000\nVanilla DP: 0.5096 \u00b1 0.0000\nDP-SAT: 0.4850 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5366\n\u2022 Vanilla DP: 0.5400\n\u2022 DP-SAT: 0.5353\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0013 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0047 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.98,
      "fisher_dp": 56.98,
      "vanilla_dp": 35.16,
      "dp_sat": 31.32,
      "fisher_vs_vanilla": 21.82,
      "fisher_vs_dp_sat": 25.66,
      "dp_sat_vs_vanilla": -3.84,
      "baseline_confidence_auc": 0.6427,
      "fisher_dp_confidence_auc": 0.5424,
      "vanilla_dp_confidence_auc": 0.5164,
      "dp_sat_confidence_auc": 0.4992,
      "baseline_shadow_auc": 0.6638,
      "fisher_dp_shadow_auc": 0.4974,
      "vanilla_dp_shadow_auc": 0.4836,
      "dp_sat_shadow_auc": 0.5109,
      "fisher_dp_worst_auc": 0.5424,
      "vanilla_dp_worst_auc": 0.5164,
      "dp_sat_worst_auc": 0.5109,
      "experiment_name": "200_users_positive",
      "users": 200,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 200 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 8.56e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 8.563] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.336\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.856\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.336 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.336\n\u2022 Median \u2016g_user\u2016_Mah = 2102.44\n\u2022 Total noise \u2113\u2082 \u2208 [134.4,174.5]\n\u2022 Last batch noise: Fisher only=165.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=5.167 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2307.14\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.8,624.1]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2572.30\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.2,623.2]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  73.98% (cat 54.80%)\nFisher DP        :  56.98% (cat 41.60%)\nVanilla DP       :  35.16% (cat 26.60%)\nDP-SAT           :  31.32% (cat 17.40%)\nFisher vs Vanilla: +21.82% improvement\nFisher vs DP-SAT : +25.66% improvement\nDP-SAT vs Vanilla: -3.84% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6427 \u00b1 0.0000\nFisher DP: 0.5424 \u00b1 0.0000\nVanilla DP: 0.5164 \u00b1 0.0000\nDP-SAT: 0.4992 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6638 \u00b1 0.0000\nFisher DP: 0.4974 \u00b1 0.0000\nVanilla DP: 0.4836 \u00b1 0.0000\nDP-SAT: 0.5109 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5424\n\u2022 Vanilla DP: 0.5164\n\u2022 DP-SAT: 0.5109\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0315 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0056 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.98,
      "fisher_dp": 37.8,
      "vanilla_dp": 35.16,
      "dp_sat": 31.32,
      "fisher_vs_vanilla": 2.64,
      "fisher_vs_dp_sat": 6.48,
      "dp_sat_vs_vanilla": -3.84,
      "baseline_confidence_auc": 0.6427,
      "fisher_dp_confidence_auc": 0.5077,
      "vanilla_dp_confidence_auc": 0.5164,
      "dp_sat_confidence_auc": 0.4992,
      "baseline_shadow_auc": 0.6638,
      "fisher_dp_shadow_auc": 0.5065,
      "vanilla_dp_shadow_auc": 0.4836,
      "dp_sat_shadow_auc": 0.5109,
      "fisher_dp_worst_auc": 0.5077,
      "vanilla_dp_worst_auc": 0.5164,
      "dp_sat_worst_auc": 0.5109,
      "experiment_name": "200_users_negative",
      "users": 200,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 200 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 8.56e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 8.563] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.455\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.815\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.455 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.455\n\u2022 Median \u2016g_user\u2016_Mah = 2706.05\n\u2022 Total noise \u2113\u2082 \u2208 [268.7,365.8]\n\u2022 Last batch noise: Fisher only=347.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=5.431 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2307.14\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.8,624.1]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2572.30\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.2,623.2]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  73.98% (cat 54.80%)\nFisher DP        :  37.80% (cat 33.20%)\nVanilla DP       :  35.16% (cat 26.60%)\nDP-SAT           :  31.32% (cat 17.40%)\nFisher vs Vanilla: +2.64% improvement\nFisher vs DP-SAT : +6.48% improvement\nDP-SAT vs Vanilla: -3.84% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6427 \u00b1 0.0000\nFisher DP: 0.5077 \u00b1 0.0000\nVanilla DP: 0.5164 \u00b1 0.0000\nDP-SAT: 0.4992 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6638 \u00b1 0.0000\nFisher DP: 0.5065 \u00b1 0.0000\nVanilla DP: 0.4836 \u00b1 0.0000\nDP-SAT: 0.5109 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5077\n\u2022 Vanilla DP: 0.5164\n\u2022 DP-SAT: 0.5109\n\ud83c\udfc6 Fisher DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Vanilla DP: 0.0087 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0031 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 25.06,
      "vanilla_dp": 14.3,
      "dp_sat": 17.84,
      "fisher_vs_vanilla": 10.76,
      "fisher_vs_dp_sat": 7.22,
      "dp_sat_vs_vanilla": 3.54,
      "baseline_confidence_auc": 0.6694,
      "fisher_dp_confidence_auc": 0.5089,
      "vanilla_dp_confidence_auc": 0.5274,
      "dp_sat_confidence_auc": 0.5085,
      "baseline_shadow_auc": 0.7014,
      "fisher_dp_shadow_auc": 0.5031,
      "vanilla_dp_shadow_auc": 0.4946,
      "dp_sat_shadow_auc": 0.4723,
      "fisher_dp_worst_auc": 0.5089,
      "vanilla_dp_worst_auc": 0.5274,
      "dp_sat_worst_auc": 0.5085,
      "experiment_name": "400_users_positive",
      "users": 400,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 400 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (400 synthetic users)\n\u25b6  400 synthetic users (125.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([400, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.24e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0080\n\u2022 Required noise multiplier: 2.5488\n\u2022 Sigma (for both methods): 5.0977\n\u2022 Total steps: 20000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 1.244] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.344\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.853\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.344 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.344\n\u2022 Median \u2016g_user\u2016_Mah = 1515.60\n\u2022 Total noise \u2113\u2082 \u2208 [322.0,397.3]\n\u2022 Last batch noise: Fisher only=391.9 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=11.950 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1343.48\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1398.7,1444.1]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1226.28\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1402.1,1439.0]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udcca  Accuracy summary (User-level (400 users) DP)\nbaseline         :  75.02% (cat 60.40%)\nFisher DP        :  25.06% (cat 26.00%)\nVanilla DP       :  14.30% (cat 15.00%)\nDP-SAT           :  17.84% (cat 17.00%)\nFisher vs Vanilla: +10.76% improvement\nFisher vs DP-SAT : +7.22% improvement\nDP-SAT vs Vanilla: +3.54% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6694 \u00b1 0.0000\nFisher DP: 0.5089 \u00b1 0.0000\nVanilla DP: 0.5274 \u00b1 0.0000\nDP-SAT: 0.5085 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.7014 \u00b1 0.0000\nFisher DP: 0.5031 \u00b1 0.0000\nVanilla DP: 0.4946 \u00b1 0.0000\nDP-SAT: 0.4723 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5089\n\u2022 Vanilla DP: 0.5274\n\u2022 DP-SAT: 0.5085\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0004 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0188 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 16.84,
      "vanilla_dp": 14.3,
      "dp_sat": 17.84,
      "fisher_vs_vanilla": 2.54,
      "fisher_vs_dp_sat": -1.0,
      "dp_sat_vs_vanilla": 3.54,
      "baseline_confidence_auc": 0.6694,
      "fisher_dp_confidence_auc": 0.5274,
      "vanilla_dp_confidence_auc": 0.5274,
      "dp_sat_confidence_auc": 0.5085,
      "baseline_shadow_auc": 0.7014,
      "fisher_dp_shadow_auc": 0.5048,
      "vanilla_dp_shadow_auc": 0.4946,
      "dp_sat_shadow_auc": 0.4723,
      "fisher_dp_worst_auc": 0.5274,
      "vanilla_dp_worst_auc": 0.5274,
      "dp_sat_worst_auc": 0.5085,
      "experiment_name": "400_users_negative",
      "users": 400,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 400 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 43\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (400 synthetic users)\n\u25b6  400 synthetic users (125.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([400, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.24e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0080\n\u2022 Required noise multiplier: 2.5488\n\u2022 Sigma (for both methods): 5.0977\n\u2022 Total steps: 20000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 1.244] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.295\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.871\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.295 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.295\n\u2022 Median \u2016g_user\u2016_Mah = 1169.76\n\u2022 Total noise \u2113\u2082 \u2208 [644.0,777.9]\n\u2022 Last batch noise: Fisher only=767.2 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=11.700 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1343.48\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1398.7,1444.1]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1226.28\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1402.1,1439.0]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udcca  Accuracy summary (User-level (400 users) DP)\nbaseline         :  75.02% (cat 60.40%)\nFisher DP        :  16.84% (cat 11.40%)\nVanilla DP       :  14.30% (cat 15.00%)\nDP-SAT           :  17.84% (cat 17.00%)\nFisher vs Vanilla: +2.54% improvement\nFisher vs DP-SAT : -1.00% improvement\nDP-SAT vs Vanilla: +3.54% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 43\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6694 \u00b1 0.0000\nFisher DP: 0.5274 \u00b1 0.0000\nVanilla DP: 0.5274 \u00b1 0.0000\nDP-SAT: 0.5085 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.7014 \u00b1 0.0000\nFisher DP: 0.5048 \u00b1 0.0000\nVanilla DP: 0.4946 \u00b1 0.0000\nDP-SAT: 0.4723 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5274\n\u2022 Vanilla DP: 0.5274\n\u2022 DP-SAT: 0.5085\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0189 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0188 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    }
  ],
  "config": {
    "experiment_name": "positively_correlated_noise_validation",
    "base_command": "uv run main.py",
    "common_args": {
      "k": 2048,
      "epochs": 50,
      "dataset-size": 50000,
      "target-epsilon": 2.0,
      "delta": 1e-05,
      "clip-radius": 2.0,
      "dp-layer": "conv1,conv2",
      "mps": true,
      "clean": true,
      "compare-others": true,
      "run-mia": true
    },
    "experiments": [
      {
        "name": "25_users_positive",
        "users": 25,
        "positively_correlated_noise": true,
        "description": "25 users with positively correlated noise"
      },
      {
        "name": "25_users_negative",
        "users": 25,
        "negatively_correlated_noise": true,
        "description": "25 users with negatively correlated noise (default)"
      },
      {
        "name": "25_users_l2_reg",
        "users": 25,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "25 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "50_users_positive",
        "users": 50,
        "positively_correlated_noise": true,
        "description": "50 users with positively correlated noise"
      },
      {
        "name": "50_users_negative",
        "users": 50,
        "negatively_correlated_noise": true,
        "description": "50 users with negatively correlated noise (default)"
      },
      {
        "name": "50_users_l2_reg",
        "users": 50,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "50 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "100_users_positive",
        "users": 100,
        "positively_correlated_noise": true,
        "description": "100 users with positively correlated noise"
      },
      {
        "name": "100_users_negative",
        "users": 100,
        "negatively_correlated_noise": true,
        "description": "100 users with negatively correlated noise (default)"
      },
      {
        "name": "100_users_l2_reg",
        "users": 100,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "100 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "200_users_positive",
        "users": 200,
        "positively_correlated_noise": true,
        "description": "200 users with positively correlated noise"
      },
      {
        "name": "200_users_negative",
        "users": 200,
        "negatively_correlated_noise": true,
        "description": "200 users with negatively correlated noise (default)"
      },
      {
        "name": "200_users_l2_reg",
        "users": 200,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "200 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "400_users_positive",
        "users": 400,
        "positively_correlated_noise": true,
        "description": "400 users with positively correlated noise"
      },
      {
        "name": "400_users_negative",
        "users": 400,
        "negatively_correlated_noise": true,
        "description": "400 users with negatively correlated noise (default)"
      },
      {
        "name": "400_users_l2_reg",
        "users": 400,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "400 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      }
    ],
    "output_settings": {
      "results_dir": "validation_results",
      "plots_dir": "validation_plots",
      "save_logs": true,
      "figure_dpi": 300
    }
  },
  "metadata": {
    "timestamp": "20250702_134908",
    "seed": 43,
    "total_experiments": 15,
    "successful_experiments": 10,
    "failed_experiments": 5
  }
}