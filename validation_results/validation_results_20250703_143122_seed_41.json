{
  "results": [
    {
      "baseline": 74.6,
      "fisher_dp": 75.26,
      "vanilla_dp": 74.92,
      "dp_sat": 74.82,
      "fisher_vs_vanilla": 0.34,
      "fisher_vs_dp_sat": 0.44,
      "dp_sat_vs_vanilla": -0.1,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.6496,
      "vanilla_dp_confidence_auc": 0.6438,
      "dp_sat_confidence_auc": 0.6429,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5665,
      "vanilla_dp_shadow_auc": 0.5118,
      "dp_sat_shadow_auc": 0.6211,
      "fisher_dp_worst_auc": 0.6496,
      "vanilla_dp_worst_auc": 0.6438,
      "dp_sat_worst_auc": 0.6429,
      "experiment_name": "clip_0.5_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 0.5 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 0.5530\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500\n\u2022 Calibrated Mahalanobis threshold: 0.533\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.938\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=0.533 for noise (was euclidean_target=0.500)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 0.533\n\u2022 Median \u2016g_user\u2016_Mah = 607.04\n\u2022 Total noise \u2113\u2082 \u2208 [8.6,9.9]\n\u2022 Last batch noise: Fisher only=9.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=0.295 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 610.27\n\u2022 Isotropic noise \u2113\u2082 \u2208 [38.0,39.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 626.78\n\u2022 Isotropic noise \u2113\u2082 \u2208 [38.0,39.1]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  75.26% (cat 59.47%)\nVanilla DP       :  74.92% (cat 55.49%)\nDP-SAT           :  74.82% (cat 57.01%)\nFisher vs Vanilla: +0.34% improvement\nFisher vs DP-SAT : +0.44% improvement\nDP-SAT vs Vanilla: -0.10% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.6496 \u00b1 0.0000\nVanilla DP: 0.6438 \u00b1 0.0000\nDP-SAT: 0.6429 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5665 \u00b1 0.0000\nVanilla DP: 0.5118 \u00b1 0.0000\nDP-SAT: 0.6211 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6496\n\u2022 Vanilla DP: 0.6438\n\u2022 DP-SAT: 0.6429\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0067 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0009 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u26a0\ufe0f  Vanilla DP provides WEAK privacy protection!\n\u26a0\ufe0f  DP-SAT provides WEAK privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 75.0,
      "vanilla_dp": 74.92,
      "dp_sat": 74.82,
      "fisher_vs_vanilla": 0.08,
      "fisher_vs_dp_sat": 0.18,
      "dp_sat_vs_vanilla": -0.1,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.645,
      "vanilla_dp_confidence_auc": 0.6438,
      "dp_sat_confidence_auc": 0.6429,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5475,
      "vanilla_dp_shadow_auc": 0.5118,
      "dp_sat_shadow_auc": 0.6211,
      "fisher_dp_worst_auc": 0.645,
      "vanilla_dp_worst_auc": 0.6438,
      "dp_sat_worst_auc": 0.6429,
      "experiment_name": "clip_0.5_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 0.5 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 0.5530\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500\n\u2022 Calibrated Mahalanobis threshold: 0.540\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.926\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=0.540 for noise (was euclidean_target=0.500)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 0.540\n\u2022 Median \u2016g_user\u2016_Mah = 657.39\n\u2022 Total noise \u2113\u2082 \u2208 [17.2,19.9]\n\u2022 Last batch noise: Fisher only=19.5 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=0.299 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 610.27\n\u2022 Isotropic noise \u2113\u2082 \u2208 [38.0,39.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 626.78\n\u2022 Isotropic noise \u2113\u2082 \u2208 [38.0,39.1]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  75.00% (cat 58.90%)\nVanilla DP       :  74.92% (cat 55.49%)\nDP-SAT           :  74.82% (cat 57.01%)\nFisher vs Vanilla: +0.08% improvement\nFisher vs DP-SAT : +0.18% improvement\nDP-SAT vs Vanilla: -0.10% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.6450 \u00b1 0.0000\nVanilla DP: 0.6438 \u00b1 0.0000\nDP-SAT: 0.6429 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5475 \u00b1 0.0000\nVanilla DP: 0.5118 \u00b1 0.0000\nDP-SAT: 0.6211 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6450\n\u2022 Vanilla DP: 0.6438\n\u2022 DP-SAT: 0.6429\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0021 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0009 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u26a0\ufe0f  Vanilla DP provides WEAK privacy protection!\n\u26a0\ufe0f  DP-SAT provides WEAK privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 73.72,
      "vanilla_dp": 70.88,
      "dp_sat": 70.1,
      "fisher_vs_vanilla": 2.84,
      "fisher_vs_dp_sat": 3.62,
      "dp_sat_vs_vanilla": -0.78,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.6349,
      "vanilla_dp_confidence_auc": 0.6013,
      "dp_sat_confidence_auc": 0.593,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5152,
      "vanilla_dp_shadow_auc": 0.4911,
      "dp_sat_shadow_auc": 0.5425,
      "fisher_dp_worst_auc": 0.6349,
      "vanilla_dp_worst_auc": 0.6013,
      "dp_sat_worst_auc": 0.593,
      "experiment_name": "clip_1.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 1.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 1.1060\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000\n\u2022 Calibrated Mahalanobis threshold: 1.091\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.917\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=1.091 for noise (was euclidean_target=1.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 1.091\n\u2022 Median \u2016g_user\u2016_Mah = 784.15\n\u2022 Total noise \u2113\u2082 \u2208 [34.4,40.6]\n\u2022 Last batch noise: Fisher only=39.4 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=1.206 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1156.78\n\u2022 Isotropic noise \u2113\u2082 \u2208 [152.1,156.2]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1231.44\n\u2022 Isotropic noise \u2113\u2082 \u2208 [152.0,156.4]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  73.72% (cat 61.74%)\nVanilla DP       :  70.88% (cat 49.24%)\nDP-SAT           :  70.10% (cat 51.89%)\nFisher vs Vanilla: +2.84% improvement\nFisher vs DP-SAT : +3.62% improvement\nDP-SAT vs Vanilla: -0.78% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.6349 \u00b1 0.0000\nVanilla DP: 0.6013 \u00b1 0.0000\nDP-SAT: 0.5930 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5152 \u00b1 0.0000\nVanilla DP: 0.4911 \u00b1 0.0000\nDP-SAT: 0.5425 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6349\n\u2022 Vanilla DP: 0.6013\n\u2022 DP-SAT: 0.5930\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0420 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0083 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u26a0\ufe0f  Vanilla DP provides WEAK privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 70.62,
      "vanilla_dp": 70.88,
      "dp_sat": 70.1,
      "fisher_vs_vanilla": -0.26,
      "fisher_vs_dp_sat": 0.52,
      "dp_sat_vs_vanilla": -0.78,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.6007,
      "vanilla_dp_confidence_auc": 0.6013,
      "dp_sat_confidence_auc": 0.593,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.4997,
      "vanilla_dp_shadow_auc": 0.4911,
      "dp_sat_shadow_auc": 0.5425,
      "fisher_dp_worst_auc": 0.6007,
      "vanilla_dp_worst_auc": 0.6013,
      "dp_sat_worst_auc": 0.593,
      "experiment_name": "clip_1.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 1.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 1.1060\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000\n\u2022 Calibrated Mahalanobis threshold: 1.130\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.885\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=1.130 for noise (was euclidean_target=1.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 1.130\n\u2022 Median \u2016g_user\u2016_Mah = 1249.89\n\u2022 Total noise \u2113\u2082 \u2208 [68.8,83.4]\n\u2022 Last batch noise: Fisher only=81.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=1.250 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1156.78\n\u2022 Isotropic noise \u2113\u2082 \u2208 [152.1,156.2]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1231.44\n\u2022 Isotropic noise \u2113\u2082 \u2208 [152.0,156.4]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  70.62% (cat 54.92%)\nVanilla DP       :  70.88% (cat 49.24%)\nDP-SAT           :  70.10% (cat 51.89%)\nFisher vs Vanilla: -0.26% improvement\nFisher vs DP-SAT : +0.52% improvement\nDP-SAT vs Vanilla: -0.78% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.6007 \u00b1 0.0000\nVanilla DP: 0.6013 \u00b1 0.0000\nDP-SAT: 0.5930 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.4997 \u00b1 0.0000\nVanilla DP: 0.4911 \u00b1 0.0000\nDP-SAT: 0.5425 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6007\n\u2022 Vanilla DP: 0.6013\n\u2022 DP-SAT: 0.5930\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0077 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0083 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u26a0\ufe0f  Vanilla DP provides WEAK privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 56.02,
      "vanilla_dp": 38.12,
      "dp_sat": 33.4,
      "fisher_vs_vanilla": 17.9,
      "fisher_vs_dp_sat": 22.62,
      "dp_sat_vs_vanilla": -4.72,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.5473,
      "vanilla_dp_confidence_auc": 0.5242,
      "dp_sat_confidence_auc": 0.5143,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5056,
      "vanilla_dp_shadow_auc": 0.508,
      "dp_sat_shadow_auc": 0.5134,
      "fisher_dp_worst_auc": 0.5473,
      "vanilla_dp_worst_auc": 0.5242,
      "dp_sat_worst_auc": 0.5143,
      "experiment_name": "clip_2.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 2.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.367\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.845\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.367 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.367\n\u2022 Median \u2016g_user\u2016_Mah = 2033.50\n\u2022 Total noise \u2113\u2082 \u2208 [137.6,176.2]\n\u2022 Last batch noise: Fisher only=171.0 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=5.236 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2579.31\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.6,624.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2563.72\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.9,625.5]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  56.02% (cat 61.93%)\nVanilla DP       :  38.12% (cat 24.81%)\nDP-SAT           :  33.40% (cat 21.78%)\nFisher vs Vanilla: +17.90% improvement\nFisher vs DP-SAT : +22.62% improvement\nDP-SAT vs Vanilla: -4.72% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.5473 \u00b1 0.0000\nVanilla DP: 0.5242 \u00b1 0.0000\nDP-SAT: 0.5143 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5056 \u00b1 0.0000\nVanilla DP: 0.5080 \u00b1 0.0000\nDP-SAT: 0.5134 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5473\n\u2022 Vanilla DP: 0.5242\n\u2022 DP-SAT: 0.5143\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0329 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0098 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 38.7,
      "vanilla_dp": 38.12,
      "dp_sat": 33.4,
      "fisher_vs_vanilla": 0.58,
      "fisher_vs_dp_sat": 5.3,
      "dp_sat_vs_vanilla": -4.72,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.5154,
      "vanilla_dp_confidence_auc": 0.5242,
      "dp_sat_confidence_auc": 0.5143,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5164,
      "vanilla_dp_shadow_auc": 0.508,
      "dp_sat_shadow_auc": 0.5134,
      "fisher_dp_worst_auc": 0.5164,
      "vanilla_dp_worst_auc": 0.5242,
      "dp_sat_worst_auc": 0.5143,
      "experiment_name": "clip_2.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 2.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.413\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.829\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.413 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.413\n\u2022 Median \u2016g_user\u2016_Mah = 2325.86\n\u2022 Total noise \u2113\u2082 \u2208 [275.1,356.1]\n\u2022 Last batch noise: Fisher only=348.4 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=5.337 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2579.31\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.6,624.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2563.72\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.9,625.5]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  38.70% (cat 29.36%)\nVanilla DP       :  38.12% (cat 24.81%)\nDP-SAT           :  33.40% (cat 21.78%)\nFisher vs Vanilla: +0.58% improvement\nFisher vs DP-SAT : +5.30% improvement\nDP-SAT vs Vanilla: -4.72% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.5154 \u00b1 0.0000\nVanilla DP: 0.5242 \u00b1 0.0000\nDP-SAT: 0.5143 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5164 \u00b1 0.0000\nVanilla DP: 0.5080 \u00b1 0.0000\nDP-SAT: 0.5134 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5164\n\u2022 Vanilla DP: 0.5242\n\u2022 DP-SAT: 0.5143\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0020 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0098 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 18.84,
      "vanilla_dp": 19.18,
      "dp_sat": 13.78,
      "fisher_vs_vanilla": -0.34,
      "fisher_vs_dp_sat": 5.06,
      "dp_sat_vs_vanilla": -5.4,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.481,
      "vanilla_dp_confidence_auc": 0.5251,
      "dp_sat_confidence_auc": 0.5112,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5067,
      "vanilla_dp_shadow_auc": 0.5054,
      "dp_sat_shadow_auc": 0.4992,
      "fisher_dp_worst_auc": 0.5067,
      "vanilla_dp_worst_auc": 0.5251,
      "dp_sat_worst_auc": 0.5112,
      "experiment_name": "clip_5.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 5.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 5.5298\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000\n\u2022 Calibrated Mahalanobis threshold: 5.397\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.926\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=5.397 for noise (was euclidean_target=5.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 5.397\n\u2022 Median \u2016g_user\u2016_Mah = 1399.41\n\u2022 Total noise \u2113\u2082 \u2208 [860.1,1004.2]\n\u2022 Last batch noise: Fisher only=974.8 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=29.843 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1073.97\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3803.4,3904.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1000.33\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3799.1,3909.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  18.84% (cat 33.90%)\nVanilla DP       :  19.18% (cat  5.11%)\nDP-SAT           :  13.78% (cat 15.15%)\nFisher vs Vanilla: -0.34% improvement\nFisher vs DP-SAT : +5.06% improvement\nDP-SAT vs Vanilla: -5.40% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.4810 \u00b1 0.0000\nVanilla DP: 0.5251 \u00b1 0.0000\nDP-SAT: 0.5112 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5067 \u00b1 0.0000\nVanilla DP: 0.5054 \u00b1 0.0000\nDP-SAT: 0.4992 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5067\n\u2022 Vanilla DP: 0.5251\n\u2022 DP-SAT: 0.5112\n\ud83c\udfc6 Fisher DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Vanilla DP: 0.0184 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0045 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 16.96,
      "vanilla_dp": 19.18,
      "dp_sat": 13.78,
      "fisher_vs_vanilla": -2.22,
      "fisher_vs_dp_sat": 3.18,
      "dp_sat_vs_vanilla": -5.4,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.4817,
      "vanilla_dp_confidence_auc": 0.5251,
      "dp_sat_confidence_auc": 0.5112,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.4898,
      "vanilla_dp_shadow_auc": 0.5054,
      "dp_sat_shadow_auc": 0.4992,
      "fisher_dp_worst_auc": 0.4898,
      "vanilla_dp_worst_auc": 0.5251,
      "dp_sat_worst_auc": 0.5112,
      "experiment_name": "clip_5.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 5.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 5.5298\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000\n\u2022 Calibrated Mahalanobis threshold: 4.937\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.013\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=4.937 for noise (was euclidean_target=5.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 4.937\n\u2022 Median \u2016g_user\u2016_Mah = 1035.21\n\u2022 Total noise \u2113\u2082 \u2208 [1686.5,1821.7]\n\u2022 Last batch noise: Fisher only=1782.3 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=27.302 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1073.97\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3803.4,3904.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1000.33\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3799.1,3909.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  16.96% (cat 11.55%)\nVanilla DP       :  19.18% (cat  5.11%)\nDP-SAT           :  13.78% (cat 15.15%)\nFisher vs Vanilla: -2.22% improvement\nFisher vs DP-SAT : +3.18% improvement\nDP-SAT vs Vanilla: -5.40% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.4817 \u00b1 0.0000\nVanilla DP: 0.5251 \u00b1 0.0000\nDP-SAT: 0.5112 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.4898 \u00b1 0.0000\nVanilla DP: 0.5054 \u00b1 0.0000\nDP-SAT: 0.4992 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.4898\n\u2022 Vanilla DP: 0.5251\n\u2022 DP-SAT: 0.5112\n\ud83c\udfc6 Fisher DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Vanilla DP: 0.0352 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0213 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 16.24,
      "vanilla_dp": 17.4,
      "dp_sat": 12.12,
      "fisher_vs_vanilla": -1.16,
      "fisher_vs_dp_sat": 4.12,
      "dp_sat_vs_vanilla": -5.28,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.4769,
      "vanilla_dp_confidence_auc": 0.5286,
      "dp_sat_confidence_auc": 0.5027,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5064,
      "vanilla_dp_shadow_auc": 0.5071,
      "dp_sat_shadow_auc": 0.4991,
      "fisher_dp_worst_auc": 0.5064,
      "vanilla_dp_worst_auc": 0.5286,
      "dp_sat_worst_auc": 0.5027,
      "experiment_name": "clip_10.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 10.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 11.0596\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000\n\u2022 Calibrated Mahalanobis threshold: 8.879\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.126\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=8.879 for noise (was euclidean_target=10.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 8.879\n\u2022 Median \u2016g_user\u2016_Mah = 669.39\n\u2022 Total noise \u2113\u2082 \u2208 [3042.8,3622.1]\n\u2022 Last batch noise: Fisher only=3207.5 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=98.196 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 290.35\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15213.8,15618.6]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 258.92\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15196.5,15638.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  16.24% (cat 25.57%)\nVanilla DP       :  17.40% (cat  3.41%)\nDP-SAT           :  12.12% (cat 11.36%)\nFisher vs Vanilla: -1.16% improvement\nFisher vs DP-SAT : +4.12% improvement\nDP-SAT vs Vanilla: -5.28% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.4769 \u00b1 0.0000\nVanilla DP: 0.5286 \u00b1 0.0000\nDP-SAT: 0.5027 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5064 \u00b1 0.0000\nVanilla DP: 0.5071 \u00b1 0.0000\nDP-SAT: 0.4991 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5064\n\u2022 Vanilla DP: 0.5286\n\u2022 DP-SAT: 0.5027\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0037 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0259 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 15.88,
      "vanilla_dp": 17.4,
      "dp_sat": 12.12,
      "fisher_vs_vanilla": -1.52,
      "fisher_vs_dp_sat": 3.76,
      "dp_sat_vs_vanilla": -5.28,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.4729,
      "vanilla_dp_confidence_auc": 0.5286,
      "dp_sat_confidence_auc": 0.5027,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5199,
      "vanilla_dp_shadow_auc": 0.5071,
      "dp_sat_shadow_auc": 0.4991,
      "fisher_dp_worst_auc": 0.5199,
      "vanilla_dp_worst_auc": 0.5286,
      "dp_sat_worst_auc": 0.5027,
      "experiment_name": "clip_10.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 10.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 11.0596\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000\n\u2022 Calibrated Mahalanobis threshold: 8.239\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.214\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=8.239 for noise (was euclidean_target=10.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 8.239\n\u2022 Median \u2016g_user\u2016_Mah = 409.07\n\u2022 Total noise \u2113\u2082 \u2208 [5628.6,7201.8]\n\u2022 Last batch noise: Fisher only=5948.4 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=91.117 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 290.35\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15213.8,15618.6]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 258.92\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15196.5,15638.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  15.88% (cat 12.12%)\nVanilla DP       :  17.40% (cat  3.41%)\nDP-SAT           :  12.12% (cat 11.36%)\nFisher vs Vanilla: -1.52% improvement\nFisher vs DP-SAT : +3.76% improvement\nDP-SAT vs Vanilla: -5.28% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.4729 \u00b1 0.0000\nVanilla DP: 0.5286 \u00b1 0.0000\nDP-SAT: 0.5027 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5199 \u00b1 0.0000\nVanilla DP: 0.5071 \u00b1 0.0000\nDP-SAT: 0.4991 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5199\n\u2022 Vanilla DP: 0.5286\n\u2022 DP-SAT: 0.5027\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0171 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0259 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    }
  ],
  "config": {
    "experiment_name": "clip_radius_sensitivity_analysis",
    "base_command": "uv run main.py",
    "common_args": {
      "k": 2048,
      "epochs": 50,
      "dataset-size": 50000,
      "target-epsilon": 2.0,
      "delta": 1e-05,
      "users": 200,
      "dp-layer": "conv1,conv2",
      "mps": true,
      "clean": true,
      "compare-others": true,
      "run-mia": true
    },
    "experiments": [
      {
        "name": "clip_0.5_positive",
        "clip-radius": 0.5,
        "positively_correlated_noise": true,
        "description": "Clip radius 0.5 with positively correlated noise"
      },
      {
        "name": "clip_0.5_negative",
        "clip-radius": 0.5,
        "negatively_correlated_noise": true,
        "description": "Clip radius 0.5 with negatively correlated noise (default)"
      },
      {
        "name": "clip_1.0_positive",
        "clip-radius": 1.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 1.0 with positively correlated noise"
      },
      {
        "name": "clip_1.0_negative",
        "clip-radius": 1.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 1.0 with negatively correlated noise (default)"
      },
      {
        "name": "clip_2.0_positive",
        "clip-radius": 2.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 2.0 with positively correlated noise"
      },
      {
        "name": "clip_2.0_negative",
        "clip-radius": 2.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 2.0 with negatively correlated noise (default)"
      },
      {
        "name": "clip_5.0_positive",
        "clip-radius": 5.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 5.0 with positively correlated noise"
      },
      {
        "name": "clip_5.0_negative",
        "clip-radius": 5.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 5.0 with negatively correlated noise (default)"
      },
      {
        "name": "clip_10.0_positive",
        "clip-radius": 10.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 10.0 with positively correlated noise"
      },
      {
        "name": "clip_10.0_negative",
        "clip-radius": 10.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 10.0 with negatively correlated noise (default)"
      }
    ],
    "output_settings": {
      "results_dir": "validation_results",
      "plots_dir": "validation_plots",
      "save_logs": true,
      "figure_dpi": 300
    }
  },
  "metadata": {
    "timestamp": "20250703_143122",
    "seed": 41,
    "total_experiments": 10,
    "successful_experiments": 10,
    "failed_experiments": 0
  }
}