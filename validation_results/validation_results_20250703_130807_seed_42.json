{
  "results": [
    {
      "baseline": 75.02,
      "fisher_dp": 75.14,
      "vanilla_dp": 75.2,
      "dp_sat": 75.0,
      "fisher_vs_vanilla": -0.06,
      "fisher_vs_dp_sat": 0.14,
      "dp_sat_vs_vanilla": -0.2,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.6535,
      "vanilla_dp_confidence_auc": 0.6488,
      "dp_sat_confidence_auc": 0.6471,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.6326,
      "vanilla_dp_shadow_auc": 0.627,
      "dp_sat_shadow_auc": 0.3942,
      "fisher_dp_worst_auc": 0.6535,
      "vanilla_dp_worst_auc": 0.6488,
      "dp_sat_worst_auc": 0.6471,
      "experiment_name": "clip_0.5_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 0.5 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 0.5530\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500\n\u2022 Calibrated Mahalanobis threshold: 0.450\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.112\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=0.450 for noise (was euclidean_target=0.500)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 0.450\n\u2022 Median \u2016g_user\u2016_Mah = 465.30\n\u2022 Total noise \u2113\u2082 \u2208 [7.6,8.9]\n\u2022 Last batch noise: Fisher only=7.9 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=0.249 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 597.12\n\u2022 Isotropic noise \u2113\u2082 \u2208 [37.8,39.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 596.05\n\u2022 Isotropic noise \u2113\u2082 \u2208 [38.1,38.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  75.14% (cat 57.97%)\nVanilla DP       :  75.20% (cat 58.76%)\nDP-SAT           :  75.00% (cat 54.78%)\nFisher vs Vanilla: -0.06% improvement\nFisher vs DP-SAT : +0.14% improvement\nDP-SAT vs Vanilla: -0.20% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.6535 \u00b1 0.0000\nVanilla DP: 0.6488 \u00b1 0.0000\nDP-SAT: 0.6471 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.6326 \u00b1 0.0000\nVanilla DP: 0.6270 \u00b1 0.0000\nDP-SAT: 0.3942 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6535\n\u2022 Vanilla DP: 0.6488\n\u2022 DP-SAT: 0.6471\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0064 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0017 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u26a0\ufe0f  Vanilla DP provides WEAK privacy protection!\n\u26a0\ufe0f  DP-SAT provides WEAK privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 74.94,
      "vanilla_dp": 75.2,
      "dp_sat": 75.0,
      "fisher_vs_vanilla": -0.26,
      "fisher_vs_dp_sat": -0.06,
      "dp_sat_vs_vanilla": -0.2,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.6518,
      "vanilla_dp_confidence_auc": 0.6488,
      "dp_sat_confidence_auc": 0.6471,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.6303,
      "vanilla_dp_shadow_auc": 0.627,
      "dp_sat_shadow_auc": 0.3942,
      "fisher_dp_worst_auc": 0.6518,
      "vanilla_dp_worst_auc": 0.6488,
      "dp_sat_worst_auc": 0.6471,
      "experiment_name": "clip_0.5_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 0.5 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 0.5530\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500\n\u2022 Calibrated Mahalanobis threshold: 0.459\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.090\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=0.459 for noise (was euclidean_target=0.500)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 0.500 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 0.459\n\u2022 Median \u2016g_user\u2016_Mah = 503.71\n\u2022 Total noise \u2113\u2082 \u2208 [15.5,17.8]\n\u2022 Last batch noise: Fisher only=16.2 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=0.254 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 597.12\n\u2022 Isotropic noise \u2113\u2082 \u2208 [37.8,39.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 0.5530\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=0.5530\n\u2022 Euclidean clipping with radius 0.5\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 596.05\n\u2022 Isotropic noise \u2113\u2082 \u2208 [38.1,38.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  74.94% (cat 57.97%)\nVanilla DP       :  75.20% (cat 58.76%)\nDP-SAT           :  75.00% (cat 54.78%)\nFisher vs Vanilla: -0.26% improvement\nFisher vs DP-SAT : -0.06% improvement\nDP-SAT vs Vanilla: -0.20% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.6518 \u00b1 0.0000\nVanilla DP: 0.6488 \u00b1 0.0000\nDP-SAT: 0.6471 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.6303 \u00b1 0.0000\nVanilla DP: 0.6270 \u00b1 0.0000\nDP-SAT: 0.3942 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6518\n\u2022 Vanilla DP: 0.6488\n\u2022 DP-SAT: 0.6471\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0047 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0017 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u26a0\ufe0f  Vanilla DP provides WEAK privacy protection!\n\u26a0\ufe0f  DP-SAT provides WEAK privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 75.02,
      "vanilla_dp": 71.02,
      "dp_sat": 70.68,
      "fisher_vs_vanilla": 4.0,
      "fisher_vs_dp_sat": 4.34,
      "dp_sat_vs_vanilla": -0.34,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.6484,
      "vanilla_dp_confidence_auc": 0.5957,
      "dp_sat_confidence_auc": 0.6031,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.6246,
      "vanilla_dp_shadow_auc": 0.5489,
      "dp_sat_shadow_auc": 0.4497,
      "fisher_dp_worst_auc": 0.6484,
      "vanilla_dp_worst_auc": 0.5957,
      "dp_sat_worst_auc": 0.6031,
      "experiment_name": "clip_1.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 1.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 1.1060\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000\n\u2022 Calibrated Mahalanobis threshold: 0.905\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.105\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=0.905 for noise (was euclidean_target=1.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 0.905\n\u2022 Median \u2016g_user\u2016_Mah = 564.99\n\u2022 Total noise \u2113\u2082 \u2208 [30.5,35.7]\n\u2022 Last batch noise: Fisher only=32.0 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=1.001 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1209.56\n\u2022 Isotropic noise \u2113\u2082 \u2208 [151.4,155.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1118.11\n\u2022 Isotropic noise \u2113\u2082 \u2208 [152.4,155.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  75.02% (cat 59.16%)\nVanilla DP       :  71.02% (cat 53.98%)\nDP-SAT           :  70.68% (cat 46.61%)\nFisher vs Vanilla: +4.00% improvement\nFisher vs DP-SAT : +4.34% improvement\nDP-SAT vs Vanilla: -0.34% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.6484 \u00b1 0.0000\nVanilla DP: 0.5957 \u00b1 0.0000\nDP-SAT: 0.6031 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.6246 \u00b1 0.0000\nVanilla DP: 0.5489 \u00b1 0.0000\nDP-SAT: 0.4497 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6484\n\u2022 Vanilla DP: 0.5957\n\u2022 DP-SAT: 0.6031\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0528 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0075 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u26a0\ufe0f  DP-SAT provides WEAK privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 73.46,
      "vanilla_dp": 71.02,
      "dp_sat": 70.68,
      "fisher_vs_vanilla": 2.44,
      "fisher_vs_dp_sat": 2.78,
      "dp_sat_vs_vanilla": -0.34,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.6283,
      "vanilla_dp_confidence_auc": 0.5957,
      "dp_sat_confidence_auc": 0.6031,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.5939,
      "vanilla_dp_shadow_auc": 0.5489,
      "dp_sat_shadow_auc": 0.4497,
      "fisher_dp_worst_auc": 0.6283,
      "vanilla_dp_worst_auc": 0.5957,
      "dp_sat_worst_auc": 0.6031,
      "experiment_name": "clip_1.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 1.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 1.1060\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000\n\u2022 Calibrated Mahalanobis threshold: 0.965\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.036\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=0.965 for noise (was euclidean_target=1.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 1.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 0.965\n\u2022 Median \u2016g_user\u2016_Mah = 933.07\n\u2022 Total noise \u2113\u2082 \u2208 [65.1,72.2]\n\u2022 Last batch noise: Fisher only=68.2 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=1.067 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1209.56\n\u2022 Isotropic noise \u2113\u2082 \u2208 [151.4,155.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.1060\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.1060\n\u2022 Euclidean clipping with radius 1.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1118.11\n\u2022 Isotropic noise \u2113\u2082 \u2208 [152.4,155.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  73.46% (cat 52.99%)\nVanilla DP       :  71.02% (cat 53.98%)\nDP-SAT           :  70.68% (cat 46.61%)\nFisher vs Vanilla: +2.44% improvement\nFisher vs DP-SAT : +2.78% improvement\nDP-SAT vs Vanilla: -0.34% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.6283 \u00b1 0.0000\nVanilla DP: 0.5957 \u00b1 0.0000\nDP-SAT: 0.6031 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.5939 \u00b1 0.0000\nVanilla DP: 0.5489 \u00b1 0.0000\nDP-SAT: 0.4497 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.6283\n\u2022 Vanilla DP: 0.5957\n\u2022 DP-SAT: 0.6031\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0326 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0075 AUC reduction\n\u26a0\ufe0f  Fisher DP provides WEAK privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u26a0\ufe0f  DP-SAT provides WEAK privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 68.06,
      "vanilla_dp": 37.2,
      "dp_sat": 37.56,
      "fisher_vs_vanilla": 30.86,
      "fisher_vs_dp_sat": 30.5,
      "dp_sat_vs_vanilla": 0.36,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.5906,
      "vanilla_dp_confidence_auc": 0.4966,
      "dp_sat_confidence_auc": 0.5134,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.5398,
      "vanilla_dp_shadow_auc": 0.4975,
      "dp_sat_shadow_auc": 0.5162,
      "fisher_dp_worst_auc": 0.5906,
      "vanilla_dp_worst_auc": 0.4975,
      "dp_sat_worst_auc": 0.5162,
      "experiment_name": "clip_2.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 2.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.119\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.944\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.119 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.119\n\u2022 Median \u2016g_user\u2016_Mah = 1566.70\n\u2022 Total noise \u2113\u2082 \u2208 [138.9,158.5]\n\u2022 Last batch noise: Fisher only=149.7 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=4.688 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2517.97\n\u2022 Isotropic noise \u2113\u2082 \u2208 [605.5,623.4]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2505.01\n\u2022 Isotropic noise \u2113\u2082 \u2208 [609.5,623.2]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  68.06% (cat 47.21%)\nVanilla DP       :  37.20% (cat 26.69%)\nDP-SAT           :  37.56% (cat 16.93%)\nFisher vs Vanilla: +30.86% improvement\nFisher vs DP-SAT : +30.50% improvement\nDP-SAT vs Vanilla: +0.36% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.5906 \u00b1 0.0000\nVanilla DP: 0.4966 \u00b1 0.0000\nDP-SAT: 0.5134 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.5398 \u00b1 0.0000\nVanilla DP: 0.4975 \u00b1 0.0000\nDP-SAT: 0.5162 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5906\n\u2022 Vanilla DP: 0.4975\n\u2022 DP-SAT: 0.5162\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0930 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0186 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 50.92,
      "vanilla_dp": 37.2,
      "dp_sat": 37.56,
      "fisher_vs_vanilla": 13.72,
      "fisher_vs_dp_sat": 13.36,
      "dp_sat_vs_vanilla": 0.36,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.543,
      "vanilla_dp_confidence_auc": 0.4966,
      "dp_sat_confidence_auc": 0.5134,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.5035,
      "vanilla_dp_shadow_auc": 0.4975,
      "dp_sat_shadow_auc": 0.5162,
      "fisher_dp_worst_auc": 0.543,
      "vanilla_dp_worst_auc": 0.4975,
      "dp_sat_worst_auc": 0.5162,
      "experiment_name": "clip_2.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 2.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.234\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.895\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.234 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.234\n\u2022 Median \u2016g_user\u2016_Mah = 2588.66\n\u2022 Total noise \u2113\u2082 \u2208 [277.9,334.0]\n\u2022 Last batch noise: Fisher only=315.5 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=4.940 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2517.97\n\u2022 Isotropic noise \u2113\u2082 \u2208 [605.5,623.4]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2505.01\n\u2022 Isotropic noise \u2113\u2082 \u2208 [609.5,623.2]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  50.92% (cat 37.25%)\nVanilla DP       :  37.20% (cat 26.69%)\nDP-SAT           :  37.56% (cat 16.93%)\nFisher vs Vanilla: +13.72% improvement\nFisher vs DP-SAT : +13.36% improvement\nDP-SAT vs Vanilla: +0.36% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.5430 \u00b1 0.0000\nVanilla DP: 0.4966 \u00b1 0.0000\nDP-SAT: 0.5134 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.5035 \u00b1 0.0000\nVanilla DP: 0.4975 \u00b1 0.0000\nDP-SAT: 0.5162 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5430\n\u2022 Vanilla DP: 0.4975\n\u2022 DP-SAT: 0.5162\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0454 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0186 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 34.52,
      "vanilla_dp": 18.46,
      "dp_sat": 14.22,
      "fisher_vs_vanilla": 16.06,
      "fisher_vs_dp_sat": 20.3,
      "dp_sat_vs_vanilla": -4.24,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.5196,
      "vanilla_dp_confidence_auc": 0.4828,
      "dp_sat_confidence_auc": 0.489,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.4996,
      "vanilla_dp_shadow_auc": 0.5155,
      "dp_sat_shadow_auc": 0.5026,
      "fisher_dp_worst_auc": 0.5196,
      "vanilla_dp_worst_auc": 0.5155,
      "dp_sat_worst_auc": 0.5026,
      "experiment_name": "clip_5.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 5.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 5.5298\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000\n\u2022 Calibrated Mahalanobis threshold: 4.958\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.008\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=4.958 for noise (was euclidean_target=5.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 4.958\n\u2022 Median \u2016g_user\u2016_Mah = 1581.36\n\u2022 Total noise \u2113\u2082 \u2208 [836.4,926.8]\n\u2022 Last batch noise: Fisher only=875.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=27.418 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 907.91\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3784.2,3896.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1082.75\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3809.3,3894.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  34.52% (cat 21.31%)\nVanilla DP       :  18.46% (cat 11.16%)\nDP-SAT           :  14.22% (cat  8.37%)\nFisher vs Vanilla: +16.06% improvement\nFisher vs DP-SAT : +20.30% improvement\nDP-SAT vs Vanilla: -4.24% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.5196 \u00b1 0.0000\nVanilla DP: 0.4828 \u00b1 0.0000\nDP-SAT: 0.4890 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.4996 \u00b1 0.0000\nVanilla DP: 0.5155 \u00b1 0.0000\nDP-SAT: 0.5026 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5196\n\u2022 Vanilla DP: 0.5155\n\u2022 DP-SAT: 0.5026\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0170 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0129 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 30.58,
      "vanilla_dp": 18.46,
      "dp_sat": 14.22,
      "fisher_vs_vanilla": 12.12,
      "fisher_vs_dp_sat": 16.36,
      "dp_sat_vs_vanilla": -4.24,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.509,
      "vanilla_dp_confidence_auc": 0.4828,
      "dp_sat_confidence_auc": 0.489,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.4948,
      "vanilla_dp_shadow_auc": 0.5155,
      "dp_sat_shadow_auc": 0.5026,
      "fisher_dp_worst_auc": 0.509,
      "vanilla_dp_worst_auc": 0.5155,
      "dp_sat_worst_auc": 0.5026,
      "experiment_name": "clip_5.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 5.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 5.5298\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000\n\u2022 Calibrated Mahalanobis threshold: 4.231\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 1.182\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=4.231 for noise (was euclidean_target=5.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 5.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 4.231\n\u2022 Median \u2016g_user\u2016_Mah = 850.45\n\u2022 Total noise \u2113\u2082 \u2208 [1427.4,1783.7]\n\u2022 Last batch noise: Fisher only=1494.3 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=23.396 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 907.91\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3784.2,3896.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.5298\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.5298\n\u2022 Euclidean clipping with radius 5.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1082.75\n\u2022 Isotropic noise \u2113\u2082 \u2208 [3809.3,3894.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  30.58% (cat 14.34%)\nVanilla DP       :  18.46% (cat 11.16%)\nDP-SAT           :  14.22% (cat  8.37%)\nFisher vs Vanilla: +12.12% improvement\nFisher vs DP-SAT : +16.36% improvement\nDP-SAT vs Vanilla: -4.24% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.5090 \u00b1 0.0000\nVanilla DP: 0.4828 \u00b1 0.0000\nDP-SAT: 0.4890 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.4948 \u00b1 0.0000\nVanilla DP: 0.5155 \u00b1 0.0000\nDP-SAT: 0.5026 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5090\n\u2022 Vanilla DP: 0.5155\n\u2022 DP-SAT: 0.5026\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0064 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0129 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 31.66,
      "vanilla_dp": 17.82,
      "dp_sat": 13.26,
      "fisher_vs_vanilla": 13.84,
      "fisher_vs_dp_sat": 18.4,
      "dp_sat_vs_vanilla": -4.56,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.5105,
      "vanilla_dp_confidence_auc": 0.483,
      "dp_sat_confidence_auc": 0.4941,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.4797,
      "vanilla_dp_shadow_auc": 0.5098,
      "dp_sat_shadow_auc": 0.5138,
      "fisher_dp_worst_auc": 0.5105,
      "vanilla_dp_worst_auc": 0.5098,
      "dp_sat_worst_auc": 0.5138,
      "experiment_name": "clip_10.0_positive",
      "users": 0,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 10.0 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 11.0596\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000\n\u2022 Calibrated Mahalanobis threshold: 4.890\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 2.045\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=4.890 for noise (was euclidean_target=10.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 4.890\n\u2022 Median \u2016g_user\u2016_Mah = 697.22\n\u2022 Total noise \u2113\u2082 \u2208 [1649.6,3567.4]\n\u2022 Last batch noise: Fisher only=1726.9 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=54.076 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 245.00\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15136.9,15583.9]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 294.00\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15237.1,15579.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  31.66% (cat 10.36%)\nVanilla DP       :  17.82% (cat 10.96%)\nDP-SAT           :  13.26% (cat  8.96%)\nFisher vs Vanilla: +13.84% improvement\nFisher vs DP-SAT : +18.40% improvement\nDP-SAT vs Vanilla: -4.56% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.5105 \u00b1 0.0000\nVanilla DP: 0.4830 \u00b1 0.0000\nDP-SAT: 0.4941 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.4797 \u00b1 0.0000\nVanilla DP: 0.5098 \u00b1 0.0000\nDP-SAT: 0.5138 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5105\n\u2022 Vanilla DP: 0.5098\n\u2022 DP-SAT: 0.5138\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0007 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0041 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.02,
      "fisher_dp": 32.36,
      "vanilla_dp": 17.82,
      "dp_sat": 13.26,
      "fisher_vs_vanilla": 14.54,
      "fisher_vs_dp_sat": 19.1,
      "dp_sat_vs_vanilla": -4.56,
      "baseline_confidence_auc": 0.6495,
      "fisher_dp_confidence_auc": 0.5054,
      "vanilla_dp_confidence_auc": 0.483,
      "dp_sat_confidence_auc": 0.4941,
      "baseline_shadow_auc": 0.6748,
      "fisher_dp_shadow_auc": 0.4796,
      "vanilla_dp_shadow_auc": 0.5098,
      "dp_sat_shadow_auc": 0.5138,
      "fisher_dp_worst_auc": 0.5054,
      "vanilla_dp_worst_auc": 0.5098,
      "dp_sat_worst_auc": 0.5138,
      "experiment_name": "clip_10.0_negative",
      "users": 0,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --users 200 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --clip-radius 10.0 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 42\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.88e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 11.0596\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000\n\u2022 Calibrated Mahalanobis threshold: 2.779\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 3.598\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.779 for noise (was euclidean_target=10.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 10.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.779\n\u2022 Median \u2016g_user\u2016_Mah = 575.36\n\u2022 Total noise \u2113\u2082 \u2208 [1875.2,7134.9]\n\u2022 Last batch noise: Fisher only=1963.1 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=30.736 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 245.00\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15136.9,15583.9]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 11.0596\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=11.0596\n\u2022 Euclidean clipping with radius 10.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 294.00\n\u2022 Isotropic noise \u2113\u2082 \u2208 [15237.1,15579.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.02% (cat 56.97%)\nFisher DP        :  32.36% (cat  9.16%)\nVanilla DP       :  17.82% (cat 10.96%)\nDP-SAT           :  13.26% (cat  8.96%)\nFisher vs Vanilla: +14.54% improvement\nFisher vs DP-SAT : +19.10% improvement\nDP-SAT vs Vanilla: -4.56% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 42\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6495 \u00b1 0.0000\nFisher DP: 0.5054 \u00b1 0.0000\nVanilla DP: 0.4830 \u00b1 0.0000\nDP-SAT: 0.4941 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6748 \u00b1 0.0000\nFisher DP: 0.4796 \u00b1 0.0000\nVanilla DP: 0.5098 \u00b1 0.0000\nDP-SAT: 0.5138 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5054\n\u2022 Vanilla DP: 0.5098\n\u2022 DP-SAT: 0.5138\n\ud83c\udfc6 Fisher DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Vanilla DP: 0.0044 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0084 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    }
  ],
  "config": {
    "experiment_name": "clip_radius_sensitivity_analysis",
    "base_command": "uv run main.py",
    "common_args": {
      "k": 2048,
      "epochs": 50,
      "dataset-size": 50000,
      "target-epsilon": 2.0,
      "delta": 1e-05,
      "users": 200,
      "dp-layer": "conv1,conv2",
      "mps": true,
      "clean": true,
      "compare-others": true,
      "run-mia": true
    },
    "experiments": [
      {
        "name": "clip_0.5_positive",
        "clip-radius": 0.5,
        "positively_correlated_noise": true,
        "description": "Clip radius 0.5 with positively correlated noise"
      },
      {
        "name": "clip_0.5_negative",
        "clip-radius": 0.5,
        "negatively_correlated_noise": true,
        "description": "Clip radius 0.5 with negatively correlated noise (default)"
      },
      {
        "name": "clip_1.0_positive",
        "clip-radius": 1.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 1.0 with positively correlated noise"
      },
      {
        "name": "clip_1.0_negative",
        "clip-radius": 1.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 1.0 with negatively correlated noise (default)"
      },
      {
        "name": "clip_2.0_positive",
        "clip-radius": 2.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 2.0 with positively correlated noise"
      },
      {
        "name": "clip_2.0_negative",
        "clip-radius": 2.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 2.0 with negatively correlated noise (default)"
      },
      {
        "name": "clip_5.0_positive",
        "clip-radius": 5.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 5.0 with positively correlated noise"
      },
      {
        "name": "clip_5.0_negative",
        "clip-radius": 5.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 5.0 with negatively correlated noise (default)"
      },
      {
        "name": "clip_10.0_positive",
        "clip-radius": 10.0,
        "positively_correlated_noise": true,
        "description": "Clip radius 10.0 with positively correlated noise"
      },
      {
        "name": "clip_10.0_negative",
        "clip-radius": 10.0,
        "negatively_correlated_noise": true,
        "description": "Clip radius 10.0 with negatively correlated noise (default)"
      }
    ],
    "output_settings": {
      "results_dir": "validation_results",
      "plots_dir": "validation_plots",
      "save_logs": true,
      "figure_dpi": 300
    }
  },
  "metadata": {
    "timestamp": "20250703_130807",
    "seed": 42,
    "total_experiments": 10,
    "successful_experiments": 10,
    "failed_experiments": 0
  }
}