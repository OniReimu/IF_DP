{
  "results": [
    {
      "baseline": 69.46,
      "fisher_dp": 68.3,
      "vanilla_dp": 61.22,
      "dp_sat": 61.36,
      "fisher_vs_vanilla": 7.08,
      "fisher_vs_dp_sat": 6.94,
      "dp_sat_vs_vanilla": 0.14,
      "baseline_confidence_auc": 0.5486,
      "fisher_dp_confidence_auc": 0.5442,
      "vanilla_dp_confidence_auc": 0.543,
      "dp_sat_confidence_auc": 0.5384,
      "baseline_shadow_auc": 0.5235,
      "fisher_dp_shadow_auc": 0.4993,
      "vanilla_dp_shadow_auc": 0.4872,
      "dp_sat_shadow_auc": 0.5155,
      "fisher_dp_worst_auc": 0.5442,
      "vanilla_dp_worst_auc": 0.543,
      "dp_sat_worst_auc": 0.5384,
      "experiment_name": "25_users_positive",
      "users": 25,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 25 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (25 synthetic users)\n\u25b6  25 synthetic users (2000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([25, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 9.89e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0005\n\u2022 Required noise multiplier: 0.6152\n\u2022 Sigma (for both methods): 1.2305\n\u2022 Total steps: 1250\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.492\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.802\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.492 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.492\n\u2022 Median \u2016g_user\u2016_Mah = 4429.61\n\u2022 Total noise \u2113\u2082 \u2208 [78.0,102.5]\n\u2022 Last batch noise: Fisher only=97.4 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.067 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5369.55\n\u2022 Isotropic noise \u2113\u2082 \u2208 [337.5,347.3]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5749.73\n\u2022 Isotropic noise \u2113\u2082 \u2208 [337.9,344.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udcca  Accuracy summary (User-level (25 users) DP)\nbaseline         :  69.46% (cat 47.92%)\nFisher DP        :  68.30% (cat 44.32%)\nVanilla DP       :  61.22% (cat 46.59%)\nDP-SAT           :  61.36% (cat 40.15%)\nFisher vs Vanilla: +7.08% improvement\nFisher vs DP-SAT : +6.94% improvement\nDP-SAT vs Vanilla: +0.14% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5486 \u00b1 0.0000\nFisher DP: 0.5442 \u00b1 0.0000\nVanilla DP: 0.5430 \u00b1 0.0000\nDP-SAT: 0.5384 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5235 \u00b1 0.0000\nFisher DP: 0.4993 \u00b1 0.0000\nVanilla DP: 0.4872 \u00b1 0.0000\nDP-SAT: 0.5155 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5442\n\u2022 Vanilla DP: 0.5430\n\u2022 DP-SAT: 0.5384\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0058 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0046 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 69.42,
      "fisher_dp": 66.64,
      "vanilla_dp": 61.18,
      "dp_sat": 61.4,
      "fisher_vs_vanilla": 5.46,
      "fisher_vs_dp_sat": 5.24,
      "dp_sat_vs_vanilla": 0.22,
      "baseline_confidence_auc": 0.5486,
      "fisher_dp_confidence_auc": 0.5447,
      "vanilla_dp_confidence_auc": 0.543,
      "dp_sat_confidence_auc": 0.5384,
      "baseline_shadow_auc": 0.5235,
      "fisher_dp_shadow_auc": 0.4972,
      "vanilla_dp_shadow_auc": 0.487,
      "dp_sat_shadow_auc": 0.5155,
      "fisher_dp_worst_auc": 0.5447,
      "vanilla_dp_worst_auc": 0.543,
      "dp_sat_worst_auc": 0.5384,
      "experiment_name": "25_users_negative",
      "users": 25,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 25 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (25 synthetic users)\n\u25b6  25 synthetic users (2000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([25, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.03e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0005\n\u2022 Required noise multiplier: 0.6152\n\u2022 Sigma (for both methods): 1.2305\n\u2022 Total steps: 1250\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.287] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.195\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.911\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.195 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.195\n\u2022 Median \u2016g_user\u2016_Mah = 6811.16\n\u2022 Total noise \u2113\u2082 \u2208 [155.9,180.4]\n\u2022 Last batch noise: Fisher only=171.5 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=2.701 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5401.88\n\u2022 Isotropic noise \u2113\u2082 \u2208 [337.5,347.3]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5730.38\n\u2022 Isotropic noise \u2113\u2082 \u2208 [337.9,344.6]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udcca  Accuracy summary (User-level (25 users) DP)\nbaseline         :  69.42% (cat 47.92%)\nFisher DP        :  66.64% (cat 46.59%)\nVanilla DP       :  61.18% (cat 46.40%)\nDP-SAT           :  61.40% (cat 39.96%)\nFisher vs Vanilla: +5.46% improvement\nFisher vs DP-SAT : +5.24% improvement\nDP-SAT vs Vanilla: +0.22% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5486 \u00b1 0.0000\nFisher DP: 0.5447 \u00b1 0.0000\nVanilla DP: 0.5430 \u00b1 0.0000\nDP-SAT: 0.5384 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5235 \u00b1 0.0000\nFisher DP: 0.4972 \u00b1 0.0000\nVanilla DP: 0.4870 \u00b1 0.0000\nDP-SAT: 0.5155 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5447\n\u2022 Vanilla DP: 0.5430\n\u2022 DP-SAT: 0.5384\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0063 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0046 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.42,
      "fisher_dp": 62.68,
      "vanilla_dp": 64.74,
      "dp_sat": 62.38,
      "fisher_vs_vanilla": -2.06,
      "fisher_vs_dp_sat": 0.3,
      "dp_sat_vs_vanilla": -2.36,
      "baseline_confidence_auc": 0.5714,
      "fisher_dp_confidence_auc": 0.5365,
      "vanilla_dp_confidence_auc": 0.5416,
      "dp_sat_confidence_auc": 0.5323,
      "baseline_shadow_auc": 0.538,
      "fisher_dp_shadow_auc": 0.4984,
      "vanilla_dp_shadow_auc": 0.5048,
      "dp_sat_shadow_auc": 0.5175,
      "fisher_dp_worst_auc": 0.5365,
      "vanilla_dp_worst_auc": 0.5416,
      "dp_sat_worst_auc": 0.5323,
      "experiment_name": "50_users_positive",
      "users": 50,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 50 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (50 synthetic users)\n\u25b6  50 synthetic users (1000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([50, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.63e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0010\n\u2022 Required noise multiplier: 0.6677\n\u2022 Sigma (for both methods): 1.3354\n\u2022 Total steps: 2500\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 16.280] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.234\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.895\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.234 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.234\n\u2022 Median \u2016g_user\u2016_Mah = 5429.99\n\u2022 Total noise \u2113\u2082 \u2208 [85.3,100.4]\n\u2022 Last batch noise: Fisher only=99.2 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=2.984 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4054.58\n\u2022 Isotropic noise \u2113\u2082 \u2208 [365.2,376.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4447.08\n\u2022 Isotropic noise \u2113\u2082 \u2208 [367.8,377.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udcca  Accuracy summary (User-level (50 users) DP)\nbaseline         :  73.42% (cat 51.70%)\nFisher DP        :  62.68% (cat 51.89%)\nVanilla DP       :  64.74% (cat 48.11%)\nDP-SAT           :  62.38% (cat 42.61%)\nFisher vs Vanilla: -2.06% improvement\nFisher vs DP-SAT : +0.30% improvement\nDP-SAT vs Vanilla: -2.36% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5714 \u00b1 0.0000\nFisher DP: 0.5365 \u00b1 0.0000\nVanilla DP: 0.5416 \u00b1 0.0000\nDP-SAT: 0.5323 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5380 \u00b1 0.0000\nFisher DP: 0.4984 \u00b1 0.0000\nVanilla DP: 0.5048 \u00b1 0.0000\nDP-SAT: 0.5175 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5365\n\u2022 Vanilla DP: 0.5416\n\u2022 DP-SAT: 0.5323\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0042 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0093 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.42,
      "fisher_dp": 60.58,
      "vanilla_dp": 64.74,
      "dp_sat": 62.38,
      "fisher_vs_vanilla": -4.16,
      "fisher_vs_dp_sat": -1.8,
      "dp_sat_vs_vanilla": -2.36,
      "baseline_confidence_auc": 0.5714,
      "fisher_dp_confidence_auc": 0.5434,
      "vanilla_dp_confidence_auc": 0.5416,
      "dp_sat_confidence_auc": 0.5323,
      "baseline_shadow_auc": 0.538,
      "fisher_dp_shadow_auc": 0.5214,
      "vanilla_dp_shadow_auc": 0.5048,
      "dp_sat_shadow_auc": 0.5175,
      "fisher_dp_worst_auc": 0.5434,
      "vanilla_dp_worst_auc": 0.5416,
      "dp_sat_worst_auc": 0.5323,
      "experiment_name": "50_users_negative",
      "users": 50,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 50 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (50 synthetic users)\n\u25b6  50 synthetic users (1000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([50, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.63e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0010\n\u2022 Required noise multiplier: 0.6677\n\u2022 Sigma (for both methods): 1.3354\n\u2022 Total steps: 2500\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 16.280] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.446\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.818\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.446 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.446\n\u2022 Median \u2016g_user\u2016_Mah = 6011.40\n\u2022 Total noise \u2113\u2082 \u2208 [169.2,216.7]\n\u2022 Last batch noise: Fisher only=216.7 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.267 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4054.58\n\u2022 Isotropic noise \u2113\u2082 \u2208 [365.2,376.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4447.08\n\u2022 Isotropic noise \u2113\u2082 \u2208 [367.8,377.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udcca  Accuracy summary (User-level (50 users) DP)\nbaseline         :  73.42% (cat 51.70%)\nFisher DP        :  60.58% (cat 49.05%)\nVanilla DP       :  64.74% (cat 48.11%)\nDP-SAT           :  62.38% (cat 42.61%)\nFisher vs Vanilla: -4.16% improvement\nFisher vs DP-SAT : -1.80% improvement\nDP-SAT vs Vanilla: -2.36% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5714 \u00b1 0.0000\nFisher DP: 0.5434 \u00b1 0.0000\nVanilla DP: 0.5416 \u00b1 0.0000\nDP-SAT: 0.5323 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5380 \u00b1 0.0000\nFisher DP: 0.5214 \u00b1 0.0000\nVanilla DP: 0.5048 \u00b1 0.0000\nDP-SAT: 0.5175 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5434\n\u2022 Vanilla DP: 0.5416\n\u2022 DP-SAT: 0.5323\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0112 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0093 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.4,
      "fisher_dp": 52.74,
      "vanilla_dp": 51.12,
      "dp_sat": 55.46,
      "fisher_vs_vanilla": 1.62,
      "fisher_vs_dp_sat": -2.72,
      "dp_sat_vs_vanilla": 4.34,
      "baseline_confidence_auc": 0.6104,
      "fisher_dp_confidence_auc": 0.5429,
      "vanilla_dp_confidence_auc": 0.528,
      "dp_sat_confidence_auc": 0.5383,
      "baseline_shadow_auc": 0.5699,
      "fisher_dp_shadow_auc": 0.5086,
      "vanilla_dp_shadow_auc": 0.4918,
      "dp_sat_shadow_auc": 0.4972,
      "fisher_dp_worst_auc": 0.5429,
      "vanilla_dp_worst_auc": 0.528,
      "dp_sat_worst_auc": 0.5383,
      "experiment_name": "100_users_positive",
      "users": 100,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 100 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (100 synthetic users)\n\u25b6  100 synthetic users (500.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([100, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 3.49e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0020\n\u2022 Required noise multiplier: 0.7629\n\u2022 Sigma (for both methods): 1.5259\n\u2022 Total steps: 5000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 34.882] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.401\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.833\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.401 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.401\n\u2022 Median \u2016g_user\u2016_Mah = 3318.17\n\u2022 Total noise \u2113\u2082 \u2208 [97.5,128.6]\n\u2022 Last batch noise: Fisher only=122.9 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.663 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3467.54\n\u2022 Isotropic noise \u2113\u2082 \u2208 [419.1,432.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3330.90\n\u2022 Isotropic noise \u2113\u2082 \u2208 [420.2,430.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udcca  Accuracy summary (User-level (100 users) DP)\nbaseline         :  74.40% (cat 54.17%)\nFisher DP        :  52.74% (cat 39.20%)\nVanilla DP       :  51.12% (cat 30.49%)\nDP-SAT           :  55.46% (cat 34.09%)\nFisher vs Vanilla: +1.62% improvement\nFisher vs DP-SAT : -2.72% improvement\nDP-SAT vs Vanilla: +4.34% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6104 \u00b1 0.0000\nFisher DP: 0.5429 \u00b1 0.0000\nVanilla DP: 0.5280 \u00b1 0.0000\nDP-SAT: 0.5383 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5699 \u00b1 0.0000\nFisher DP: 0.5086 \u00b1 0.0000\nVanilla DP: 0.4918 \u00b1 0.0000\nDP-SAT: 0.4972 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5429\n\u2022 Vanilla DP: 0.5280\n\u2022 DP-SAT: 0.5383\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0149 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0102 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.4,
      "fisher_dp": 49.94,
      "vanilla_dp": 51.12,
      "dp_sat": 55.46,
      "fisher_vs_vanilla": -1.18,
      "fisher_vs_dp_sat": -5.52,
      "dp_sat_vs_vanilla": 4.34,
      "baseline_confidence_auc": 0.6104,
      "fisher_dp_confidence_auc": 0.5374,
      "vanilla_dp_confidence_auc": 0.528,
      "dp_sat_confidence_auc": 0.5383,
      "baseline_shadow_auc": 0.5699,
      "fisher_dp_shadow_auc": 0.4858,
      "vanilla_dp_shadow_auc": 0.4918,
      "dp_sat_shadow_auc": 0.4972,
      "fisher_dp_worst_auc": 0.5374,
      "vanilla_dp_worst_auc": 0.528,
      "dp_sat_worst_auc": 0.5383,
      "experiment_name": "100_users_negative",
      "users": 100,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 100 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (100 synthetic users)\n\u25b6  100 synthetic users (500.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([100, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 3.49e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0020\n\u2022 Required noise multiplier: 0.7629\n\u2022 Sigma (for both methods): 1.5259\n\u2022 Total steps: 5000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 34.882] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.529\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.791\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.529 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.529\n\u2022 Median \u2016g_user\u2016_Mah = 4362.37\n\u2022 Total noise \u2113\u2082 \u2208 [194.5,254.8]\n\u2022 Last batch noise: Fisher only=247.2 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.860 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3467.54\n\u2022 Isotropic noise \u2113\u2082 \u2208 [419.1,432.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3330.90\n\u2022 Isotropic noise \u2113\u2082 \u2208 [420.2,430.8]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udcca  Accuracy summary (User-level (100 users) DP)\nbaseline         :  74.40% (cat 54.17%)\nFisher DP        :  49.94% (cat 70.64%)\nVanilla DP       :  51.12% (cat 30.49%)\nDP-SAT           :  55.46% (cat 34.09%)\nFisher vs Vanilla: -1.18% improvement\nFisher vs DP-SAT : -5.52% improvement\nDP-SAT vs Vanilla: +4.34% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6104 \u00b1 0.0000\nFisher DP: 0.5374 \u00b1 0.0000\nVanilla DP: 0.5280 \u00b1 0.0000\nDP-SAT: 0.5383 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5699 \u00b1 0.0000\nFisher DP: 0.4858 \u00b1 0.0000\nVanilla DP: 0.4918 \u00b1 0.0000\nDP-SAT: 0.4972 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5374\n\u2022 Vanilla DP: 0.5280\n\u2022 DP-SAT: 0.5383\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0093 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0102 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 56.02,
      "vanilla_dp": 38.12,
      "dp_sat": 33.4,
      "fisher_vs_vanilla": 17.9,
      "fisher_vs_dp_sat": 22.62,
      "dp_sat_vs_vanilla": -4.72,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.5473,
      "vanilla_dp_confidence_auc": 0.5242,
      "dp_sat_confidence_auc": 0.5143,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5056,
      "vanilla_dp_shadow_auc": 0.508,
      "dp_sat_shadow_auc": 0.5134,
      "fisher_dp_worst_auc": 0.5473,
      "vanilla_dp_worst_auc": 0.5242,
      "dp_sat_worst_auc": 0.5143,
      "experiment_name": "200_users_positive",
      "users": 200,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 200 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.367\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.845\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.367 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.367\n\u2022 Median \u2016g_user\u2016_Mah = 2033.50\n\u2022 Total noise \u2113\u2082 \u2208 [137.6,176.2]\n\u2022 Last batch noise: Fisher only=171.0 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=5.236 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2579.31\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.6,624.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2563.72\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.9,625.5]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  56.02% (cat 61.93%)\nVanilla DP       :  38.12% (cat 24.81%)\nDP-SAT           :  33.40% (cat 21.78%)\nFisher vs Vanilla: +17.90% improvement\nFisher vs DP-SAT : +22.62% improvement\nDP-SAT vs Vanilla: -4.72% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.5473 \u00b1 0.0000\nVanilla DP: 0.5242 \u00b1 0.0000\nDP-SAT: 0.5143 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5056 \u00b1 0.0000\nVanilla DP: 0.5080 \u00b1 0.0000\nDP-SAT: 0.5134 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5473\n\u2022 Vanilla DP: 0.5242\n\u2022 DP-SAT: 0.5143\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0329 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0098 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.6,
      "fisher_dp": 38.7,
      "vanilla_dp": 38.12,
      "dp_sat": 33.4,
      "fisher_vs_vanilla": 0.58,
      "fisher_vs_dp_sat": 5.3,
      "dp_sat_vs_vanilla": -4.72,
      "baseline_confidence_auc": 0.6415,
      "fisher_dp_confidence_auc": 0.5154,
      "vanilla_dp_confidence_auc": 0.5242,
      "dp_sat_confidence_auc": 0.5143,
      "baseline_shadow_auc": 0.6067,
      "fisher_dp_shadow_auc": 0.5164,
      "vanilla_dp_shadow_auc": 0.508,
      "dp_sat_shadow_auc": 0.5134,
      "fisher_dp_worst_auc": 0.5164,
      "vanilla_dp_worst_auc": 0.5242,
      "dp_sat_worst_auc": 0.5143,
      "experiment_name": "200_users_negative",
      "users": 200,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 200 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.04e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 10.356] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.413\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.829\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.413 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.413\n\u2022 Median \u2016g_user\u2016_Mah = 2325.86\n\u2022 Total noise \u2113\u2082 \u2208 [275.1,356.1]\n\u2022 Last batch noise: Fisher only=348.4 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=5.337 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2579.31\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.6,624.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2563.72\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.9,625.5]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  74.60% (cat 53.22%)\nFisher DP        :  38.70% (cat 29.36%)\nVanilla DP       :  38.12% (cat 24.81%)\nDP-SAT           :  33.40% (cat 21.78%)\nFisher vs Vanilla: +0.58% improvement\nFisher vs DP-SAT : +5.30% improvement\nDP-SAT vs Vanilla: -4.72% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6415 \u00b1 0.0000\nFisher DP: 0.5154 \u00b1 0.0000\nVanilla DP: 0.5242 \u00b1 0.0000\nDP-SAT: 0.5143 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6067 \u00b1 0.0000\nFisher DP: 0.5164 \u00b1 0.0000\nVanilla DP: 0.5080 \u00b1 0.0000\nDP-SAT: 0.5134 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5164\n\u2022 Vanilla DP: 0.5242\n\u2022 DP-SAT: 0.5143\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0020 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0098 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.3,
      "fisher_dp": 28.42,
      "vanilla_dp": 20.84,
      "dp_sat": 22.74,
      "fisher_vs_vanilla": 7.58,
      "fisher_vs_dp_sat": 5.68,
      "dp_sat_vs_vanilla": 1.9,
      "baseline_confidence_auc": 0.6759,
      "fisher_dp_confidence_auc": 0.5153,
      "vanilla_dp_confidence_auc": 0.5043,
      "dp_sat_confidence_auc": 0.5152,
      "baseline_shadow_auc": 0.6512,
      "fisher_dp_shadow_auc": 0.4981,
      "vanilla_dp_shadow_auc": 0.4982,
      "dp_sat_shadow_auc": 0.5194,
      "fisher_dp_worst_auc": 0.5153,
      "vanilla_dp_worst_auc": 0.5043,
      "dp_sat_worst_auc": 0.5194,
      "experiment_name": "400_users_positive",
      "users": 400,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 400 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (400 synthetic users)\n\u25b6  400 synthetic users (125.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([400, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.44e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0080\n\u2022 Required noise multiplier: 2.5488\n\u2022 Sigma (for both methods): 5.0977\n\u2022 Total steps: 20000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 2.438] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.411\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.830\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.411 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.411\n\u2022 Median \u2016g_user\u2016_Mah = 1614.89\n\u2022 Total noise \u2113\u2082 \u2208 [322.9,409.0]\n\u2022 Last batch noise: Fisher only=388.1 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=12.290 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1121.97\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1398.0,1438.6]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1203.18\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1396.2,1439.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udcca  Accuracy summary (User-level (400 users) DP)\nbaseline         :  74.30% (cat 57.01%)\nFisher DP        :  28.42% (cat 23.30%)\nVanilla DP       :  20.84% (cat 16.10%)\nDP-SAT           :  22.74% (cat 19.89%)\nFisher vs Vanilla: +7.58% improvement\nFisher vs DP-SAT : +5.68% improvement\nDP-SAT vs Vanilla: +1.90% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6759 \u00b1 0.0000\nFisher DP: 0.5153 \u00b1 0.0000\nVanilla DP: 0.5043 \u00b1 0.0000\nDP-SAT: 0.5152 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6512 \u00b1 0.0000\nFisher DP: 0.4981 \u00b1 0.0000\nVanilla DP: 0.4982 \u00b1 0.0000\nDP-SAT: 0.5194 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5153\n\u2022 Vanilla DP: 0.5043\n\u2022 DP-SAT: 0.5194\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0110 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0151 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 74.3,
      "fisher_dp": 19.92,
      "vanilla_dp": 20.84,
      "dp_sat": 22.74,
      "fisher_vs_vanilla": -0.92,
      "fisher_vs_dp_sat": -2.82,
      "dp_sat_vs_vanilla": 1.9,
      "baseline_confidence_auc": 0.6759,
      "fisher_dp_confidence_auc": 0.5134,
      "vanilla_dp_confidence_auc": 0.5043,
      "dp_sat_confidence_auc": 0.5152,
      "baseline_shadow_auc": 0.6512,
      "fisher_dp_shadow_auc": 0.5001,
      "vanilla_dp_shadow_auc": 0.4982,
      "dp_sat_shadow_auc": 0.5194,
      "fisher_dp_worst_auc": 0.5134,
      "vanilla_dp_worst_auc": 0.5043,
      "dp_sat_worst_auc": 0.5194,
      "experiment_name": "400_users_negative",
      "users": 400,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 400 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 41\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (400 synthetic users)\n\u25b6  400 synthetic users (125.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([400, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 2.44e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0080\n\u2022 Required noise multiplier: 2.5488\n\u2022 Sigma (for both methods): 5.0977\n\u2022 Total steps: 20000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 2.438] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.259\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.885\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.259 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.259\n\u2022 Median \u2016g_user\u2016_Mah = 1214.74\n\u2022 Total noise \u2113\u2082 \u2208 [645.3,765.7]\n\u2022 Last batch noise: Fisher only=725.9 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=11.514 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1121.97\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1398.0,1438.6]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1203.18\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1396.2,1439.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udcca  Accuracy summary (User-level (400 users) DP)\nbaseline         :  74.30% (cat 57.01%)\nFisher DP        :  19.92% (cat 12.12%)\nVanilla DP       :  20.84% (cat 16.10%)\nDP-SAT           :  22.74% (cat 19.89%)\nFisher vs Vanilla: -0.92% improvement\nFisher vs DP-SAT : -2.82% improvement\nDP-SAT vs Vanilla: +1.90% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 41\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6759 \u00b1 0.0000\nFisher DP: 0.5134 \u00b1 0.0000\nVanilla DP: 0.5043 \u00b1 0.0000\nDP-SAT: 0.5152 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6512 \u00b1 0.0000\nFisher DP: 0.5001 \u00b1 0.0000\nVanilla DP: 0.4982 \u00b1 0.0000\nDP-SAT: 0.5194 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5134\n\u2022 Vanilla DP: 0.5043\n\u2022 DP-SAT: 0.5194\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0091 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0151 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    }
  ],
  "config": {
    "experiment_name": "positively_correlated_noise_validation",
    "base_command": "uv run main.py",
    "common_args": {
      "k": 2048,
      "epochs": 50,
      "dataset-size": 50000,
      "target-epsilon": 2.0,
      "delta": 1e-05,
      "clip-radius": 2.0,
      "dp-layer": "conv1,conv2",
      "mps": true,
      "clean": true,
      "compare-others": true,
      "run-mia": true
    },
    "experiments": [
      {
        "name": "25_users_positive",
        "users": 25,
        "positively_correlated_noise": true,
        "description": "25 users with positively correlated noise"
      },
      {
        "name": "25_users_negative",
        "users": 25,
        "negatively_correlated_noise": true,
        "description": "25 users with negatively correlated noise (default)"
      },
      {
        "name": "25_users_l2_reg",
        "users": 25,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "25 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "50_users_positive",
        "users": 50,
        "positively_correlated_noise": true,
        "description": "50 users with positively correlated noise"
      },
      {
        "name": "50_users_negative",
        "users": 50,
        "negatively_correlated_noise": true,
        "description": "50 users with negatively correlated noise (default)"
      },
      {
        "name": "50_users_l2_reg",
        "users": 50,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "50 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "100_users_positive",
        "users": 100,
        "positively_correlated_noise": true,
        "description": "100 users with positively correlated noise"
      },
      {
        "name": "100_users_negative",
        "users": 100,
        "negatively_correlated_noise": true,
        "description": "100 users with negatively correlated noise (default)"
      },
      {
        "name": "100_users_l2_reg",
        "users": 100,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "100 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "200_users_positive",
        "users": 200,
        "positively_correlated_noise": true,
        "description": "200 users with positively correlated noise"
      },
      {
        "name": "200_users_negative",
        "users": 200,
        "negatively_correlated_noise": true,
        "description": "200 users with negatively correlated noise (default)"
      },
      {
        "name": "200_users_l2_reg",
        "users": 200,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "200 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "400_users_positive",
        "users": 400,
        "positively_correlated_noise": true,
        "description": "400 users with positively correlated noise"
      },
      {
        "name": "400_users_negative",
        "users": 400,
        "negatively_correlated_noise": true,
        "description": "400 users with negatively correlated noise (default)"
      },
      {
        "name": "400_users_l2_reg",
        "users": 400,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "400 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      }
    ],
    "output_settings": {
      "results_dir": "validation_results",
      "plots_dir": "validation_plots",
      "save_logs": true,
      "figure_dpi": 300
    }
  },
  "metadata": {
    "timestamp": "20250702_175112",
    "seed": 41,
    "total_experiments": 15,
    "successful_experiments": 10,
    "failed_experiments": 5
  }
}