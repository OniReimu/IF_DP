{
  "results": [
    {
      "baseline": 71.08,
      "fisher_dp": 69.54,
      "vanilla_dp": 65.88,
      "dp_sat": 63.8,
      "fisher_vs_vanilla": 3.66,
      "fisher_vs_dp_sat": 5.74,
      "dp_sat_vs_vanilla": -2.08,
      "baseline_confidence_auc": 0.5246,
      "fisher_dp_confidence_auc": 0.5234,
      "vanilla_dp_confidence_auc": 0.5175,
      "dp_sat_confidence_auc": 0.5158,
      "baseline_shadow_auc": 0.5023,
      "fisher_dp_shadow_auc": 0.5027,
      "vanilla_dp_shadow_auc": 0.5046,
      "dp_sat_shadow_auc": 0.5081,
      "fisher_dp_worst_auc": 0.5234,
      "vanilla_dp_worst_auc": 0.5175,
      "dp_sat_worst_auc": 0.5158,
      "experiment_name": "25_users_positive",
      "users": 25,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 25 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (25 synthetic users)\n\u25b6  25 synthetic users (2000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([25, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 9.03e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0005\n\u2022 Required noise multiplier: 0.6152\n\u2022 Sigma (for both methods): 1.2305\n\u2022 Total steps: 1250\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.712\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.737\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.712 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.712\n\u2022 Median \u2016g_user\u2016_Mah = 8199.59\n\u2022 Total noise \u2113\u2082 \u2208 [78.4,110.5]\n\u2022 Last batch noise: Fisher only=106.6 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.337 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5025.02\n\u2022 Isotropic noise \u2113\u2082 \u2208 [339.4,347.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 6424.29\n\u2022 Isotropic noise \u2113\u2082 \u2208 [338.0,345.4]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udcca  Accuracy summary (User-level (25 users) DP)\nbaseline         :  71.08% (cat 50.31%)\nFisher DP        :  69.54% (cat 52.57%)\nVanilla DP       :  65.88% (cat 41.48%)\nDP-SAT           :  63.80% (cat 41.48%)\nFisher vs Vanilla: +3.66% improvement\nFisher vs DP-SAT : +5.74% improvement\nDP-SAT vs Vanilla: -2.08% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5246 \u00b1 0.0000\nFisher DP: 0.5234 \u00b1 0.0000\nVanilla DP: 0.5175 \u00b1 0.0000\nDP-SAT: 0.5158 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5023 \u00b1 0.0000\nFisher DP: 0.5027 \u00b1 0.0000\nVanilla DP: 0.5046 \u00b1 0.0000\nDP-SAT: 0.5081 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5234\n\u2022 Vanilla DP: 0.5175\n\u2022 DP-SAT: 0.5158\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0076 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0017 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 71.08,
      "fisher_dp": 65.06,
      "vanilla_dp": 65.88,
      "dp_sat": 63.8,
      "fisher_vs_vanilla": -0.82,
      "fisher_vs_dp_sat": 1.26,
      "dp_sat_vs_vanilla": -2.08,
      "baseline_confidence_auc": 0.5246,
      "fisher_dp_confidence_auc": 0.5177,
      "vanilla_dp_confidence_auc": 0.5175,
      "dp_sat_confidence_auc": 0.5158,
      "baseline_shadow_auc": 0.5023,
      "fisher_dp_shadow_auc": 0.4989,
      "vanilla_dp_shadow_auc": 0.5046,
      "dp_sat_shadow_auc": 0.5081,
      "fisher_dp_worst_auc": 0.5177,
      "vanilla_dp_worst_auc": 0.5175,
      "dp_sat_worst_auc": 0.5158,
      "experiment_name": "25_users_negative",
      "users": 25,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 25 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (25 synthetic users)\n\u25b6  25 synthetic users (2000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([25, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 9.03e+02   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0005\n\u2022 Required noise multiplier: 0.6152\n\u2022 Sigma (for both methods): 1.2305\n\u2022 Total steps: 1250\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.693\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.743\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.693 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.693\n\u2022 Median \u2016g_user\u2016_Mah = 13730.40\n\u2022 Total noise \u2113\u2082 \u2208 [156.7,219.4]\n\u2022 Last batch noise: Fisher only=211.7 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.314 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 5025.02\n\u2022 Isotropic noise \u2113\u2082 \u2208 [339.4,347.0]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.2305\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.2305\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 6424.29\n\u2022 Isotropic noise \u2113\u2082 \u2208 [338.0,345.4]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9997\n\u2022 Noise multiplier: 0.6152\n\u2022 Total steps: 1250\n\u2022 Sample rate: 0.0005\n\n\ud83d\udcca  Accuracy summary (User-level (25 users) DP)\nbaseline         :  71.08% (cat 50.31%)\nFisher DP        :  65.06% (cat 48.67%)\nVanilla DP       :  65.88% (cat 41.48%)\nDP-SAT           :  63.80% (cat 41.48%)\nFisher vs Vanilla: -0.82% improvement\nFisher vs DP-SAT : +1.26% improvement\nDP-SAT vs Vanilla: -2.08% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 25 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5246 \u00b1 0.0000\nFisher DP: 0.5177 \u00b1 0.0000\nVanilla DP: 0.5175 \u00b1 0.0000\nDP-SAT: 0.5158 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5023 \u00b1 0.0000\nFisher DP: 0.4989 \u00b1 0.0000\nVanilla DP: 0.5046 \u00b1 0.0000\nDP-SAT: 0.5081 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5177\n\u2022 Vanilla DP: 0.5175\n\u2022 DP-SAT: 0.5158\n\ud83c\udfc6 DP-SAT provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0018 AUC reduction\n\ud83d\udcc8 vs Vanilla DP: 0.0017 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.98,
      "fisher_dp": 68.96,
      "vanilla_dp": 62.32,
      "dp_sat": 64.12,
      "fisher_vs_vanilla": 6.64,
      "fisher_vs_dp_sat": 4.84,
      "dp_sat_vs_vanilla": 1.8,
      "baseline_confidence_auc": 0.5541,
      "fisher_dp_confidence_auc": 0.5469,
      "vanilla_dp_confidence_auc": 0.5354,
      "dp_sat_confidence_auc": 0.5378,
      "baseline_shadow_auc": 0.519,
      "fisher_dp_shadow_auc": 0.5114,
      "vanilla_dp_shadow_auc": 0.4999,
      "dp_sat_shadow_auc": 0.5167,
      "fisher_dp_worst_auc": 0.5469,
      "vanilla_dp_worst_auc": 0.5354,
      "dp_sat_worst_auc": 0.5378,
      "experiment_name": "50_users_positive",
      "users": 50,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 50 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (50 synthetic users)\n\u25b6  50 synthetic users (1000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([50, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.73e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0010\n\u2022 Required noise multiplier: 0.6677\n\u2022 Sigma (for both methods): 1.3354\n\u2022 Total steps: 2500\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 17.330] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.307\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.867\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.307 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.307\n\u2022 Median \u2016g_user\u2016_Mah = 4423.99\n\u2022 Total noise \u2113\u2082 \u2208 [83.2,110.1]\n\u2022 Last batch noise: Fisher only=97.0 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.081 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4397.30\n\u2022 Isotropic noise \u2113\u2082 \u2208 [368.6,375.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4595.26\n\u2022 Isotropic noise \u2113\u2082 \u2208 [368.0,376.3]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udcca  Accuracy summary (User-level (50 users) DP)\nbaseline         :  73.98% (cat 62.42%)\nFisher DP        :  68.96% (cat 36.96%)\nVanilla DP       :  62.32% (cat 50.31%)\nDP-SAT           :  64.12% (cat 49.49%)\nFisher vs Vanilla: +6.64% improvement\nFisher vs DP-SAT : +4.84% improvement\nDP-SAT vs Vanilla: +1.80% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5541 \u00b1 0.0000\nFisher DP: 0.5469 \u00b1 0.0000\nVanilla DP: 0.5354 \u00b1 0.0000\nDP-SAT: 0.5378 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5190 \u00b1 0.0000\nFisher DP: 0.5114 \u00b1 0.0000\nVanilla DP: 0.4999 \u00b1 0.0000\nDP-SAT: 0.5167 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5469\n\u2022 Vanilla DP: 0.5354\n\u2022 DP-SAT: 0.5378\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0114 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0024 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 73.98,
      "fisher_dp": 60.9,
      "vanilla_dp": 62.32,
      "dp_sat": 64.12,
      "fisher_vs_vanilla": -1.42,
      "fisher_vs_dp_sat": -3.22,
      "dp_sat_vs_vanilla": 1.8,
      "baseline_confidence_auc": 0.5541,
      "fisher_dp_confidence_auc": 0.5364,
      "vanilla_dp_confidence_auc": 0.5354,
      "dp_sat_confidence_auc": 0.5378,
      "baseline_shadow_auc": 0.519,
      "fisher_dp_shadow_auc": 0.5109,
      "vanilla_dp_shadow_auc": 0.4999,
      "dp_sat_shadow_auc": 0.5167,
      "fisher_dp_worst_auc": 0.5364,
      "vanilla_dp_worst_auc": 0.5354,
      "dp_sat_worst_auc": 0.5378,
      "experiment_name": "50_users_negative",
      "users": 50,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 50 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (50 synthetic users)\n\u25b6  50 synthetic users (1000.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([50, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.73e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0010\n\u2022 Required noise multiplier: 0.6677\n\u2022 Sigma (for both methods): 1.3354\n\u2022 Total steps: 2500\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 17.330] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.339\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.855\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.339 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.339\n\u2022 Median \u2016g_user\u2016_Mah = 5070.44\n\u2022 Total noise \u2113\u2082 \u2208 [166.2,208.0]\n\u2022 Last batch noise: Fisher only=196.5 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.124 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4397.30\n\u2022 Isotropic noise \u2113\u2082 \u2208 [368.6,375.8]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.3354\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.3354\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 4595.26\n\u2022 Isotropic noise \u2113\u2082 \u2208 [368.0,376.3]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9992\n\u2022 Noise multiplier: 0.6677\n\u2022 Total steps: 2500\n\u2022 Sample rate: 0.0010\n\n\ud83d\udcca  Accuracy summary (User-level (50 users) DP)\nbaseline         :  73.98% (cat 62.42%)\nFisher DP        :  60.90% (cat 25.26%)\nVanilla DP       :  62.32% (cat 50.31%)\nDP-SAT           :  64.12% (cat 49.49%)\nFisher vs Vanilla: -1.42% improvement\nFisher vs DP-SAT : -3.22% improvement\nDP-SAT vs Vanilla: +1.80% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 50 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.5541 \u00b1 0.0000\nFisher DP: 0.5364 \u00b1 0.0000\nVanilla DP: 0.5354 \u00b1 0.0000\nDP-SAT: 0.5378 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5190 \u00b1 0.0000\nFisher DP: 0.5109 \u00b1 0.0000\nVanilla DP: 0.4999 \u00b1 0.0000\nDP-SAT: 0.5167 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5364\n\u2022 Vanilla DP: 0.5354\n\u2022 DP-SAT: 0.5378\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0010 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0024 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.3,
      "fisher_dp": 73.14,
      "vanilla_dp": 53.08,
      "dp_sat": 53.36,
      "fisher_vs_vanilla": 20.06,
      "fisher_vs_dp_sat": 19.78,
      "dp_sat_vs_vanilla": 0.28,
      "baseline_confidence_auc": 0.6041,
      "fisher_dp_confidence_auc": 0.5884,
      "vanilla_dp_confidence_auc": 0.5248,
      "dp_sat_confidence_auc": 0.5458,
      "baseline_shadow_auc": 0.5715,
      "fisher_dp_shadow_auc": 0.5471,
      "vanilla_dp_shadow_auc": 0.4875,
      "dp_sat_shadow_auc": 0.5119,
      "fisher_dp_worst_auc": 0.5884,
      "vanilla_dp_worst_auc": 0.5248,
      "dp_sat_worst_auc": 0.5458,
      "experiment_name": "100_users_positive",
      "users": 100,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 100 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (100 synthetic users)\n\u25b6  100 synthetic users (500.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([100, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.61e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0020\n\u2022 Required noise multiplier: 0.7629\n\u2022 Sigma (for both methods): 1.5259\n\u2022 Total steps: 5000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.427\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.824\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.427 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.427\n\u2022 Median \u2016g_user\u2016_Mah = 2468.90\n\u2022 Total noise \u2113\u2082 \u2208 [96.8,122.2]\n\u2022 Last batch noise: Fisher only=118.7 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.703 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3597.25\n\u2022 Isotropic noise \u2113\u2082 \u2208 [420.9,429.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3610.56\n\u2022 Isotropic noise \u2113\u2082 \u2208 [418.6,431.7]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udcca  Accuracy summary (User-level (100 users) DP)\nbaseline         :  75.30% (cat 58.73%)\nFisher DP        :  73.14% (cat 56.88%)\nVanilla DP       :  53.08% (cat 43.94%)\nDP-SAT           :  53.36% (cat 36.96%)\nFisher vs Vanilla: +20.06% improvement\nFisher vs DP-SAT : +19.78% improvement\nDP-SAT vs Vanilla: +0.28% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6041 \u00b1 0.0000\nFisher DP: 0.5884 \u00b1 0.0000\nVanilla DP: 0.5248 \u00b1 0.0000\nDP-SAT: 0.5458 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5715 \u00b1 0.0000\nFisher DP: 0.5471 \u00b1 0.0000\nVanilla DP: 0.4875 \u00b1 0.0000\nDP-SAT: 0.5119 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5884\n\u2022 Vanilla DP: 0.5248\n\u2022 DP-SAT: 0.5458\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0637 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0210 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.3,
      "fisher_dp": 61.24,
      "vanilla_dp": 53.08,
      "dp_sat": 53.36,
      "fisher_vs_vanilla": 8.16,
      "fisher_vs_dp_sat": 7.88,
      "dp_sat_vs_vanilla": 0.28,
      "baseline_confidence_auc": 0.6041,
      "fisher_dp_confidence_auc": 0.5501,
      "vanilla_dp_confidence_auc": 0.5248,
      "dp_sat_confidence_auc": 0.5458,
      "baseline_shadow_auc": 0.5715,
      "fisher_dp_shadow_auc": 0.5197,
      "vanilla_dp_shadow_auc": 0.4875,
      "dp_sat_shadow_auc": 0.5119,
      "fisher_dp_worst_auc": 0.5501,
      "vanilla_dp_worst_auc": 0.5248,
      "dp_sat_worst_auc": 0.5458,
      "experiment_name": "100_users_negative",
      "users": 100,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 100 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (100 synthetic users)\n\u25b6  100 synthetic users (500.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([100, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.61e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0020\n\u2022 Required noise multiplier: 0.7629\n\u2022 Sigma (for both methods): 1.5259\n\u2022 Total steps: 5000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\neigh failed (Internal Error.) \u2013 using diagonal fallback\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.484\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.805\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.484 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.484\n\u2022 Median \u2016g_user\u2016_Mah = 4578.36\n\u2022 Total noise \u2113\u2082 \u2208 [193.5,250.1]\n\u2022 Last batch noise: Fisher only=243.0 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=3.790 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3597.25\n\u2022 Isotropic noise \u2113\u2082 \u2208 [420.9,429.7]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 1.5259\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=1.5259\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 3610.56\n\u2022 Isotropic noise \u2113\u2082 \u2208 [418.6,431.7]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9991\n\u2022 Noise multiplier: 0.7629\n\u2022 Total steps: 5000\n\u2022 Sample rate: 0.0020\n\n\ud83d\udcca  Accuracy summary (User-level (100 users) DP)\nbaseline         :  75.30% (cat 58.73%)\nFisher DP        :  61.24% (cat 47.02%)\nVanilla DP       :  53.08% (cat 43.94%)\nDP-SAT           :  53.36% (cat 36.96%)\nFisher vs Vanilla: +8.16% improvement\nFisher vs DP-SAT : +7.88% improvement\nDP-SAT vs Vanilla: +0.28% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 100 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6041 \u00b1 0.0000\nFisher DP: 0.5501 \u00b1 0.0000\nVanilla DP: 0.5248 \u00b1 0.0000\nDP-SAT: 0.5458 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.5715 \u00b1 0.0000\nFisher DP: 0.5197 \u00b1 0.0000\nVanilla DP: 0.4875 \u00b1 0.0000\nDP-SAT: 0.5119 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5501\n\u2022 Vanilla DP: 0.5248\n\u2022 DP-SAT: 0.5458\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0253 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0210 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.32,
      "fisher_dp": 62.7,
      "vanilla_dp": 34.7,
      "dp_sat": 37.06,
      "fisher_vs_vanilla": 28.0,
      "fisher_vs_dp_sat": 25.64,
      "dp_sat_vs_vanilla": 2.36,
      "baseline_confidence_auc": 0.6315,
      "fisher_dp_confidence_auc": 0.5341,
      "vanilla_dp_confidence_auc": 0.5027,
      "dp_sat_confidence_auc": 0.5275,
      "baseline_shadow_auc": 0.6156,
      "fisher_dp_shadow_auc": 0.4994,
      "vanilla_dp_shadow_auc": 0.5039,
      "dp_sat_shadow_auc": 0.5068,
      "fisher_dp_worst_auc": 0.5341,
      "vanilla_dp_worst_auc": 0.5039,
      "dp_sat_worst_auc": 0.5275,
      "experiment_name": "200_users_positive",
      "users": 200,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 200 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.27e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 12.732] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.118\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.944\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.118 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.118\n\u2022 Median \u2016g_user\u2016_Mah = 2194.78\n\u2022 Total noise \u2113\u2082 \u2208 [142.3,158.7]\n\u2022 Last batch noise: Fisher only=147.3 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=4.685 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2448.59\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.9,627.9]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2440.18\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.3,624.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.32% (cat 67.56%)\nFisher DP        :  62.70% (cat 49.69%)\nVanilla DP       :  34.70% (cat 20.33%)\nDP-SAT           :  37.06% (cat 21.97%)\nFisher vs Vanilla: +28.00% improvement\nFisher vs DP-SAT : +25.64% improvement\nDP-SAT vs Vanilla: +2.36% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6315 \u00b1 0.0000\nFisher DP: 0.5341 \u00b1 0.0000\nVanilla DP: 0.5027 \u00b1 0.0000\nDP-SAT: 0.5275 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6156 \u00b1 0.0000\nFisher DP: 0.4994 \u00b1 0.0000\nVanilla DP: 0.5039 \u00b1 0.0000\nDP-SAT: 0.5068 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5341\n\u2022 Vanilla DP: 0.5039\n\u2022 DP-SAT: 0.5275\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0302 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0236 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 75.32,
      "fisher_dp": 37.56,
      "vanilla_dp": 34.7,
      "dp_sat": 37.06,
      "fisher_vs_vanilla": 2.86,
      "fisher_vs_dp_sat": 0.5,
      "dp_sat_vs_vanilla": 2.36,
      "baseline_confidence_auc": 0.6315,
      "fisher_dp_confidence_auc": 0.4825,
      "vanilla_dp_confidence_auc": 0.5027,
      "dp_sat_confidence_auc": 0.5275,
      "baseline_shadow_auc": 0.6156,
      "fisher_dp_shadow_auc": 0.5133,
      "vanilla_dp_shadow_auc": 0.5039,
      "dp_sat_shadow_auc": 0.5068,
      "fisher_dp_worst_auc": 0.5133,
      "vanilla_dp_worst_auc": 0.5039,
      "dp_sat_worst_auc": 0.5275,
      "experiment_name": "200_users_negative",
      "users": 200,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 200 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (200 synthetic users)\n\u25b6  200 synthetic users (250.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([200, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 1.27e+03   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0040\n\u2022 Required noise multiplier: 1.1060\n\u2022 Sigma (for both methods): 2.2119\n\u2022 Total steps: 10000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 12.732] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.243\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.892\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.243 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.243\n\u2022 Median \u2016g_user\u2016_Mah = 2578.95\n\u2022 Total noise \u2113\u2082 \u2208 [280.5,330.2]\n\u2022 Last batch noise: Fisher only=311.4 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=4.962 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2448.59\n\u2022 Isotropic noise \u2113\u2082 \u2208 [608.9,627.9]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 2.2119\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=2.2119\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 2440.18\n\u2022 Isotropic noise \u2113\u2082 \u2208 [607.3,624.9]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9946\n\u2022 Noise multiplier: 1.1060\n\u2022 Total steps: 10000\n\u2022 Sample rate: 0.0040\n\n\ud83d\udcca  Accuracy summary (User-level (200 users) DP)\nbaseline         :  75.32% (cat 67.56%)\nFisher DP        :  37.56% (cat 40.86%)\nVanilla DP       :  34.70% (cat 20.33%)\nDP-SAT           :  37.06% (cat 21.97%)\nFisher vs Vanilla: +2.86% improvement\nFisher vs DP-SAT : +0.50% improvement\nDP-SAT vs Vanilla: +2.36% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 200 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6315 \u00b1 0.0000\nFisher DP: 0.4825 \u00b1 0.0000\nVanilla DP: 0.5027 \u00b1 0.0000\nDP-SAT: 0.5275 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.6156 \u00b1 0.0000\nFisher DP: 0.5133 \u00b1 0.0000\nVanilla DP: 0.5039 \u00b1 0.0000\nDP-SAT: 0.5068 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5133\n\u2022 Vanilla DP: 0.5039\n\u2022 DP-SAT: 0.5275\n\ud83c\udfc6 Vanilla DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Fisher DP: 0.0093 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0236 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 76.58,
      "fisher_dp": 32.78,
      "vanilla_dp": 15.42,
      "dp_sat": 20.72,
      "fisher_vs_vanilla": 17.36,
      "fisher_vs_dp_sat": 12.06,
      "dp_sat_vs_vanilla": 5.3,
      "baseline_confidence_auc": 0.6679,
      "fisher_dp_confidence_auc": 0.4894,
      "vanilla_dp_confidence_auc": 0.4922,
      "dp_sat_confidence_auc": 0.4751,
      "baseline_shadow_auc": 0.7012,
      "fisher_dp_shadow_auc": 0.5044,
      "vanilla_dp_shadow_auc": 0.5286,
      "dp_sat_shadow_auc": 0.5289,
      "fisher_dp_worst_auc": 0.5044,
      "vanilla_dp_worst_auc": 0.5286,
      "dp_sat_worst_auc": 0.5289,
      "experiment_name": "400_users_positive",
      "users": 400,
      "noise_strategy": "positive",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 400 --positively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (400 synthetic users)\n\u25b6  400 synthetic users (125.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([400, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 3.61e+01   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0080\n\u2022 Required noise multiplier: 2.5488\n\u2022 Sigma (for both methods): 5.0977\n\u2022 Total steps: 20000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Positively correlated noise (noise \u221d \u221a\u03bb)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.168\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.923\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.168 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.168\n\u2022 Median \u2016g_user\u2016_Mah = 1544.76\n\u2022 Total noise \u2113\u2082 \u2208 [325.7,368.7]\n\u2022 Last batch noise: Fisher only=354.8 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=11.050 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1147.51\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1396.5,1439.4]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1096.65\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1396.1,1438.5]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udcca  Accuracy summary (User-level (400 users) DP)\nbaseline         :  76.58% (cat 60.16%)\nFisher DP        :  32.78% (cat 23.41%)\nVanilla DP       :  15.42% (cat 32.85%)\nDP-SAT           :  20.72% (cat  9.03%)\nFisher vs Vanilla: +17.36% improvement\nFisher vs DP-SAT : +12.06% improvement\nDP-SAT vs Vanilla: +5.30% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6679 \u00b1 0.0000\nFisher DP: 0.4894 \u00b1 0.0000\nVanilla DP: 0.4922 \u00b1 0.0000\nDP-SAT: 0.4751 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.7012 \u00b1 0.0000\nFisher DP: 0.5044 \u00b1 0.0000\nVanilla DP: 0.5286 \u00b1 0.0000\nDP-SAT: 0.5289 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5044\n\u2022 Vanilla DP: 0.5286\n\u2022 DP-SAT: 0.5289\n\ud83c\udfc6 Fisher DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Vanilla DP: 0.0241 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0245 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    },
    {
      "baseline": 76.58,
      "fisher_dp": 17.1,
      "vanilla_dp": 15.42,
      "dp_sat": 20.72,
      "fisher_vs_vanilla": 1.68,
      "fisher_vs_dp_sat": -3.62,
      "dp_sat_vs_vanilla": 5.3,
      "baseline_confidence_auc": 0.6679,
      "fisher_dp_confidence_auc": 0.483,
      "vanilla_dp_confidence_auc": 0.4922,
      "dp_sat_confidence_auc": 0.4751,
      "baseline_shadow_auc": 0.7012,
      "fisher_dp_shadow_auc": 0.5063,
      "vanilla_dp_shadow_auc": 0.5286,
      "dp_sat_shadow_auc": 0.5289,
      "fisher_dp_worst_auc": 0.5063,
      "vanilla_dp_worst_auc": 0.5286,
      "dp_sat_worst_auc": 0.5289,
      "experiment_name": "400_users_negative",
      "users": 400,
      "noise_strategy": "negative",
      "command": "uv run main.py --k 2048 --epochs 50 --dataset-size 50000 --target-epsilon 2.0 --delta 1e-05 --clip-radius 2.0 --dp-layer conv1,conv2 --mps --clean --compare-others --run-mia --users 400 --negatively_correlated_noise",
      "stdout": "\ud83c\udfb2 Random seeds set to 38\nUsing MPS\nCleaning saved models\u2026\nremoved ./saved_models/DP\u6a21\u578b.pth\nremoved ./saved_models/DP_SAT\u6a21\u578b.pth\nremoved ./saved_models/Vanilla_DP\u6a21\u578b.pth\nremoved ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\nFiles already downloaded and verified\nFiles already downloaded and verified\n\ud83d\udcca Data split:\n\u2022 Private data: 50000 samples from trainset (for training)\n\u2022 Public data: 5000 samples from testset (for calibration)\n\u2022 Evaluation data: 5000 samples from testset (for evaluation & MIA non-members)\n\ud83d\udc65 Using USER-level DP-SGD (400 synthetic users)\n\u25b6  400 synthetic users (125.0 samples each)\n\n\u2699\ufe0f  Training baseline\u2026\n\n\ud83d\udd0d  Fisher matrix\u2026\n\ud83c\udfaf computing Fisher for layers ['conv1', 'conv2']\n\u76ee\u6807\u53c2\u6570: ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias']\n\u53c2\u6570\u603b\u6570: 19392\n\u963b\u5c3c\u7cfb\u6570 \u03c1 = 0.01\nFisher\u77e9\u9635\u5927\u5c0f: 19392\u00d719392 \u2248 1.50 GB\n\u68af\u5ea6\u77e9\u9635\u5f62\u72b6: torch.Size([400, 19392])\nFisher\u77e9\u9635\u5f62\u72b6: torch.Size([19392, 19392])\n\u6761\u4ef6\u6570\u2248 3.61e+01   \u4f30\u8ba1rank 19392/19392\n\n\ud83d\ude80 Fisher-informed DP-SGD\u2026\n\n\ud83d\udd12 Using Proper Privacy Accounting (Opacus RDP)\n\n\ud83c\udfaf Proper Privacy Accounting Setup:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Sample rate: 0.0080\n\u2022 Required noise multiplier: 2.5488\n\u2022 Sigma (for both methods): 5.0977\n\u2022 Total steps: 20000\n\u8ba1\u7b97 top-2048 eigenpairs (requested 2048), \u03bb_floor = 0.5\n\u7279\u5f81\u503c\u8303\u56f4: [0.500, 0.500] (got 2048 eigenpairs)\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Fisher DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Fisher subspace: k=2048, complement dim=17344\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (will convert to Mahalanobis)\n\u2022 Adaptive clipping: False\n\u2022 Full complement noise: False\n\u2022 Noise scaling strategy: Negatively correlated noise (noise \u221d 1/\u221a\u03bb, default)\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\ud83c\udfaf User-level norm calibration:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000\n\u2022 Calibrated Mahalanobis threshold: 2.222\n\u2022 Sample ratio: ||g||\u2082/||g||_{F\u207b\u00b9} \u2248 0.900\n\ud83d\udd27 NOISE SCALING FIX: Using actual_radius=2.222 for noise (was euclidean_target=2.000)\n\n\n\ud83d\udcca  Fisher DP-SGD final stats:\n\u2022 Target Euclidean sensitivity: \u0394\u2082 = 2.000 (same as vanilla DP-SGD)\n\u2022 Calibrated Mahalanobis threshold: 2.222\n\u2022 Median \u2016g_user\u2016_Mah = 1169.93\n\u2022 Total noise \u2113\u2082 \u2208 [651.4,755.9]\n\u2022 Last batch noise: Fisher only=727.3 (complement disabled)\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 FAIR COMPARISON: Same noise scale \u03c3\u00d7\u0394=11.326 as vanilla DP-SGD\n\u2022 \ud83d\udd27 NOISE SCALING FIXED: Using actual_radius for noise (not euclidean_target)\n\n\ud83d\udcd0 Vanilla DP-SGD (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf Vanilla DP-SGD config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  Vanilla DP-SGD final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1147.51\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1396.5,1439.4]\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\n\ud83d\udd12 Fisher DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udd12 Vanilla DP-SGD Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\u2705 Fair privacy comparison: Both methods at same privacy level\n\n\ud83d\udd3a DP-SAT: Sharpness-Aware Training (comparison)\u2026\n\u2022 Using provided sigma: 5.0977\n\n\ud83c\udfaf DP-SAT config: User-level DP  layers=conv1,conv2  \u03b5=2.0\n\u2022 Proper privacy accounting: \u03c3=5.0977\n\u2022 Euclidean clipping with radius 2.0\n\u2022 Isotropic Gaussian noise + Sharpness-aware flatness adjustment\n\u2022 Flatness coefficient \u03bb: 0.0100\n\u2022 Adaptive clipping: False\n\u2022 User-level mode: Clipping aggregated user gradients\n\n\n\ud83d\udcca  DP-SAT final stats:\n\u2022 Median \u2016g_user\u2016\u2082 = 1096.65\n\u2022 Isotropic noise \u2113\u2082 \u2208 [1396.1,1438.5]\n\u2022 Flatness adjustment \u2113\u2082 \u2208 [0.000,0.010]\n\u2022 Flatness coefficient \u03bb = 0.0100\n\u2022 Privacy: (\u03b5=2.0, \u03b4=1e-05) over 50 epochs\n\u2022 \u2705 CORRECTED: Uses previous step gradient for flatness adjustment\n\n\ud83d\udd12 DP-SAT Privacy Summary:\n\u2022 Target (\u03b5, \u03b4): (2.0, 1e-05)\n\u2022 Actual \u03b5: 1.9947\n\u2022 Noise multiplier: 2.5488\n\u2022 Total steps: 20000\n\u2022 Sample rate: 0.0080\n\n\ud83d\udcca  Accuracy summary (User-level (400 users) DP)\nbaseline         :  76.58% (cat 60.16%)\nFisher DP        :  17.10% (cat 13.35%)\nVanilla DP       :  15.42% (cat 32.85%)\nDP-SAT           :  20.72% (cat  9.03%)\nFisher vs Vanilla: +1.68% improvement\nFisher vs DP-SAT : -3.62% improvement\nDP-SAT vs Vanilla: +5.30% improvement\n\n\ud83d\udcbe Saving models for MIA evaluation...\n\u2705 Saved baseline model to ./saved_models/\u57fa\u7ebf\u6a21\u578b.pth\n\u2705 Saved Fisher DP model to ./saved_models/DP\u6a21\u578b.pth\n\u2705 Saved Vanilla DP model to ./saved_models/Vanilla_DP\u6a21\u578b.pth\n\u2705 Saved DP-SAT model to ./saved_models/DP_SAT\u6a21\u578b.pth\n\n\ud83d\udee1\ufe0f  To evaluate privacy protection, add --run-mia to your command:\nRe-run with: --run-mia --mia-size 1000 (will compare Fisher vs Vanilla & DP-SAT)\n\ud83c\udfb2 Random seeds set to 38\n\n\ud83d\udee1\ufe0f  MEMBERSHIP INFERENCE ATTACK EVALUATION\nComparing: Baseline vs Fisher DP vs Vanilla DP vs DP-SAT\n============================================================\n\ud83d\udc65 User-level MIA: Using actual private users as members\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\u2022 Members: 1000 samples\n\u2022 Non-members: 1000 samples\n\u2022 Members: 1000 random samples from 400 training users\n\u2022 Non-members: 1000 random samples from evaluation data\n\n1\ufe0f\u20e3  CONFIDENCE ATTACK\n------------------------------\n\n2\ufe0f\u20e3  SHADOW MODEL ATTACK\n------------------------------\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\ud83d\udd27 Shadow attack: Using eval_data for non-members (correct)\n\n\ud83d\udcca FINAL RESULTS\n========================================\n\ud83c\udfaf Confidence Attack AUC:\nBaseline: 0.6679 \u00b1 0.0000\nFisher DP: 0.4830 \u00b1 0.0000\nVanilla DP: 0.4922 \u00b1 0.0000\nDP-SAT: 0.4751 \u00b1 0.0000\n\n\ud83d\udd76\ufe0f  Shadow Attack AUC:\nBaseline: 0.7012 \u00b1 0.0000\nFisher DP: 0.5063 \u00b1 0.0000\nVanilla DP: 0.5286 \u00b1 0.0000\nDP-SAT: 0.5289 \u00b1 0.0000\n\n\ud83e\uddee Statistical Significance Tests (p-values):\nFisher DP vs Baseline (Confidence): p = nan\nFisher DP vs Baseline (Shadow): p = nan\nVanilla DP vs Baseline (Confidence): p = nan\nVanilla DP vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs Vanilla DP (Shadow): p = nan\nDP-SAT vs Baseline (Confidence): p = nan\nDP-SAT vs Baseline (Shadow): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Confidence): p = nan\n\ud83d\udd25 Fisher DP vs DP-SAT (Shadow): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Confidence): p = nan\n\ud83c\udd9a Vanilla DP vs DP-SAT (Shadow): p = nan\n\n\ud83c\udfaf FINAL PRIVACY PROTECTION COMPARISON\n==================================================\n\ud83d\udcca Worst-case AUC:\n\u2022 Fisher DP: 0.5063\n\u2022 Vanilla DP: 0.5286\n\u2022 DP-SAT: 0.5289\n\ud83c\udfc6 Fisher DP provides the BEST privacy protection!\n\ud83d\udcc8 vs Vanilla DP: 0.0223 AUC reduction\n\ud83d\udcc8 vs DP-SAT: 0.0226 AUC reduction\n\u2705 Fisher DP provides STRONG privacy protection!\n\u2705 Vanilla DP provides STRONG privacy protection!\n\u2705 DP-SAT provides STRONG privacy protection!",
      "success": true
    }
  ],
  "config": {
    "experiment_name": "positively_correlated_noise_validation",
    "base_command": "uv run main.py",
    "common_args": {
      "k": 2048,
      "epochs": 50,
      "dataset-size": 50000,
      "target-epsilon": 2.0,
      "delta": 1e-05,
      "clip-radius": 2.0,
      "dp-layer": "conv1,conv2",
      "mps": true,
      "clean": true,
      "compare-others": true,
      "run-mia": true
    },
    "experiments": [
      {
        "name": "25_users_positive",
        "users": 25,
        "positively_correlated_noise": true,
        "description": "25 users with positively correlated noise"
      },
      {
        "name": "25_users_negative",
        "users": 25,
        "negatively_correlated_noise": true,
        "description": "25 users with negatively correlated noise (default)"
      },
      {
        "name": "25_users_l2_reg",
        "users": 25,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "25 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "50_users_positive",
        "users": 50,
        "positively_correlated_noise": true,
        "description": "50 users with positively correlated noise"
      },
      {
        "name": "50_users_negative",
        "users": 50,
        "negatively_correlated_noise": true,
        "description": "50 users with negatively correlated noise (default)"
      },
      {
        "name": "50_users_l2_reg",
        "users": 50,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "50 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "100_users_positive",
        "users": 100,
        "positively_correlated_noise": true,
        "description": "100 users with positively correlated noise"
      },
      {
        "name": "100_users_negative",
        "users": 100,
        "negatively_correlated_noise": true,
        "description": "100 users with negatively correlated noise (default)"
      },
      {
        "name": "100_users_l2_reg",
        "users": 100,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "100 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "200_users_positive",
        "users": 200,
        "positively_correlated_noise": true,
        "description": "200 users with positively correlated noise"
      },
      {
        "name": "200_users_negative",
        "users": 200,
        "negatively_correlated_noise": true,
        "description": "200 users with negatively correlated noise (default)"
      },
      {
        "name": "200_users_l2_reg",
        "users": 200,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "200 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      },
      {
        "name": "400_users_positive",
        "users": 400,
        "positively_correlated_noise": true,
        "description": "400 users with positively correlated noise"
      },
      {
        "name": "400_users_negative",
        "users": 400,
        "negatively_correlated_noise": true,
        "description": "400 users with negatively correlated noise (default)"
      },
      {
        "name": "400_users_l2_reg",
        "users": 400,
        "negatively_correlated_noise": true,
        "l2_regularization": 0.01,
        "description": "400 users with L2 regularization (\u03bb=0.01) for hypothesis testing"
      }
    ],
    "output_settings": {
      "results_dir": "validation_results",
      "plots_dir": "validation_plots",
      "save_logs": true,
      "figure_dpi": 300
    }
  },
  "metadata": {
    "timestamp": "20250702_233217",
    "seed": 38,
    "total_experiments": 15,
    "successful_experiments": 10,
    "failed_experiments": 5
  }
}