#!/usr/bin/env python3
"""
Visual Plotter: Plotting from Validation Results
=================================================

This script reads JSON results from visual_discovery_analysis.py and creates
comprehensive visualizations. It's decoupled from the experiment running logic
to allow for flexible replotting and analysis.

Key Features:
1. Loads results from JSON files generated by visual_discovery_analysis.py
2. Reconstructs plot data structure from experiment results
3. Creates comprehensive validation plots
4. Supports loading latest results automatically
5. Independent of expensive experiment re-runs
"""

import json
import os
import glob
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pandas as pd

# Set style for publication-quality plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

def find_latest_results_file(results_dir="validation_results"):
    """Find the most recent results JSON file"""
    if not os.path.exists(results_dir):
        raise FileNotFoundError(f"Results directory {results_dir} not found")
    
    pattern = os.path.join(results_dir, "validation_results_*.json")
    files = glob.glob(pattern)
    
    if not files:
        raise FileNotFoundError(f"No validation results files found in {results_dir}")
    
    # Sort by modification time, most recent first
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_results_from_json(json_file_path):
    """Load experimental results and config from JSON file"""
    print(f"ğŸ“„ Loading results from: {json_file_path}")
    
    with open(json_file_path, 'r') as f:
        data = json.load(f)
    
    # Handle both old format (list of results) and new format (dict with results + config)
    if isinstance(data, list):
        # Old format: just results list
        results = data
        config = None
        print("âš ï¸  Old format detected - config information not available")
    elif isinstance(data, dict) and 'results' in data:
        # New format: dict with results and config
        results = data['results']
        config = data.get('config', None)
        print(f"âœ… New format detected - loaded {len(results)} results with config")
    else:
        raise ValueError("Unrecognized JSON format")
    
    print(f"ğŸ“Š Loaded {len(results)} experimental results")
    
    # Print summary of results
    if results:
        user_counts = sorted(set(r.get('users', 0) for r in results))
        noise_strategies = set(r.get('noise_strategy', 'unknown') for r in results)
        print(f"   â€¢ User counts: {user_counts}")
        print(f"   â€¢ Noise strategies: {sorted(noise_strategies)}")
    
    return results, config

def analyze_results_from_json(results, config=None):
    """Convert JSON results to plot data structure including MIA data"""
    if not results:
        print("âŒ No results to analyze")
        return {}
    
    print(f"ğŸ“Š Analyzing {len(results)} experimental results...")
    
    # Convert to DataFrame for easier analysis
    df = pd.DataFrame(results)
    
    # Group by user count and experiment type
    user_counts = sorted(df['users'].unique())
    
    # Prepare data for plotting
    plot_data = {}
    for user_count in user_counts:
        user_data = df[df['users'] == user_count]
        
        positive_data = user_data[user_data['noise_strategy'] == 'positive']
        negative_data = user_data[user_data['noise_strategy'] == 'negative']
        
        plot_data[user_count] = {
            'baseline': 0,
            'positive_fisher': 0,
            'negative_fisher': 0,
            'positive_vanilla': 0,
            'negative_vanilla': 0,
            'positive_dp_sat': 0,
            'negative_dp_sat': 0,
            'strategy_difference': 0,
            # MIA data
            'baseline_confidence_auc': 0,
            'fisher_dp_confidence_auc_positive': 0,
            'fisher_dp_confidence_auc_negative': 0,
            'vanilla_dp_confidence_auc': 0,
            'dp_sat_confidence_auc': 0,
            'baseline_shadow_auc': 0,
            'fisher_dp_shadow_auc_positive': 0,
            'fisher_dp_shadow_auc_negative': 0,
            'vanilla_dp_shadow_auc': 0,
            'dp_sat_shadow_auc': 0
        }
        
        # Extract positively correlated data
        if len(positive_data) > 0:
            pos_row = positive_data.iloc[0]
            plot_data[user_count]['positive_fisher'] = pos_row.get('fisher_dp', 0)
            plot_data[user_count]['positive_vanilla'] = pos_row.get('vanilla_dp', 0)
            plot_data[user_count]['positive_dp_sat'] = pos_row.get('dp_sat', 0)
            plot_data[user_count]['baseline'] = pos_row.get('baseline', 0)
            
            # MIA data from positive experiments
            # Try both formats for backward compatibility
            plot_data[user_count]['baseline_confidence_auc'] = pos_row.get('baseline_confidence_auc', pos_row.get('baseline_conf_auc', 0))
            plot_data[user_count]['fisher_dp_confidence_auc_positive'] = pos_row.get('fisher_dp_confidence_auc', pos_row.get('fisher_dp_conf_auc', 0))
            plot_data[user_count]['vanilla_dp_confidence_auc'] = pos_row.get('vanilla_dp_confidence_auc', pos_row.get('vanilla_dp_conf_auc', 0))
            plot_data[user_count]['dp_sat_confidence_auc'] = pos_row.get('dp_sat_confidence_auc', pos_row.get('dp_sat_conf_auc', 0))
            
            plot_data[user_count]['baseline_shadow_auc'] = pos_row.get('baseline_shadow_auc', 0)
            plot_data[user_count]['fisher_dp_shadow_auc_positive'] = pos_row.get('fisher_dp_shadow_auc', 0)
            plot_data[user_count]['vanilla_dp_shadow_auc'] = pos_row.get('vanilla_dp_shadow_auc', 0)
            plot_data[user_count]['dp_sat_shadow_auc'] = pos_row.get('dp_sat_shadow_auc', 0)
        
        # Extract negatively correlated data
        if len(negative_data) > 0:
            neg_row = negative_data.iloc[0]
            plot_data[user_count]['negative_fisher'] = neg_row.get('fisher_dp', 0)
            plot_data[user_count]['negative_vanilla'] = neg_row.get('vanilla_dp', 0)
            plot_data[user_count]['negative_dp_sat'] = neg_row.get('dp_sat', 0)
            
            # MIA data from negative experiments - try both formats
            plot_data[user_count]['fisher_dp_confidence_auc_negative'] = neg_row.get('fisher_dp_confidence_auc', neg_row.get('fisher_dp_conf_auc', 0))
            plot_data[user_count]['fisher_dp_shadow_auc_negative'] = neg_row.get('fisher_dp_shadow_auc', 0)
        
        # Calculate strategy difference
        if plot_data[user_count]['positive_fisher'] > 0 and plot_data[user_count]['negative_fisher'] > 0:
            plot_data[user_count]['strategy_difference'] = plot_data[user_count]['positive_fisher'] - plot_data[user_count]['negative_fisher']
    
    return plot_data

def create_validation_plots(plot_data, config=None, output_dir="validation_plots", json_file_path=None):
    """Create comprehensive validation plots from plot data"""
    if not plot_data:
        print("âŒ No plot data available")
        return
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    user_counts = list(plot_data.keys())
    strategy_diffs = [plot_data[uc]['strategy_difference'] for uc in user_counts]
    
    # Extract config values with defaults
    if config and 'common_args' in config:
        k_value = config['common_args'].get('k', 'Unknown')
        epsilon_value = config['common_args'].get('target-epsilon', 'Unknown') 
        epochs_value = config['common_args'].get('epochs', 'Unknown')
        dp_layer_value = config['common_args'].get('dp-layer', 'Unknown')
        title_suffix = f'k={k_value}, Îµ={epsilon_value}, epochs={epochs_value}, {dp_layer_value}'
    else:
        title_suffix = 'Configuration details not available'
    
    # Get seed from config metadata if available
    seed_info = ""
    if config and 'metadata' in config and 'seed' in config['metadata']:
        seed_info = f" (seed={config['metadata']['seed']})"
    
    # Create 2x2 layout for the 4 requested plots
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'Visual Plotter: Fisher DP Validation Results{seed_info}\n{title_suffix}', 
                fontsize=16, fontweight='bold')
    
    # Plot 1: Key Discovery: Positively Correlated Advantage
    ax1 = axes[0, 0]
    ax1.plot(user_counts, strategy_diffs, 'o-', linewidth=3, markersize=8, color='darkred')
    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
    ax1.fill_between(user_counts, strategy_diffs, 0, alpha=0.3, color='darkred')
    ax1.set_xlabel('Number of Users')
    ax1.set_ylabel('Positively Correlated Advantage (%)')
    ax1.set_title('Key Discovery: Positively Correlated Advantage')
    ax1.grid(True, alpha=0.3)
    
    # Highlight positive values
    for x, y in zip(user_counts, strategy_diffs):
        if y > 0.5:
            ax1.annotate(f'+{y:.1f}%', (x, y), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontweight='bold', color='darkred')
    
    # Plot 2: All Methods Comparison
    ax2 = axes[0, 1]
    positive_fisher = [plot_data[uc]['positive_fisher'] for uc in user_counts]
    negative_fisher = [plot_data[uc]['negative_fisher'] for uc in user_counts]
    vanilla_accs = [plot_data[uc]['positive_vanilla'] for uc in user_counts]
    dp_sat_accs = [plot_data[uc]['positive_dp_sat'] for uc in user_counts]
    baseline_accs = [plot_data[uc]['baseline'] for uc in user_counts]
    
    ax2.plot(user_counts, positive_fisher, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
    ax2.plot(user_counts, negative_fisher, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
    ax2.plot(user_counts, vanilla_accs, '^-', label='Vanilla DP', linewidth=2, markersize=6)
    ax2.plot(user_counts, dp_sat_accs, 'd-', label='DP-SAT', linewidth=2, markersize=6)
    ax2.plot(user_counts, baseline_accs, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
    
    ax2.set_xlabel('Number of Users')
    ax2.set_ylabel('Test Accuracy (%)')
    ax2.set_title('All Methods Comparison')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Privacy: Confidence Attack Results (Lower is Better)
    ax3 = axes[1, 0]
    baseline_conf = [plot_data[uc]['baseline_confidence_auc'] for uc in user_counts]
    fisher_conf_pos = [plot_data[uc]['fisher_dp_confidence_auc_positive'] for uc in user_counts]
    fisher_conf_neg = [plot_data[uc]['fisher_dp_confidence_auc_negative'] for uc in user_counts]
    vanilla_conf = [plot_data[uc]['vanilla_dp_confidence_auc'] for uc in user_counts]
    dp_sat_conf = [plot_data[uc]['dp_sat_confidence_auc'] for uc in user_counts]
    
    # Only plot if we have MIA data
    if any(baseline_conf) or any(fisher_conf_pos) or any(vanilla_conf):
        ax3.plot(user_counts, baseline_conf, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
        ax3.plot(user_counts, fisher_conf_pos, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
        ax3.plot(user_counts, fisher_conf_neg, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
        ax3.plot(user_counts, vanilla_conf, '^-', label='Vanilla DP', linewidth=2, markersize=6)
        ax3.plot(user_counts, dp_sat_conf, 'd-', label='DP-SAT', linewidth=2, markersize=6)
        
        ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (0.5)')
        ax3.set_ylim(0.4, max(0.8, max(baseline_conf + fisher_conf_pos + vanilla_conf + dp_sat_conf) + 0.05))
    else:
        ax3.text(0.5, 0.5, 'No MIA Confidence Data Available', transform=ax3.transAxes, 
                ha='center', va='center', fontsize=12, style='italic')
    
    ax3.set_xlabel('Number of Users')
    ax3.set_ylabel('Confidence Attack AUC')
    ax3.set_title('Privacy: Confidence Attack Results (Lower is Better)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Privacy: Shadow Attack Results (Lower is Better)
    ax4 = axes[1, 1]
    baseline_shadow = [plot_data[uc]['baseline_shadow_auc'] for uc in user_counts]
    fisher_shadow_pos = [plot_data[uc]['fisher_dp_shadow_auc_positive'] for uc in user_counts]
    fisher_shadow_neg = [plot_data[uc]['fisher_dp_shadow_auc_negative'] for uc in user_counts]
    vanilla_shadow = [plot_data[uc]['vanilla_dp_shadow_auc'] for uc in user_counts]
    dp_sat_shadow = [plot_data[uc]['dp_sat_shadow_auc'] for uc in user_counts]
    
    # Only plot if we have MIA data
    if any(baseline_shadow) or any(fisher_shadow_pos) or any(vanilla_shadow):
        ax4.plot(user_counts, baseline_shadow, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
        ax4.plot(user_counts, fisher_shadow_pos, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
        ax4.plot(user_counts, fisher_shadow_neg, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
        ax4.plot(user_counts, vanilla_shadow, '^-', label='Vanilla DP', linewidth=2, markersize=6)
        ax4.plot(user_counts, dp_sat_shadow, 'd-', label='DP-SAT', linewidth=2, markersize=6)
        
        ax4.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (0.5)')
        ax4.set_ylim(0.4, max(0.8, max(baseline_shadow + fisher_shadow_pos + vanilla_shadow + dp_sat_shadow) + 0.05))
    else:
        ax4.text(0.5, 0.5, 'No MIA Shadow Data Available', transform=ax4.transAxes, 
                ha='center', va='center', fontsize=12, style='italic')
    
    ax4.set_xlabel('Number of Users')
    ax4.set_ylabel('Shadow Attack AUC')
    ax4.set_title('Privacy: Shadow Attack Results (Lower is Better)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save plot - use the same sequence ID as the input JSON file
    if json_file_path:
        input_basename = os.path.basename(json_file_path)
        
        if input_basename.startswith('validation_results_') and input_basename.endswith('.json'):
            # Extract the sequence ID from input filename: validation_results_20250702_134908_seed_43.json
            sequence_id = input_basename[len('validation_results_'):-len('.json')]
            plot_filename = f"validation_results_{sequence_id}.png"
        else:
            # Fallback to timestamp if filename format is unexpected
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_filename = f"validation_plot_{timestamp}.png"
    else:
        # Fallback to timestamp if no input file path provided
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"validation_plot_{timestamp}.png"
    
    plot_file = os.path.join(output_dir, plot_filename)
    
    # Get DPI from config or use default
    dpi = 300
    if config and 'output_settings' in config:
        dpi = config['output_settings'].get('figure_dpi', 300)
    
    plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
    print(f"ğŸ“Š Validation plot saved to: {plot_file}")
    
    plt.show()
    return plot_file

def print_analysis_summary(plot_data, config=None):
    """Print a detailed analysis summary including MIA results"""
    print(f"\n" + "="*60)
    print(f"VISUAL PLOTTER ANALYSIS SUMMARY")
    print(f"="*60)
    
    user_counts = list(plot_data.keys())
    strategy_diffs = [plot_data[uc]['strategy_difference'] for uc in user_counts]
    
    # Extract config information if available
    if config and 'common_args' in config:
        print(f"ğŸ“Š Experiment Configuration:")
        print(f"   â€¢ Fisher k: {config['common_args'].get('k', 'N/A')}")
        print(f"   â€¢ Privacy: Îµ={config['common_args'].get('target-epsilon', 'N/A')}, Î´={config['common_args'].get('delta', 'N/A')}")
        print(f"   â€¢ Epochs: {config['common_args'].get('epochs', 'N/A')}")
        print(f"   â€¢ Layers: {config['common_args'].get('dp-layer', 'N/A')}")
    else:
        print(f"ğŸ“Š Configuration details not available in loaded data")
    
    # Add seed information
    if config and 'metadata' in config and 'seed' in config['metadata']:
        print(f"   â€¢ Random seed: {config['metadata']['seed']}")
    
    print(f"   â€¢ User counts tested: {user_counts}")
    
    print(f"\nğŸ¯ Key Findings:")
    
    # Noise strategy results
    wins = sum(1 for d in strategy_diffs if d > 0.5)
    print(f"   ğŸ“Š Noise Strategy Comparison:")
    print(f"     â€¢ Positively correlated noise wins: {wins}/{len(user_counts)} conditions")
    
    if wins > 0:
        best_idx = np.argmax(strategy_diffs)
        best_user_count = user_counts[best_idx]
        best_advantage = strategy_diffs[best_idx]
        print(f"     â€¢ Best condition: {best_user_count} users (+{best_advantage:.2f}% advantage)")
        
        best_data = plot_data[best_user_count]
        print(f"     â€¢ At best condition:")
        print(f"       - Positively correlated Fisher DP: {best_data['positive_fisher']:.2f}%")
        print(f"       - Negatively correlated Fisher DP: {best_data['negative_fisher']:.2f}%")
        print(f"       - Vanilla DP: {best_data['positive_vanilla']:.2f}%")
        print(f"       - DP-SAT: {best_data['positive_dp_sat']:.2f}%")
    
    # Check if we have MIA data
    has_mia_data = any(plot_data[uc]['baseline_confidence_auc'] > 0 for uc in user_counts)
    
    if has_mia_data:
        print(f"\nğŸ›¡ï¸  Privacy Analysis (MIA Results):")
        print(f"   ğŸ“Š Confidence Attack AUCs (lower = better privacy):")
        for uc in user_counts:
            data = plot_data[uc]
            if data['baseline_confidence_auc'] > 0:
                print(f"     {uc:3d} users:")
                print(f"       - Baseline: {data['baseline_confidence_auc']:.4f}")
                print(f"       - Fisher DP (Pos): {data['fisher_dp_confidence_auc_positive']:.4f}")
                print(f"       - Fisher DP (Neg): {data['fisher_dp_confidence_auc_negative']:.4f}")
                print(f"       - Vanilla DP: {data['vanilla_dp_confidence_auc']:.4f}")
                print(f"       - DP-SAT: {data['dp_sat_confidence_auc']:.4f}")
        
        print(f"\n   ğŸ“Š Shadow Attack AUCs (lower = better privacy):")
        for uc in user_counts:
            data = plot_data[uc]
            if data['baseline_shadow_auc'] > 0:
                print(f"     {uc:3d} users:")
                print(f"       - Baseline: {data['baseline_shadow_auc']:.4f}")
                print(f"       - Fisher DP (Pos): {data['fisher_dp_shadow_auc_positive']:.4f}")
                print(f"       - Fisher DP (Neg): {data['fisher_dp_shadow_auc_negative']:.4f}")
                print(f"       - Vanilla DP: {data['vanilla_dp_shadow_auc']:.4f}")
                print(f"       - DP-SAT: {data['dp_sat_shadow_auc']:.4f}")
    else:
        print(f"\nğŸ›¡ï¸  Privacy Analysis:")
        print(f"   âš ï¸  No MIA data available in results")
        print(f"   ğŸ’¡ Run experiments with --run-mia flag to get privacy analysis")
    
    print(f"\nğŸ“ˆ Detailed Results by User Count:")
    for uc in user_counts:
        data = plot_data[uc]
        print(f"   {uc:3d} users:")
        print(f"     â€¢ Fisher DP: Pos={data['positive_fisher']:.2f}% vs Neg={data['negative_fisher']:.1f}% " +
              f"(diff: {data['strategy_difference']:+5.1f}%)")
    
    print(f"\nâœ… Analysis completed using visual plotter")
    print(f"   Plots generated from saved experimental results")
    print(f"   ğŸ“Š 4 plots created: Discovery, Methods, Confidence MIA, Shadow MIA")

def main():
    """Main plotting function with command line interface"""
    parser = argparse.ArgumentParser(
        description='Visual Plotter: Create plots from validation results JSON files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --latest                    # Plot most recent results
  %(prog)s --results results.json      # Plot specific results file
  %(prog)s --results results.json --output plots/  # Custom output directory
        """
    )
    
    parser.add_argument('--results', type=str, 
                       help='Path to validation results JSON file')
    parser.add_argument('--latest', action='store_true',
                       help='Automatically find and plot the latest results file')
    parser.add_argument('--output', type=str, default='validation_plots',
                       help='Output directory for plots (default: validation_plots)')
    parser.add_argument('--results-dir', type=str, default='validation_results',
                       help='Directory to search for results files (default: validation_results)')
    
    args = parser.parse_args()
    
    print("ğŸ“Š VISUAL PLOTTER: Plotting from Validation Results")
    print("=" * 60)
    print("Generates comprehensive plots from JSON results files")
    print("produced by visual_discovery_analysis.py")
    print("=" * 60)
    
    # Determine which results file to use
    if args.latest:
        try:
            results_file = find_latest_results_file(args.results_dir)
            print(f"ğŸ” Auto-detected latest results file")
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return
    elif args.results:
        results_file = args.results
        if not os.path.exists(results_file):
            print(f"âŒ Results file not found: {results_file}")
            return
    else:
        print("âŒ Please specify either --latest or --results <file>")
        parser.print_help()
        return
    
    try:
        # Load results
        results, config = load_results_from_json(results_file)
        
        if not results:
            print("âŒ No results found in the file")
            return
        
        # Analyze results
        plot_data = analyze_results_from_json(results, config)
        
        if not plot_data:
            print("âŒ Failed to process results into plot data")
            return
        
        # Create plots
        plot_file = create_validation_plots(plot_data, config, args.output, results_file)
        
        # Print summary
        print_analysis_summary(plot_data, config)
        
        print(f"\nğŸ‰ Visual plotting complete!")
        print(f"ğŸ“ Input: {results_file}")
        print(f"ğŸ“Š Output: {plot_file}")
        print(f"ğŸ“‚ Plots directory: {args.output}")
        
    except Exception as e:
        print(f"âŒ Error during plotting: {e}")
        raise

if __name__ == "__main__":
    main() 