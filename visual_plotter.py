#!/usr/bin/env python3
"""
Visual Plotter: Plotting from Validation Results
=================================================

This script reads JSON results from visual_discovery_analysis.py and creates
comprehensive visualizations. It's decoupled from the experiment running logic
to allow for flexible replotting and analysis.

Key Features:
1. Loads results from JSON files generated by visual_discovery_analysis.py
2. Reconstructs plot data structure from experiment results
3. Creates comprehensive validation plots
4. Supports loading latest results automatically
5. Independent of expensive experiment re-runs
"""

import json
import os
import glob
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pandas as pd

# Set style for publication-quality plots
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

def find_latest_results_file(results_dir="validation_results"):
    """Find the most recent results JSON file"""
    if not os.path.exists(results_dir):
        raise FileNotFoundError(f"Results directory {results_dir} not found")
    
    pattern = os.path.join(results_dir, "validation_results_*.json")
    files = glob.glob(pattern)
    
    if not files:
        raise FileNotFoundError(f"No validation results files found in {results_dir}")
    
    # Sort by modification time, most recent first
    latest_file = max(files, key=os.path.getmtime)
    return latest_file

def load_results_from_json(json_file_path):
    """Load experimental results and config from JSON file"""
    print(f"ğŸ“„ Loading results from: {json_file_path}")
    
    with open(json_file_path, 'r') as f:
        data = json.load(f)
    
    # Handle both old format (list of results) and new format (dict with results + config)
    if isinstance(data, list):
        # Old format: just results list
        results = data
        config = None
        print("âš ï¸  Old format detected - config information not available")
    elif isinstance(data, dict) and 'results' in data:
        # New format: dict with results and config
        results = data['results']
        config = data.get('config', None)
        print(f"âœ… New format detected - loaded {len(results)} results with config")
    else:
        raise ValueError("Unrecognized JSON format")
    
    print(f"ğŸ“Š Loaded {len(results)} experimental results")
    
    # Print summary of results
    if results:
        user_counts = sorted(set(r.get('users', 0) for r in results))
        noise_strategies = set(r.get('noise_strategy', 'unknown') for r in results)
        print(f"   â€¢ User counts: {user_counts}")
        print(f"   â€¢ Noise strategies: {sorted(noise_strategies)}")
    
    return results, config

def detect_experiment_parameter(results):
    """Detect which parameter is being varied in the experiments"""
    if not results:
        return None, []
    
    # Check what parameters vary across experiments
    varying_params = {}
    
    # Common parameters to check
    param_keys = ['users', 'target-epsilon', 'clip-radius', 'dp-layer', 'k']
    
    for param in param_keys:
        # Collect all values for this parameter
        values = []
        for result in results:
            if param in result:
                values.append(result[param])
            # Also check experiment metadata for parsed values
            elif 'experiment_name' in result:
                # Try to extract parameter from experiment name
                exp_name = result['experiment_name']
                if 'eps_' in exp_name and param == 'target-epsilon':
                    # Extract epsilon value from name like "eps_1.0_layer_c1_pos"
                    parts = exp_name.split('_')
                    for i, part in enumerate(parts):
                        if part == 'eps' and i + 1 < len(parts):
                            try:
                                values.append(float(parts[i + 1]))
                            except ValueError:
                                pass
                elif 'clip_' in exp_name and param == 'clip-radius':
                    # Extract clip value from name like "clip_1.0_pos"
                    parts = exp_name.split('_')
                    for i, part in enumerate(parts):
                        if part == 'clip' and i + 1 < len(parts):
                            try:
                                values.append(float(parts[i + 1]))
                            except ValueError:
                                pass
                elif 'u' in exp_name and param == 'users':
                    # Extract users from name like "u100_pos"
                    parts = exp_name.split('_')
                    for part in parts:
                        if part.startswith('u') and len(part) > 1:
                            try:
                                values.append(int(part[1:]))
                            except ValueError:
                                pass
        
        # Check if this parameter actually varies
        unique_values = list(set(values))
        if len(unique_values) > 1:
            varying_params[param] = sorted(unique_values)
    
    # Determine the primary varying parameter
    if 'users' in varying_params:
        return 'users', varying_params['users']
    elif 'target-epsilon' in varying_params:
        return 'target-epsilon', varying_params['target-epsilon']
    elif 'clip-radius' in varying_params:
        return 'clip-radius', varying_params['clip-radius']
    elif 'dp-layer' in varying_params:
        return 'dp-layer', varying_params['dp-layer']
    elif 'k' in varying_params:
        return 'k', varying_params['k']
    else:
        # Fallback to users if no clear parameter is detected
        user_counts = [r.get('users', 0) for r in results]
        return 'users', sorted(set(user_counts))

def get_parameter_value(result, param_name):
    """Extract parameter value from result, handling different formats"""
    # Direct parameter access
    if param_name in result:
        return result[param_name]
    
    # Extract from experiment name for grid-generated experiments
    if 'experiment_name' in result:
        exp_name = result['experiment_name']
        
        if param_name == 'target-epsilon' and 'eps_' in exp_name:
            parts = exp_name.split('_')
            for i, part in enumerate(parts):
                if part == 'eps' and i + 1 < len(parts):
                    try:
                        return float(parts[i + 1])
                    except ValueError:
                        pass
        elif param_name == 'clip-radius' and 'clip_' in exp_name:
            parts = exp_name.split('_')
            for i, part in enumerate(parts):
                if part == 'clip' and i + 1 < len(parts):
                    try:
                        return float(parts[i + 1])
                    except ValueError:
                        pass
        elif param_name == 'users' and 'u' in exp_name:
            parts = exp_name.split('_')
            for part in parts:
                if part.startswith('u') and len(part) > 1:
                    try:
                        return int(part[1:])
                    except ValueError:
                        pass
    
    # Fallback defaults
    if param_name == 'users':
        return result.get('users', 0)
    else:
        return 0

def get_axis_label(param_name):
    """Get appropriate axis label for parameter"""
    labels = {
        'users': 'Number of Users',
        'target-epsilon': 'Privacy Budget (Îµ)',
        'clip-radius': 'Clipping Radius',
        'dp-layer': 'DP Layers',
        'k': 'Fisher Subspace Dimension (k)'
    }
    return labels.get(param_name, param_name.replace('-', ' ').title())

def get_parameter_display_value(param_name, value):
    """Format parameter value for display"""
    if param_name == 'dp-layer':
        # Clean up layer names for display
        if value == 'conv1':
            return 'Conv1'
        elif value == 'conv1,conv2':
            return 'Conv1+2'
        elif value == 'all':
            return 'All'
        else:
            return str(value)
    else:
        return value

def analyze_results_from_json(results, config=None):
    """Convert JSON results to plot data structure including MIA data - parameter agnostic"""
    if not results:
        print("âŒ No results to analyze")
        return {}
    
    print(f"ğŸ“Š Analyzing {len(results)} experimental results...")
    
    # Detect what parameter is being varied
    param_name, param_values = detect_experiment_parameter(results)
    print(f"ğŸ” Detected varying parameter: {param_name} with values: {param_values}")
    
    # Convert to DataFrame for easier analysis
    df = pd.DataFrame(results)
    
    # Prepare data for plotting
    plot_data = {}
    for param_value in param_values:
        # Filter results for this parameter value
        param_data = []
        for result in results:
            result_param_value = get_parameter_value(result, param_name)
            if result_param_value == param_value:
                param_data.append(result)
        
        if not param_data:
            continue
            
        param_df = pd.DataFrame(param_data)
        
        positive_data = param_df[param_df['noise_strategy'] == 'positive']
        negative_data = param_df[param_df['noise_strategy'] == 'negative']
        
        plot_data[param_value] = {
            'baseline': 0,
            'positive_fisher': 0,
            'negative_fisher': 0,
            'positive_vanilla': 0,
            'negative_vanilla': 0,
            'positive_dp_sat': 0,
            'negative_dp_sat': 0,
            'strategy_difference': 0,
            # MIA data
            'baseline_confidence_auc': 0,
            'fisher_dp_confidence_auc_positive': 0,
            'fisher_dp_confidence_auc_negative': 0,
            'vanilla_dp_confidence_auc': 0,
            'dp_sat_confidence_auc': 0,
            'baseline_shadow_auc': 0,
            'fisher_dp_shadow_auc_positive': 0,
            'fisher_dp_shadow_auc_negative': 0,
            'vanilla_dp_shadow_auc': 0,
            'dp_sat_shadow_auc': 0
        }
        
        # Extract positively correlated data
        if len(positive_data) > 0:
            pos_row = positive_data.iloc[0]
            plot_data[param_value]['positive_fisher'] = pos_row.get('fisher_dp', 0)
            plot_data[param_value]['positive_vanilla'] = pos_row.get('vanilla_dp', 0)
            plot_data[param_value]['positive_dp_sat'] = pos_row.get('dp_sat', 0)
            plot_data[param_value]['baseline'] = pos_row.get('baseline', 0)
            
            # MIA data from positive experiments
            # Try both formats for backward compatibility
            plot_data[param_value]['baseline_confidence_auc'] = pos_row.get('baseline_confidence_auc', pos_row.get('baseline_conf_auc', 0))
            plot_data[param_value]['fisher_dp_confidence_auc_positive'] = pos_row.get('fisher_dp_confidence_auc', pos_row.get('fisher_dp_conf_auc', 0))
            plot_data[param_value]['vanilla_dp_confidence_auc'] = pos_row.get('vanilla_dp_confidence_auc', pos_row.get('vanilla_dp_conf_auc', 0))
            plot_data[param_value]['dp_sat_confidence_auc'] = pos_row.get('dp_sat_confidence_auc', pos_row.get('dp_sat_conf_auc', 0))
            
            plot_data[param_value]['baseline_shadow_auc'] = pos_row.get('baseline_shadow_auc', 0)
            plot_data[param_value]['fisher_dp_shadow_auc_positive'] = pos_row.get('fisher_dp_shadow_auc', 0)
            plot_data[param_value]['vanilla_dp_shadow_auc'] = pos_row.get('vanilla_dp_shadow_auc', 0)
            plot_data[param_value]['dp_sat_shadow_auc'] = pos_row.get('dp_sat_shadow_auc', 0)
        
        # Extract negatively correlated data
        if len(negative_data) > 0:
            neg_row = negative_data.iloc[0]
            plot_data[param_value]['negative_fisher'] = neg_row.get('fisher_dp', 0)
            plot_data[param_value]['negative_vanilla'] = neg_row.get('vanilla_dp', 0)
            plot_data[param_value]['negative_dp_sat'] = neg_row.get('dp_sat', 0)
            
            # MIA data from negative experiments - try both formats
            plot_data[param_value]['fisher_dp_confidence_auc_negative'] = neg_row.get('fisher_dp_confidence_auc', neg_row.get('fisher_dp_conf_auc', 0))
            plot_data[param_value]['fisher_dp_shadow_auc_negative'] = neg_row.get('fisher_dp_shadow_auc', 0)
        
        # Calculate strategy difference
        if plot_data[param_value]['positive_fisher'] > 0 and plot_data[param_value]['negative_fisher'] > 0:
            plot_data[param_value]['strategy_difference'] = plot_data[param_value]['positive_fisher'] - plot_data[param_value]['negative_fisher']
    
    # Store metadata about the detected parameter
    plot_data['_metadata'] = {
        'parameter_name': param_name,
        'parameter_values': param_values,
        'axis_label': get_axis_label(param_name)
    }
    
    return plot_data

def create_validation_plots(plot_data, config=None, output_dir="validation_plots", json_file_path=None):
    """Create comprehensive validation plots from plot data - parameter agnostic"""
    if not plot_data:
        print("âŒ No plot data available")
        return
    
    # Create output directory
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract metadata
    metadata = plot_data.get('_metadata', {})
    param_name = metadata.get('parameter_name', 'users')
    axis_label = metadata.get('axis_label', 'Parameter Value')
    
    # Get parameter values (excluding metadata)
    param_values = [k for k in plot_data.keys() if k != '_metadata']
    param_values = sorted(param_values)
    
    strategy_diffs = [plot_data[pv]['strategy_difference'] for pv in param_values]
    
    # Extract config values with defaults
    if config and 'common_args' in config:
        k_value = config['common_args'].get('k', 'Unknown')
        epsilon_value = config['common_args'].get('target-epsilon', 'Unknown') 
        epochs_value = config['common_args'].get('epochs', 'Unknown')
        dp_layer_value = config['common_args'].get('dp-layer', 'Unknown')
        title_suffix = f'k={k_value}, Îµ={epsilon_value}, epochs={epochs_value}, {dp_layer_value}'
    else:
        title_suffix = 'Configuration details not available'
    
    # Get seed from config metadata if available
    seed_info = ""
    if config and 'metadata' in config and 'seed' in config['metadata']:
        seed_info = f" (seed={config['metadata']['seed']})"
    
    # Create 2x2 layout for the 4 requested plots
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'Visual Plotter: Fisher DP Validation Results{seed_info}\n{title_suffix}', 
                fontsize=16, fontweight='bold')
    
    # Handle categorical x-axis (like dp-layer) vs numerical
    is_categorical = param_name in ['dp-layer']
    if is_categorical:
        x_positions = range(len(param_values))
        x_labels = [get_parameter_display_value(param_name, pv) for pv in param_values]
    else:
        x_positions = param_values
        x_labels = param_values
    
    # Plot 1: Key Discovery: Positively Correlated Advantage
    ax1 = axes[0, 0]
    if is_categorical:
        ax1.plot(x_positions, strategy_diffs, 'o-', linewidth=3, markersize=8, color='darkred')
        ax1.set_xticks(x_positions)
        ax1.set_xticklabels(x_labels)
    else:
        ax1.plot(param_values, strategy_diffs, 'o-', linewidth=3, markersize=8, color='darkred')
    
    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
    ax1.fill_between(x_positions if is_categorical else param_values, strategy_diffs, 0, alpha=0.3, color='darkred')
    ax1.set_xlabel(axis_label)
    ax1.set_ylabel('Positively Correlated Advantage (%)')
    ax1.set_title('Key Discovery: Positively Correlated Advantage')
    ax1.grid(True, alpha=0.3)
    
    # Highlight positive values
    for x, y in zip(x_positions if is_categorical else param_values, strategy_diffs):
        if y > 0.5:
            ax1.annotate(f'+{y:.1f}%', (x, y), textcoords="offset points", 
                        xytext=(0,10), ha='center', fontweight='bold', color='darkred')
    
    # Plot 2: All Methods Comparison
    ax2 = axes[0, 1]
    positive_fisher = [plot_data[pv]['positive_fisher'] for pv in param_values]
    negative_fisher = [plot_data[pv]['negative_fisher'] for pv in param_values]
    vanilla_accs = [plot_data[pv]['positive_vanilla'] for pv in param_values]
    dp_sat_accs = [plot_data[pv]['positive_dp_sat'] for pv in param_values]
    baseline_accs = [plot_data[pv]['baseline'] for pv in param_values]
    
    if is_categorical:
        ax2.plot(x_positions, positive_fisher, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
        ax2.plot(x_positions, negative_fisher, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
        ax2.plot(x_positions, vanilla_accs, '^-', label='Vanilla DP', linewidth=2, markersize=6)
        ax2.plot(x_positions, dp_sat_accs, 'd-', label='DP-SAT', linewidth=2, markersize=6)
        ax2.plot(x_positions, baseline_accs, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
        ax2.set_xticks(x_positions)
        ax2.set_xticklabels(x_labels)
    else:
        ax2.plot(param_values, positive_fisher, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
        ax2.plot(param_values, negative_fisher, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
        ax2.plot(param_values, vanilla_accs, '^-', label='Vanilla DP', linewidth=2, markersize=6)
        ax2.plot(param_values, dp_sat_accs, 'd-', label='DP-SAT', linewidth=2, markersize=6)
        ax2.plot(param_values, baseline_accs, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
    
    ax2.set_xlabel(axis_label)
    ax2.set_ylabel('Test Accuracy (%)')
    ax2.set_title('All Methods Comparison')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Privacy: Confidence Attack Results (Lower is Better)
    ax3 = axes[1, 0]
    baseline_conf = [plot_data[pv]['baseline_confidence_auc'] for pv in param_values]
    fisher_conf_pos = [plot_data[pv]['fisher_dp_confidence_auc_positive'] for pv in param_values]
    fisher_conf_neg = [plot_data[pv]['fisher_dp_confidence_auc_negative'] for pv in param_values]
    vanilla_conf = [plot_data[pv]['vanilla_dp_confidence_auc'] for pv in param_values]
    dp_sat_conf = [plot_data[pv]['dp_sat_confidence_auc'] for pv in param_values]
    
    # Only plot if we have MIA data
    if any(baseline_conf) or any(fisher_conf_pos) or any(vanilla_conf):
        if is_categorical:
            ax3.plot(x_positions, baseline_conf, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
            ax3.plot(x_positions, fisher_conf_pos, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
            ax3.plot(x_positions, fisher_conf_neg, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
            ax3.plot(x_positions, vanilla_conf, '^-', label='Vanilla DP', linewidth=2, markersize=6)
            ax3.plot(x_positions, dp_sat_conf, 'd-', label='DP-SAT', linewidth=2, markersize=6)
            ax3.set_xticks(x_positions)
            ax3.set_xticklabels(x_labels)
        else:
            ax3.plot(param_values, baseline_conf, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
            ax3.plot(param_values, fisher_conf_pos, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
            ax3.plot(param_values, fisher_conf_neg, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
            ax3.plot(param_values, vanilla_conf, '^-', label='Vanilla DP', linewidth=2, markersize=6)
            ax3.plot(param_values, dp_sat_conf, 'd-', label='DP-SAT', linewidth=2, markersize=6)
        
        ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (0.5)')
        ax3.set_ylim(0.4, max(0.8, max(baseline_conf + fisher_conf_pos + vanilla_conf + dp_sat_conf) + 0.05))
    else:
        ax3.text(0.5, 0.5, 'No MIA Confidence Data Available', transform=ax3.transAxes, 
                ha='center', va='center', fontsize=12, style='italic')
    
    ax3.set_xlabel(axis_label)
    ax3.set_ylabel('Confidence Attack AUC')
    ax3.set_title('Privacy: Confidence Attack Results (Lower is Better)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Privacy: Shadow Attack Results (Lower is Better)
    ax4 = axes[1, 1]
    baseline_shadow = [plot_data[pv]['baseline_shadow_auc'] for pv in param_values]
    fisher_shadow_pos = [plot_data[pv]['fisher_dp_shadow_auc_positive'] for pv in param_values]
    fisher_shadow_neg = [plot_data[pv]['fisher_dp_shadow_auc_negative'] for pv in param_values]
    vanilla_shadow = [plot_data[pv]['vanilla_dp_shadow_auc'] for pv in param_values]
    dp_sat_shadow = [plot_data[pv]['dp_sat_shadow_auc'] for pv in param_values]
    
    # Only plot if we have MIA data
    if any(baseline_shadow) or any(fisher_shadow_pos) or any(vanilla_shadow):
        if is_categorical:
            ax4.plot(x_positions, baseline_shadow, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
            ax4.plot(x_positions, fisher_shadow_pos, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
            ax4.plot(x_positions, fisher_shadow_neg, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
            ax4.plot(x_positions, vanilla_shadow, '^-', label='Vanilla DP', linewidth=2, markersize=6)
            ax4.plot(x_positions, dp_sat_shadow, 'd-', label='DP-SAT', linewidth=2, markersize=6)
            ax4.set_xticks(x_positions)
            ax4.set_xticklabels(x_labels)
        else:
            ax4.plot(param_values, baseline_shadow, 'x-', label='Baseline', linewidth=2, markersize=6, alpha=0.7)
            ax4.plot(param_values, fisher_shadow_pos, 'o-', label='Fisher DP (Positive)', linewidth=2, markersize=6)
            ax4.plot(param_values, fisher_shadow_neg, 's-', label='Fisher DP (Negative)', linewidth=2, markersize=6)
            ax4.plot(param_values, vanilla_shadow, '^-', label='Vanilla DP', linewidth=2, markersize=6)
            ax4.plot(param_values, dp_sat_shadow, 'd-', label='DP-SAT', linewidth=2, markersize=6)
        
        ax4.axhline(y=0.5, color='gray', linestyle='--', alpha=0.7, label='Random (0.5)')
        ax4.set_ylim(0.4, max(0.8, max(baseline_shadow + fisher_shadow_pos + vanilla_shadow + dp_sat_shadow) + 0.05))
    else:
        ax4.text(0.5, 0.5, 'No MIA Shadow Data Available', transform=ax4.transAxes, 
                ha='center', va='center', fontsize=12, style='italic')
    
    ax4.set_xlabel(axis_label)
    ax4.set_ylabel('Shadow Attack AUC')
    ax4.set_title('Privacy: Shadow Attack Results (Lower is Better)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # Save plot - use the same sequence ID as the input JSON file
    if json_file_path:
        input_basename = os.path.basename(json_file_path)
        
        if input_basename.startswith('validation_results_') and input_basename.endswith('.json'):
            # Extract the sequence ID from input filename: validation_results_20250702_134908_seed_43.json
            sequence_id = input_basename[len('validation_results_'):-len('.json')]
            plot_filename = f"validation_results_{sequence_id}.png"
        else:
            # Fallback to timestamp if filename format is unexpected
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_filename = f"validation_plot_{timestamp}.png"
    else:
        # Fallback to timestamp if no input file path provided
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"validation_plot_{timestamp}.png"
    
    plot_file = os.path.join(output_dir, plot_filename)
    
    # Get DPI from config or use default
    dpi = 300
    if config and 'output_settings' in config:
        dpi = config['output_settings'].get('figure_dpi', 300)
    
    plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
    print(f"ğŸ“Š Validation plot saved to: {plot_file}")
    
    plt.show()
    return plot_file

def print_analysis_summary(plot_data, config=None):
    """Print a detailed analysis summary including MIA results - parameter agnostic"""
    print(f"\n" + "="*60)
    print(f"VISUAL PLOTTER ANALYSIS SUMMARY")
    print(f"="*60)
    
    # Extract metadata
    metadata = plot_data.get('_metadata', {})
    param_name = metadata.get('parameter_name', 'users')
    axis_label = metadata.get('axis_label', 'Parameter Value')
    
    # Get parameter values (excluding metadata)
    param_values = [k for k in plot_data.keys() if k != '_metadata']
    param_values = sorted(param_values)
    
    strategy_diffs = [plot_data[pv]['strategy_difference'] for pv in param_values]
    
    # Extract config information if available
    if config and 'common_args' in config:
        print(f"ğŸ“Š Experiment Configuration:")
        print(f"   â€¢ Fisher k: {config['common_args'].get('k', 'N/A')}")
        print(f"   â€¢ Privacy: Îµ={config['common_args'].get('target-epsilon', 'N/A')}, Î´={config['common_args'].get('delta', 'N/A')}")
        print(f"   â€¢ Epochs: {config['common_args'].get('epochs', 'N/A')}")
        print(f"   â€¢ Layers: {config['common_args'].get('dp-layer', 'N/A')}")
    else:
        print(f"ğŸ“Š Configuration details not available in loaded data")
    
    # Add seed information
    if config and 'metadata' in config and 'seed' in config['metadata']:
        print(f"   â€¢ Random seed: {config['metadata']['seed']}")
    
    print(f"   â€¢ Varying parameter: {axis_label}")
    print(f"   â€¢ Parameter values tested: {param_values}")
    
    print(f"\nğŸ¯ Key Findings:")
    
    # Noise strategy results
    wins = sum(1 for d in strategy_diffs if d > 0.5)
    print(f"   ğŸ“Š Noise Strategy Comparison:")
    print(f"     â€¢ Positively correlated noise wins: {wins}/{len(param_values)} conditions")
    
    if wins > 0:
        best_idx = np.argmax(strategy_diffs)
        best_param_value = param_values[best_idx]
        best_advantage = strategy_diffs[best_idx]
        print(f"     â€¢ Best condition: {axis_label}={best_param_value} (+{best_advantage:.2f}% advantage)")
        
        best_data = plot_data[best_param_value]
        print(f"     â€¢ At best condition:")
        print(f"       - Positively correlated Fisher DP: {best_data['positive_fisher']:.2f}%")
        print(f"       - Negatively correlated Fisher DP: {best_data['negative_fisher']:.2f}%")
        print(f"       - Vanilla DP: {best_data['positive_vanilla']:.2f}%")
        print(f"       - DP-SAT: {best_data['positive_dp_sat']:.2f}%")
    
    # Check if we have MIA data
    has_mia_data = any(plot_data[pv]['baseline_confidence_auc'] > 0 for pv in param_values)
    
    if has_mia_data:
        print(f"\nğŸ›¡ï¸  Privacy Analysis (MIA Results):")
        print(f"   ğŸ“Š Confidence Attack AUCs (lower = better privacy):")
        for pv in param_values:
            data = plot_data[pv]
            if data['baseline_confidence_auc'] > 0:
                display_value = get_parameter_display_value(param_name, pv)
                print(f"     {axis_label}={display_value}:")
                print(f"       - Baseline: {data['baseline_confidence_auc']:.4f}")
                print(f"       - Fisher DP (Pos): {data['fisher_dp_confidence_auc_positive']:.4f}")
                print(f"       - Fisher DP (Neg): {data['fisher_dp_confidence_auc_negative']:.4f}")
                print(f"       - Vanilla DP: {data['vanilla_dp_confidence_auc']:.4f}")
                print(f"       - DP-SAT: {data['dp_sat_confidence_auc']:.4f}")
        
        print(f"\n   ğŸ“Š Shadow Attack AUCs (lower = better privacy):")
        for pv in param_values:
            data = plot_data[pv]
            if data['baseline_shadow_auc'] > 0:
                display_value = get_parameter_display_value(param_name, pv)
                print(f"     {axis_label}={display_value}:")
                print(f"       - Baseline: {data['baseline_shadow_auc']:.4f}")
                print(f"       - Fisher DP (Pos): {data['fisher_dp_shadow_auc_positive']:.4f}")
                print(f"       - Fisher DP (Neg): {data['fisher_dp_shadow_auc_negative']:.4f}")
                print(f"       - Vanilla DP: {data['vanilla_dp_shadow_auc']:.4f}")
                print(f"       - DP-SAT: {data['dp_sat_shadow_auc']:.4f}")
    else:
        print(f"\nğŸ›¡ï¸  Privacy Analysis:")
        print(f"   âš ï¸  No MIA data available in results")
        print(f"   ğŸ’¡ Run experiments with --run-mia flag to get privacy analysis")
    
    print(f"\nğŸ“ˆ Detailed Results by {axis_label}:")
    for pv in param_values:
        data = plot_data[pv]
        display_value = get_parameter_display_value(param_name, pv)
        print(f"   {axis_label}={display_value}:")
        print(f"     â€¢ Fisher DP: Pos={data['positive_fisher']:.2f}% vs Neg={data['negative_fisher']:.1f}% " +
              f"(diff: {data['strategy_difference']:+5.1f}%)")
    
    print(f"\nâœ… Analysis completed using visual plotter")
    print(f"   Plots generated from saved experimental results")
    print(f"   ğŸ“Š 4 plots created: Discovery, Methods, Confidence MIA, Shadow MIA")

def main():
    """Main plotting function with command line interface"""
    parser = argparse.ArgumentParser(
        description='Visual Plotter: Create plots from validation results JSON files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --latest                    # Plot most recent results
  %(prog)s --results results.json      # Plot specific results file
  %(prog)s --results results.json --output plots/  # Custom output directory
        """
    )
    
    parser.add_argument('--results', type=str, 
                       help='Path to validation results JSON file')
    parser.add_argument('--latest', action='store_true',
                       help='Automatically find and plot the latest results file')
    parser.add_argument('--output', type=str, default='validation_plots',
                       help='Output directory for plots (default: validation_plots)')
    parser.add_argument('--results-dir', type=str, default='validation_results',
                       help='Directory to search for results files (default: validation_results)')
    
    args = parser.parse_args()
    
    print("ğŸ“Š VISUAL PLOTTER: Plotting from Validation Results")
    print("=" * 60)
    print("Generates comprehensive plots from JSON results files")
    print("produced by visual_discovery_analysis.py")
    print("=" * 60)
    
    # Determine which results file to use
    if args.latest:
        try:
            results_file = find_latest_results_file(args.results_dir)
            print(f"ğŸ” Auto-detected latest results file")
        except FileNotFoundError as e:
            print(f"âŒ {e}")
            return
    elif args.results:
        results_file = args.results
        if not os.path.exists(results_file):
            print(f"âŒ Results file not found: {results_file}")
            return
    else:
        print("âŒ Please specify either --latest or --results <file>")
        parser.print_help()
        return
    
    try:
        # Load results
        results, config = load_results_from_json(results_file)
        
        if not results:
            print("âŒ No results found in the file")
            return
        
        # Analyze results
        plot_data = analyze_results_from_json(results, config)
        
        if not plot_data:
            print("âŒ Failed to process results into plot data")
            return
        
        # Create plots
        plot_file = create_validation_plots(plot_data, config, args.output, results_file)
        
        # Print summary
        print_analysis_summary(plot_data, config)
        
        print(f"\nğŸ‰ Visual plotting complete!")
        print(f"ğŸ“ Input: {results_file}")
        print(f"ğŸ“Š Output: {plot_file}")
        print(f"ğŸ“‚ Plots directory: {args.output}")
        
    except Exception as e:
        print(f"âŒ Error during plotting: {e}")
        raise

if __name__ == "__main__":
    main() 